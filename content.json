{"meta":{"title":"夏浅安のブログ","subtitle":"愿你出走半生，归来仍是少年。","description":"夏浅安的博客。","author":"夏浅安","url":"http://github.com/b2stry"},"pages":[{"title":"","date":"2017-11-05T09:23:49.891Z","updated":"2017-11-05T09:23:49.891Z","comments":true,"path":"df8ae1b842.html","permalink":"http://github.com/b2stry/df8ae1b842.html","excerpt":"","text":""},{"title":"","date":"2017-09-25T05:38:01.693Z","updated":"2017-09-25T05:37:49.572Z","comments":true,"path":"404.html","permalink":"http://github.com/b2stry/404.html","excerpt":"","text":""},{"title":"About","date":"2018-02-28T08:42:15.122Z","updated":"2018-02-28T08:42:15.122Z","comments":true,"path":"about/index.html","permalink":"http://github.com/b2stry/about/index.html","excerpt":"","text":"刘霈大数据学习中……一直以来都是个没什么梦想的人，不知道从哪天开始，突然想变成一个很厉害的人。世界远比你想象的要精彩，别放弃！ 做个男人，做个成熟的男人，做个有城府的男人一：沉稳1) 不要随便显露你的情绪。2) 不要逢人就诉说你的困难和遭遇。3) 在征询别人的意见之前，自己先思考，但不要先讲。4) 不要一有机会就唠叨你的不满。5) 重要的决定尽量有别人商量，最好隔一天再发布。6) 讲话不要有任何的慌张，走路也是。7) 自信是好，但是别忽略任何人的想法。8) 人无高低，不要一副拽拽的样子，对人对事，别忘了礼貌。你没有比任何人优秀。 二：细心1) 对身边发生的事情，常思考它们的因果关系。2) 对做不到位的问题，要发掘它们的根本症结。3) 对习以为常的做事方法，要有改进或优化的建议。4) 做什么事情都要养成有条不紊和井然有序的习惯。5) 经常去找几个别人看不出来的毛病或弊端。6) 自己要随时随地对有所不足的地方补位。 三：胆识1) 不要常用缺乏自信的词句。2) 不要常常反悔，轻易推翻已经决定的事。3) 在众人争执不休时，不要没有主见。4) 整体氛围低落时，你要乐观、阳光。5) 做任何事情都要用心，因为有人在看着你。6) 事情不顺的时候，歇口气，重新寻找突破口，就结束也要干净利落。 四：大度1) 不要刻意把有可能是伙伴的人变成对手。2) 对别人的小过失、小错误不要斤斤计较。3) 在金钱上要大方，学习三施（财施、法施、无畏施）。4) 不要有权力的傲慢和知识的偏见。5) 任何成果和成就都应和别人分享。 五：诚信1) 做不到的事情不要说，说了就努力做到。2) 虚的口号或标语不要常挂嘴上。3) 停止一切“不道德”的手段。4) 耍弄小聪明，要不得！ 六：担当1) 检讨任何过失的时候，先从自身或自己人开始反省。2) 事情结束后，先审查过错，再列述功劳。3) 一个计划，要统筹全局，规划未来。4) 勇于承担责任所造成的损失。 七：内涵1) 学习各方面的知识，虚心观察周围的事物。眼界宽阔。2) 了解自己，培养属于自己的审美观。3) 笑对生活。懒惰要不得。培养健康的生活习惯。4) 不要盲目的做任何事。要有目标。5) 不仅仅只关注内在美，外在美也很重要。6) 不要整天的对着电脑，玩着无聊的东西。7) 理智的判断，学会控制情绪。"},{"title":"Tags","date":"2017-11-03T05:13:45.143Z","updated":"2017-11-03T05:13:45.143Z","comments":true,"path":"tags/index.html","permalink":"http://github.com/b2stry/tags/index.html","excerpt":"","text":""},{"title":"Categories","date":"2017-11-03T05:13:45.143Z","updated":"2017-11-03T05:13:45.143Z","comments":true,"path":"categories/index.html","permalink":"http://github.com/b2stry/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"【置顶】《阿里感悟》如何在三年内成长为一名技术专家？","slug":"《阿里感悟》如何在三年内成长为一名技术专家？","date":"9420-04-02T07:14:07.000Z","updated":"2018-02-22T04:34:47.087Z","comments":true,"path":"9420/04/02/《阿里感悟》如何在三年内成长为一名技术专家？/","link":"","permalink":"http://github.com/b2stry/9420/04/02/《阿里感悟》如何在三年内成长为一名技术专家？/","excerpt":"引言： 工作前三年是职业生涯中成长最快的几年，在这段时间里你会充满激情，做事专注，也容易养成良好的习惯。在我们公司有些同学在前三年中就快速成为某一个领域的技术专家，有些同学也可能止步不前。本文和大家一起探讨下如何在三年内快速成长为一名技术专家。","text":"引言： 工作前三年是职业生涯中成长最快的几年，在这段时间里你会充满激情，做事专注，也容易养成良好的习惯。在我们公司有些同学在前三年中就快速成为某一个领域的技术专家，有些同学也可能止步不前。本文和大家一起探讨下如何在三年内快速成长为一名技术专家。 作者介绍清英, 蚂蚁金服技术专家，支付宝架构师，《JAVA并发编程的艺术》作者，10年+软件开发和架构经验，并发编程网（ http://ifeve.com/ ）创始人。目录学习方法 1: 掌握良好的学习心态 2: 掌握系统化的学习方法 3: 知识如何内化成能力 4: 广度和深度的选择 实战技巧 1: 你需要学会的编码习惯 2: 在业务团队做开发如何成长 掌握良好的学习心态空杯心态首先要有空杯的学习心态，而不是傲娇自满，故步自封，空杯子才可以装下更多的东西。首先要学会取百家之长，带着欣赏的眼光看团队的同事或学校的同学，欣赏每位同事或同学的优点，然后吸取他们的优点，每个同事都有其擅长的能力，比如有的同事技术能力强，那么可以观察下他如何学习的（或者找他请教学习方法），有的同学擅长解决线上问题，那么观察他是如何解决线上问题的，解决思路是什么？如果他解决不了时，他是如何寻求帮助。有的同学擅长使用IDE或MAC的快捷键，那么可以向他学习提高工作效率。有的同学能快速理解业务知识，观察他是如何做到的，自己如何达到他的程度。沟通能力，解决问题能力以及规划能力都可以向同事学习。 挑战权威从书上看到一个知识点，或者从别人那里听到一个知识点，一定要去挑战和质疑这个知识点的正确性，否则学到的知识点可能是错误的。先用逻辑思维推测下，再实战检测下，一定要记住实践是检验真理的唯一标准。比如同事说这个SQL加这个索引是最快的，首先要思考同事的结论是如何得出的，是靠历史经验还是测试过，如果我们没有经验，就加上这个索引跑下SQL，看看执行计划和执行时间，再换下其他索引试试会不会更快。依次类推，并发一定比串行快吗？无锁一定比加锁快吗? 很多结论都是在特定的场景下才会产生的，一定要自己亲手实践验证下。 坚持学习有的同学可能工作了五年，但是学习的时间可能一年都不到。学技术不能急于求成，只要学习方法正确，量变一定会引起质变。我在大学刚学JAVA时，怎么都学不会，但是坚持学习了几个月，每天看张老师的JAVA视频教学，买书按照书里的代码一行一行的敲代码，白天睡觉，晚上学习和写代码，写到宿舍关灯时就去避风塘呆一晚上，早上6点钟回宿舍睡觉，学到一定时间后，突然恍然大悟，才入了门。 在工作中，我曾经花了一个月的时间学习AOP的实现原理，学习了各种方式来实现AOP的原理，并写了几种实现方式的代码，虽然花的时间很多，但是到现在仍记忆犹新，对于排查问题和学习其他知识都非常有帮助。 要做到坚持学习，学习的环境非常重要。如果你想学，但是又不在学习状态，可以考虑换个学习环境，我经常会去星巴克看书和学习。我听说有的同事会周末抽一天去大学教室上自习。 把事做精对自己要求越高，进步越快。要有强烈的把事情做完美的心态，我刚开始工作的时候，总是快而不精，做事做的不够细致，总希望快速拿出结果证明自己，但是反而证明不了什么，技术能力也得不到提升，缺少技术亮点，在团队中也没什么影响力，后面就开始锻炼一次就把事情做对的心态和方法。我观察过，很多人都擅长快速做事情，但是把事情做好做精致的人会比较少，但是结果却是在精益求精的路上才会快速提高自己的能力。比如用100行代码实现的功能，思考下是否可以用10行来实现，以便于降低运维成本，提高下次的编码效率。引用GUAVA等类库，提取公共方法，和使用JDK8新特性等。系统的方法压测过后，单机只能承受1700QPS，可以思考和实践能否优化下程序提高QPS，减少服务器数量。 把事情做精，一定是要强迫自己多花心思多花时间在这件事情上。有位技术牛人给我分享了一个心得，我觉得说的非常好，老板给你布置了一个任务，你要花百分之150的精力做到100分，这样在老板那里你就能拿到80分或者60分。 掌握系统化的学习方法如果学习到的知识不成体系，那么遇到问题时就会非常难解决。有些同学会出现这些情况，比如编码时遇到问题百度搜索，如果百度上找不到答案，这个问题就解决不了。再比如，在开发中要用到某个技术点，就学习下API，程序调通后就不再深入研究，浅尝辄止，如果程序遇到其他问题也不知道如何解决。 以上情况我认为叫点状学习。遇到一个问题，解决一个问题，需要一项技术，学习一项技术。那么如何由点到面，由面到体，形成系统化学习呢。 首先要确定学习的知识领域，需要达成的学习目标，针对目标制定学习计划，就像你要写一本书一样，先把目录写出来，然后根据目录上的知识点逐步去学习，最后把这些知识点关联起来，形成一个系统化的知识体系。学习的时候，可以制定一个计划，以周为单位，比如第一周学什么，第二周学什么。 比如我最近在学习人工智能，学习步骤是： 1： 高数基础知识：线性代数，微积分和统计学。最近在打德州扑克时，我也会用统计学里的知识计算下输赢的概率。 2： 人工智能基础：买几本书人工智能的基础书籍，如《机器学习基础教程》《Python机器学习》 3： 框架：TensorFlow等。 4： 实战：在工作中找到一个应用场景，把学到的知识运用进去。 知识如何内化成能力作家格拉德威尔在《异类》一书中指出，1万小时的锤炼是任何人从平凡变成世界级大师的必要条件。1万小时有多久？每天学习10小时，需要大约三年。但是很多人都工作了五年甚至更长，但是为什么成为世界级大师的却非常少。读者可以先自己思考下这个问题。接下来谈谈我的看法。 成长必须经历一个步骤，就是把知识内化成能力。知识是用脑记住的，能力是用手练习出来的。在工作的几年里，我们可能看过很多书，听过很多技术讲座和视频，但是通过听和看只是让你能记住这些知识，这些知识还不能转换成你的能力。 听和看只是第一步，更重要的是实践，通过刻意练习把听到和看到的知识内化成你的能力。 刻意练习，就是有目的的练习，先规划好，再去练习。首先给自己定一个目标，目标可以有效的引导你学习，然后使用3F练习法： 1： 专注（Focus），专注在眼前的任务上，在学习过程中保持专注，可以尝试使用番茄工作法。 2：反馈（Feedback），意识到自己的不足，学习完之后进行反思，思考下自己哪些方面不足，为什么不足， 3： 修正（Fix），改进自己的不足。不停的练习和思考可以改变大脑结构，大脑像肌肉一样，挑战越大，影响越大，学习更高效，并且也会产生突破性。 广度和深度的选择技术人员的学习路径有两个维度，深度和广度。很多程序员都有这个疑问，是先深后广，还是先广后深呢？ 通过这么多年的学习和思考，我的建议先深后广，因为当技术学到一定深度后，就会有触类旁通的能力，自己掌握的广度也自然有了深度。但是在实际学习过程中，深度和广度相互穿插着学习，比如学习并发编程时，首先学习JDK源码，然后学进去之后，开始看JVM源码，最后看CPU架构，在技术点逐渐深度研究的过程中，广度也得到了完善。 所以无论哪种学习方式，学习态度才是最重要的，在广度学习的时候有深入研究的态度就能达到一定的深度，在深度学习的时候，主动学习相关的技术点，广度也得到拓宽。 你需要学会的编码习惯程序员应该学会通过技术的手段来提高效率。几个常用的手段是使用工具，快捷键和编写脚本。 1. 使用各种工具 技术人员电脑尽量用MAC，使用命令行效率一定比在10241024像素中找一个1010像素的按钮更快。IDE用IDEA，比Eclipse更智能。命令行工具用iTerm和IDEA里的Terminal。写文章用MAC的客户端工具MacDown，左边编写，右边展示，比Word等工具方便快速很多。有时候我还会用按键精灵里配置脚本需要解决工作问题，比如通过点击我们的系统，来执行任务。这样的工具很多，只要能提高工作效率的工具，大家都可以尝试使用。 2. 使用快捷键 MAC，IDEA和Eclipse有很多快捷键都要学会使用，比如在MAC命令行中通过idea .快速打开工程，通过open . 快速的打开文件夹，把IDEA里通过快捷键把一段代码抽成一个单独的方法，快速生成getter setter方法。 3. 用脚本写工具 当我们用人工的方式做一件重复性很强的事情，首先要考虑使用工具来帮我们自动完成，如果没有类似工具，可以自己写个脚本来实现，这样除了能快速解决问题，还能提高自己的技术能力。 比如，我经常要在两个maven仓库发布jar包，我就写了个脚本来实现jar包的发布，deploy.sh代码如下： 1234567cp pom.xml pom.xml.bakrm pom.xmlln -s pom-2-deploy.xml pom.xmlmvn deployrm -rf pom.xmlcp pom.xml.bak pom.xmlrm pom.xml.bak 在业务团队做开发如何成长我一直在业务团队中做开发，在业务团队最主要的提高的能力是业务抽象和架构能力，通过业务场景，不断思考如何通过合理的架构和业务抽象能快速支持业务，降低运维成本。同时在这个过程中锻炼技术能力，比如写一些技术框架来快速支持业务，做到技术驱动业务。 可配置化的方式支持业务设计业务的领域模型，把不随着业务逻辑变化的领域模型做成系统能力，把随着业务逻辑变化功能，做成可配置化，上一个新业务，通过配置的方式或少量开发就能支持。 在做客户后台功能时，由于需要展示的数据种类非常多，每种数据展示可能需要花费几天的时间，所以设计了一个通用的技术框架，实现了通过配置化的方式展示各种数据。 写框架解决业务问题我在上家公司经常做一些CRUD的业务功能，我就自己开发了一个快速做CRUD的框架jdbcutil,通过配置实体生成SQL语句，实现了子类只要继承父类，就自动拥有CRUD的能力。后面还写过生成CRUD页面代码的程序。 目前我们团队在做的TITAN框架通过模块化开发的方式，解决易变的业务系统在多人开发时遇到的问题。 技术驱动业务在业务团队，一定要不断的思考如何利用技术来支持快速支持业务，配置化是一种思路，但是有些功能配置复杂度比较高，配置加验证的工作量，可能需要一个星期的时间，那么能不能减少人工配置，实现系统自动化配置，于是可以研究下人工智能，通过人工智能的方式实现，系统告诉人需要配置哪些东西，然后交给人来进行确认，这样可以大大减少人工成本，更快的支持业务。 转载自并发编程网:http://ifeve.com/","categories":[{"name":"读书","slug":"读书","permalink":"http://github.com/b2stry/categories/读书/"}],"tags":[{"name":"读书","slug":"读书","permalink":"http://github.com/b2stry/tags/读书/"}]},{"title":"Quartz","slug":"Quartz","date":"2018-08-08T12:06:28.000Z","updated":"2018-10-09T01:35:37.650Z","comments":true,"path":"2018/08/08/Quartz/","link":"","permalink":"http://github.com/b2stry/2018/08/08/Quartz/","excerpt":"一、分类 从实现的技术上来分，主要有三种： 1) Java自带的java.util.Timer类，这个类允许调度一个java.util.TimerTask任务。使用这种方式可以让程序按照某一个周期执行，但不能在指定时间运行。一般用的较少。 2) 使用Quartz，这是一个功能比较强大的的调度器，可以让程序在指定时间执行，也可以按照某一个周期执行，配置起来会稍微复杂一些。 3) Spring3.0以后，Spring官方自主开发了一款定时任务工具–Spring Task，可以将它看作一个轻量级的Quartz，而且使用起来很简单，除了Spring相关的包外不需要额外的包，而且支持注解和XML配置两种形式。","text":"一、分类 从实现的技术上来分，主要有三种： 1) Java自带的java.util.Timer类，这个类允许调度一个java.util.TimerTask任务。使用这种方式可以让程序按照某一个周期执行，但不能在指定时间运行。一般用的较少。 2) 使用Quartz，这是一个功能比较强大的的调度器，可以让程序在指定时间执行，也可以按照某一个周期执行，配置起来会稍微复杂一些。 3) Spring3.0以后，Spring官方自主开发了一款定时任务工具–Spring Task，可以将它看作一个轻量级的Quartz，而且使用起来很简单，除了Spring相关的包外不需要额外的包，而且支持注解和XML配置两种形式。 二、Quartz和Spring Task对比 实现：Task注解实现方式，比较简单。Quartz需要手动配置Jobs。 任务执行：Task默认单线程串行执行任务，多任务时若某个任务执行时间过长，后续任务会无法及时执行。Quartz采用多线程，无这个问题。 调度：Task采用顺序执行，若当前调度占用时间过长，下一个调度无法及时执行；Quartz采用异步，下一个调度时间到达时，会另一个线程执行调度，不会发生阻塞问题。 部署：Quartz可以采用集群方式，分布式部署到多台机器，分配执行定时任务。 三、QuartzQuartz最核心的三个概念分别是： 任务(Job)、触发器(Trigger)和调度器(Scheduler)。 Job&amp;JobDetailJob定义：实现业务逻辑的任务接口，Job接口非常容易实现的，只有一个execute方法，在里面编写业务逻辑。123public interface Job &#123; public void execute(JobExecutionContext context) throws JobExcutionException;&#125; 自定义Job实现类：123456789public class HelloJob implements Job &#123; public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException &#123; // 打印当前的执行时间，格式为2018-08-08 08:08:08 SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); System.out.println(\"Current Exec Time is:\" + sdf.format(System.currentTimeMillis())); //编写具体的业务逻辑 System.out.println(\"Hello World!\"); &#125;&#125; Job实例在Quartz中的生命周期：每次调度器执行job时，它在调用execute方法前会创建一个新的job实例。当调用完成后，job对象实例会被释放，释放的实例会被垃圾回收机制回收。 JobDetail用来存储特定Job实例的状态信息，调度器需要借助JobDetail对象来添加Job实例。简单来说就是用来绑定Job，会携带一些Job没有携带但又需要的信息。1234//创建一个JobDetail实例，将该实例与HelloJob Class绑定JobDetail jobDetail = JobBuilder.newJob(HelloJob.class) .withIdentity(\"myJob\", \"group1\") .build(); TriggerTrigger是什么？用来告诉调度程序 Job什么时候触发，即Trigger对象是用来触发执行Job的。 Trigger主要分为两类：SimpleTrigger和CronTrigger。 SimpleTrigger的作用：在一个指定时间段内执行一次作业任务或是在指定的时间间隔内多次执行作业任务。 CronTrigger的作用：基于日历的作业调度器，而不是像SimpleTrigger那样精确指定间隔时间，比SimpleTrigger更常用。 123456//创建一个Trigger实例，定义该job立即执行，并且每隔两秒钟执行一次Trigger trigger = TriggerBuilder.newTrigger() .withIdentity(\"myTrigger\", \"group1\") .startNow() .withSchedule(CronScheduleBuilder.cronSchedule(\"0/2 * * * * ? \")) .build(); cron表达式cron表达式的格式如下： [秒] [分] [时] [日期（具体哪天）] [月] [星期] 在spring 4.x中已经不支持7个参数的cron表达式了，要求必须是6个参数。 字 段 是否必填 允许值和允许特殊字符 秒 是 允许的值范围是0-59，支持的特殊符号包括 , - * /，,表示特定的某一秒才会触发任务，-表示一段时间内会触发任务，*表示每一秒都会触发，/表示从哪一个时刻开始，每隔多长时间触发一次任务。 分 是 允许的值范围是0-59，支持的特殊符号和秒一样。 时 是 允许的值范围是0-23，支持的特殊符号和秒一样。 日期 是 允许的值范围是1-31，支持的特殊符号相比秒多了?，表示与{星期}互斥，即意味着若明确指定{星期}触发，则表示{日期}无意义，以免引起冲突和混乱。 月 是 允许的值范围是1-12(JAN-DEC)，支持的特殊符号与秒一样。 星期 是 允许值范围是1~7(SUN-SAT)，1代表星期天（一个星期的第一天），以此类推，7代表星期六，支持的符号相比秒多了?，表达的含义是与{日期}互斥，即意味着若明确指定{日期}触发，则表示{星期}无意义。 cron表达式举例：12345678930 * * * * ? 每分钟的第30秒触发一次任务0 0 10,14,16 * * ? 每天上午10点，下午2点，4点 0 15 10 ? * * 每天10点15分触发0 0/5 14 * * ? 每天下午的2点到2点59分(整点开始，每隔5分触发)0 12 10 ? * MON-FRI 从周一到周五每天上午10点15分触发 Scheduler调度器是用来定期定频率执行任务的。所有的Scheduler实例是由SchedulerFactory来创建的。SchedulerFactory对象通过调用getScheduler方法就能创建和初始化调度器对象。 12345//创建一个Scheduler实例SchedulerFactory sfact = new StdSchedulerFactory();Scheduler scheduler = sfact.getScheduler();scheduler.scheduleJob(jobDetail, trigger);scheduler.start(); 四、Spring&amp;Quartz除了Spring基本依赖之外还需要加入以下依赖：12345678910111 &lt;dependency&gt; 2 &lt;groupId&gt;org.springframework&lt;/groupId&gt; 3 &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; 4 &lt;version&gt;4.2.6.RELEASE&lt;/version&gt; 5 &lt;/dependency&gt; 6 &lt;dependency&gt; 7 &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; 8 &lt;artifactId&gt;quartz&lt;/artifactId&gt; 9 &lt;version&gt;2.2.1&lt;/version&gt;10 &lt;/dependency&gt; 基于特定基类1. 编写任务类，该类需要继承自QuartzJobBean1234567891011121314151617181920212223package com.shallowan.quartz; import org.quartz.JobExecutionContext;import org.quartz.JobExecutionException;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.scheduling.quartz.QuartzJobBean; public class SecondCron extends QuartzJobBean &#123; private static final Logger logger = LoggerFactory.getLogger(SecondCron.class); private Integer timeout; //调度工厂实例化后，经过timeout时间开始执行调度 public void setTimeout(Integer timeout) &#123; this.timeout = timeout; &#125; @Override protected void executeInternal(JobExecutionContext jobExecutionContext) throws JobExecutionException &#123; logger.info(\"定时任务2进行中.......\"); // do something else &#125; &#125; 2.在Spring配置文件中配置作业类JobDetailFactoryBean、作业调度的触发方式（触发器）、调度工厂12345678910111213141516171819202122232425262728293031323334&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:task=\"http://www.springframework.org/schema/task\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.2.xsd http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task.xsd\"&gt; &lt;bean name=\"secondCron\" class=\"org.springframework.scheduling.quartz.JobDetailFactoryBean\"&gt; &lt;property name=\"jobClass\" value=\"com.shallowan.quartz.SecondCron\"/&gt; &lt;/bean&gt; &lt;!--按照一定频率的触发器--&gt; &lt;!--&lt;bean id=\"simpleTrigger\" class=\"org.springframework.scheduling.quartz.SimpleTriggerFactoryBean\"&gt; &lt;property name=\"jobDetail\" ref=\"secondCron\"/&gt; &lt;property name=\"startDelay\" value=\"0\"/&gt; &lt;property name=\"repeatInterval\" value=\"2000\"/&gt; &lt;/bean&gt;--&gt; &lt;!--Cron表达式触发器--&gt; &lt;bean id=\"cronTrigger\" class=\"org.springframework.scheduling.quartz.CronTriggerFactoryBean\"&gt; &lt;property name=\"jobDetail\" ref=\"secondCron\"/&gt; &lt;property name=\"cronExpression\" value=\"0/5 * * * * ?\"/&gt; &lt;/bean&gt; &lt;!--配置调度工厂--&gt; &lt;bean class=\"org.springframework.scheduling.quartz.SchedulerFactoryBean\"&gt; &lt;property name=\"triggers\"&gt; &lt;list&gt; &lt;!--&lt;ref bean=\"simpleTrigger\"/&gt;--&gt; &lt;ref bean=\"cronTrigger\"/&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 不基于特定的基类1.编写任务调度类1234567891011121314package com.shallowan.quartz;import org.slf4j.Logger;import org.slf4j.LoggerFactory; /** * 基于Spring整合Quartz进行完成定时任务 */public class ThirdCron &#123; private static final Logger logger = LoggerFactory.getLogger(ThirdCron.class); public void executeJob() &#123; logger.info(\"定时任务3进行中.......\"); // do something else &#125;&#125; 2.在spring配置文件中配置作业类MethodInvokingJobDetailFactoryBean、作业调度的触发方式（触发器）、调度工厂12345678910111213141516171819202122232425262728293031323334353637&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.2.xsd\"&gt; &lt;bean name=\"thirdCron\" class=\"org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean\"&gt; &lt;property name=\"targetObject\"&gt; &lt;bean class=\"com.shallowan.quartz.ThirdCron\"/&gt; &lt;/property&gt; &lt;property name=\"targetMethod\" value=\"executeJob\"/&gt; &lt;!--作业不并发调度--&gt; &lt;property name=\"concurrent\" value=\"false\"/&gt; &lt;/bean&gt; &lt;!--按照一定频率的触发器--&gt; &lt;!--&lt;bean id=\"simpleTrigger\" class=\"org.springframework.scheduling.quartz.SimpleTriggerFactoryBean\"&gt; &lt;property name=\"jobDetail\" ref=\"thirdCron\"/&gt; &lt;property name=\"startDelay\" value=\"0\"/&gt; &lt;property name=\"repeatInterval\" value=\"2000\"/&gt; &lt;/bean&gt;--&gt; &lt;!--Cron表达式触发器--&gt; &lt;bean id=\"cronTrigger\" class=\"org.springframework.scheduling.quartz.CronTriggerFactoryBean\"&gt; &lt;property name=\"jobDetail\" ref=\"thirdCron\"/&gt; &lt;property name=\"cronExpression\" value=\"0/5 * * * * ?\"/&gt; &lt;/bean&gt; &lt;!--配置调度工厂--&gt; &lt;bean class=\"org.springframework.scheduling.quartz.SchedulerFactoryBean\"&gt; &lt;property name=\"triggers\"&gt; &lt;list&gt; &lt;!--&lt;ref bean=\"simpleTrigger\"/&gt;--&gt; &lt;ref bean=\"cronTrigger\"/&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; Quartz属于重量级的定时任务框架，一般都会选择轻量级的如Spring Task定时任务进行开发，但是遇到比较棘手的，这也是一种解决问题的方式。","categories":[{"name":"Quartz","slug":"Quartz","permalink":"http://github.com/b2stry/categories/Quartz/"}],"tags":[{"name":"Quartz","slug":"Quartz","permalink":"http://github.com/b2stry/tags/Quartz/"}]},{"title":"Navicat常用快捷键及注意事项","slug":"Navicat常用快捷键及注意事项","date":"2018-07-26T06:39:36.000Z","updated":"2018-07-29T10:14:28.201Z","comments":true,"path":"2018/07/26/Navicat常用快捷键及注意事项/","link":"","permalink":"http://github.com/b2stry/2018/07/26/Navicat常用快捷键及注意事项/","excerpt":"常用快捷键： ctrl + q: 打开新查询窗口 ctrl + r: 运行当前窗口内的所有语句 ctrl + w: 关闭当前窗口 F6: 打开一个mysql命令行窗口","text":"常用快捷键： ctrl + q: 打开新查询窗口 ctrl + r: 运行当前窗口内的所有语句 ctrl + w: 关闭当前窗口 F6: 打开一个mysql命令行窗口 —————————以下是然并卵的快捷键—————————————- ctrl + n: 打开新查询窗口 ctrl + shit + r: 只运行选中的语句 ctrl + /: 注释 ctrl + shift + /: 取消注释 ctrl + l: 删除一行 F7: 运行从光标当前位置开始的一条完整sql语句 ctrl + d: 在表数据窗口上查看表定义 注意事项： 对于习惯了SSMS的F5的人来说，ctrl+shift+r是一个成事不足，败事有余的快捷键。最近要在客户数据库上执行一条复杂的update，我习惯性的选中update语句中的一个子查询准备预先查看一下将要更新的结果集是否正确，于是把F5在大脑里映射为ctrl+r, 然后按下去了，结果……update直接执行了。这才发现，ctrl+r会永远执行当前窗口的所有语句，它才不理会你是否只是选中其中一部分。当时我背后一阵发凉，如果窗口里还写了其他update,甚至delete语句，我岂不是要悲剧！！！ 后来用了几次ctrl+shift+r,觉得不好用，万一shift按轻了，不知道会捅出什么篓子来，索性还是用鼠标右键来执行选中语句比较保险。 原文链接：[Navicat] 常用快捷键及注意事项","categories":[{"name":"navicat","slug":"navicat","permalink":"http://github.com/b2stry/categories/navicat/"}],"tags":[{"name":"navicat","slug":"navicat","permalink":"http://github.com/b2stry/tags/navicat/"}]},{"title":"不止代码","slug":"不止代码","date":"2018-07-11T07:58:12.000Z","updated":"2018-07-11T07:59:52.895Z","comments":true,"path":"2018/07/11/不止代码/","link":"","permalink":"http://github.com/b2stry/2018/07/11/不止代码/","excerpt":"","text":"阿里爸爸又出了一本造福广大码农的书《不止代码》，下载的看了下，还是挺有收获的，可以点击此链接直接下载：《不止代码》，这是官方提供的下载链接，如果失效了，可以留言，我可以分享下。","categories":[{"name":"不止代码","slug":"不止代码","permalink":"http://github.com/b2stry/categories/不止代码/"}],"tags":[{"name":"不止代码","slug":"不止代码","permalink":"http://github.com/b2stry/tags/不止代码/"}]},{"title":"远程通信的几种选择","slug":"远程通信的几种选择","date":"2018-03-21T14:19:26.000Z","updated":"2018-03-21T14:32:24.485Z","comments":true,"path":"2018/03/21/远程通信的几种选择/","link":"","permalink":"http://github.com/b2stry/2018/03/21/远程通信的几种选择/","excerpt":"RPC，Webservice，RMI，JMS的区别","text":"RPC，Webservice，RMI，JMS的区别 RPC（Remote Procedure Call Protocol）RPC使用C/S方式，采用http协议,发送请求到服务器，等待服务器返回结果。这个请求包括一个参数集和一个文本集，通常形成“classname.methodname”形式。优点是跨语言跨平台，C端、S端有更大的独立性，缺点是不支持对象，无法在编译器检查错误，只能在运行期检查。 Web ServiceWeb Service提供的服务是基于web容器的，底层使用http协议，类似一个远程的服务提供者，比如天气预报服务，对各地客户端提供天气预报，是一种请求应答的机制，是跨系统跨平台的。就是通过一个servlet，提供服务出去。 首先客户端从服务器的到WebService的WSDL，同时在客户端声称一个代理类(Proxy Class) 这个代理类负责与WebService服务器进行Request 和Response当一个数据（XML格式的）被封装成SOAP格式的数据流发送到服务器端的时候，就会生成一个进程对象并且把接收到这个Request的SOAP包进行解析，然后对事物进行处理，处理结束以后再对这个计算结果进行SOAP包装，然后把这个包作为一个Response发送给客户端的代理类(Proxy Class)，同样地，这个代理类也对这个SOAP包进行解析处理，继而进行后续操作。这就是WebService的一个运行过程。 Web Service大体上分为5个层次: Http传输信道 XML的数据格式 SOAP封装格式 WSDL的描述方式 UDDI UDDI是一种目录服务，企业可以使用它对Webservices进行注册和搜索 RMI （Remote Method Invocation）RMI 采用stubs 和 skeletons 来进行远程对象(remote object)的通讯。stub 充当远程对象的客户端代理，有着和远程对象相同的远程接口，远程对象的调用实际是通过调用该对象的客户端代理对象stub来完成的，通过该机制RMI就好比它是本地工作，采用tcp/ip协议，客户端直接调用服务端上的一些方法。优点是强类型，编译期可检查错误，缺点是只能基于JAVA语言，客户机与服务器紧耦合。 JMS（Java Messaging Service）JMS是Java的消息服务，JMS的客户端之间可以通过JMS服务进行异步的消息传输。JMS支持两种消息模型：Point-to-Point（P2P）和Publish/Subscribe（Pub/Sub），即点对点和发布订阅模型。 几者的区别与联系1、RPC与RMI1) RPC 跨语言，而 RMI只支持Java。 2) RMI 调用远程对象方法，允许方法返回 Java 对象以及基本数据类型，而RPC 不支持对象的概念，传送到 RPC 服务的消息由外部数据表示 (External Data Representation, XDR) 语言表示，这种语言抽象了字节序类和数据类型结构之间的差异。只有由 XDR 定义的数据类型才能被传递， 可以说 RMI 是面向对象方式的 Java RPC 。 3) 在方法调用上，RMI中，远程接口使每个远程方法都具有方法签名。如果一个方法在服务器上执行，但是没有相匹配的签名被添加到这个远程接口上，那么这个新方法就不能被RMI客户方所调用。 在RPC中，当一个请求到达RPC服务器时，这个请求就包含了一个参数集和一个文本值，通常形成“classname.methodname”的形式。这就向RPC服务器表明，被请求的方法在为 “classname”的类中，名叫“methodname”。然后RPC服务器就去搜索与之相匹配的类和方法，并把它作为那种方法参数类型的输入。这里的参数类型是与RPC请求中的类型是匹配的。一旦匹配成功，这个方法就被调用了，其结果被编码后返回客户方。 2、JMS和RMI 采用JMS 服务，对象是在物理上被异步从网络的某个JVM 上直接移动到另一个JVM 上（是消息通知机制）,而RMI 对象是绑定在本地JVM 中，只有函数参数和返回值是通过网络传送的（是请求应答机制）。 RMI一般都是同步的，也就是说，当client调用Server的一个方法的时候，需要等到对方的返回，才能继续执行client端，这个过程调用本地方法感觉上是一样的，这也是RMI的一个特点。JMS 一般只是一个点发出一个Message到Message Server,发出之后一般不会关心谁用了这个message。所以，一般RMI的应用是紧耦合，JMS的应用相对来说是松散耦合应用。 3、Webservice与RMI RMI是在tcp协议上传递可序列化的java对象，只能用在java虚拟机上，绑定语言，客户端和服务端都必须是java webservice没有这个限制，webservice是在http协议上传递xml文本文件，与语言和平台无关 4、Webservice与JMSWebservice专注于远程服务调用，jms专注于信息交换。 大多数情况下Webservice是两系统间的直接交互（Consumer &lt;–&gt; Producer），而大多数情况下jms是三方系统交互（Consumer &lt;- Broker -&gt; Producer）。当然，JMS也可以实现request-response模式的通信，只要Consumer或Producer其中一方兼任broker即可。 JMS可以做到异步调用完全隔离了客户端和服务提供者，能够抵御流量洪峰； WebService服务通常为同步调用，需要有复杂的对象转换，相比SOAP，现在JSON，rest都是很好的http架构方案；（举一个例子，电子商务的分布式系统中，有支付系统和业务系统，支付系统负责用户付款，在用户在银行付款后需要通知各个业务系统，那么这个时候，既可以用同步也可以用异步，使用异步的好处就能抵御网站暂时的流量高峰，或者能应对慢消费者。） JMS是java平台上的消息规范。一般jms消息不是一个xml，而是一个java对象，很明显，jms没考虑异构系统，说白了，JMS就没考虑非java的东西。但是好在现在大多数的jms provider（就是JMS的各种实现产品）都解决了异构问题。相比WebService的跨平台各有千秋吧。","categories":[{"name":"远程通信","slug":"远程通信","permalink":"http://github.com/b2stry/categories/远程通信/"}],"tags":[{"name":"远程通信","slug":"远程通信","permalink":"http://github.com/b2stry/tags/远程通信/"}]},{"title":"Tomcat 性能调优","slug":"Tomcat-性能调优","date":"2018-03-17T14:53:21.000Z","updated":"2018-03-17T15:47:01.797Z","comments":true,"path":"2018/03/17/Tomcat-性能调优/","link":"","permalink":"http://github.com/b2stry/2018/03/17/Tomcat-性能调优/","excerpt":"Tomcat 的缺省配置是不能稳定长期运行的，也就是不适合生产环境，它会死机，让你不断重新启动，甚至在午夜时分唤醒你。对于操作系统优化来说，是尽可能的增大可使用的内存容量、提高CPU 的频率，保证文件系统的读写速率等。经过压力测试验证，在并发连接很多的情况下，CPU 的处理能力越强，系统运行速度越快。","text":"Tomcat 的缺省配置是不能稳定长期运行的，也就是不适合生产环境，它会死机，让你不断重新启动，甚至在午夜时分唤醒你。对于操作系统优化来说，是尽可能的增大可使用的内存容量、提高CPU 的频率，保证文件系统的读写速率等。经过压力测试验证，在并发连接很多的情况下，CPU 的处理能力越强，系统运行速度越快。Tomcat 的优化不像其它软件那样，简简单单的修改几个参数就可以了，它的优化主要有三方面，分为系统优化，Tomcat 本身的优化，Java 虚拟机（JVM）调优。系统优化就不在介绍了，接下来就详细的介绍一下 Tomcat 本身与 JVM 优化，以 Tomcat 7 为例。 一、Tomcat 本身优化Tomcat 的自身参数的优化，这块很像 ApacheHttp Server。修改一下 xml 配置文件中的参数，调整最大连接数，超时等。此外，我们安装 Tomcat 是，优化就已经开始了。 1、工作方式选择为了提升性能，首先就要对代码进行动静分离，让 Tomcat 只负责 jsp 文件的解析工作。如采用 Apache 和 Tomcat 的整合方式，他们之间的连接方案有三种选择，JK、http_proxy 和 ajp_proxy。相对于 JK 的连接方式，后两种在配置上比较简单的，灵活性方面也一点都不逊色。但就稳定性而言不像JK 这样久经考验，所以建议采用 JK 的连接方式。 2、Connector 连接器的配置之前文件介绍过的 Tomcat 连接器的三种方式： bio、nio 和 apr，三种方式性能差别很大，apr 的性能最优， bio 的性能最差。而 Tomcat 7 使用的 Connector 默认就启用的 Apr 协议，但需要系统安装 Apr 库，否则就会使用 bio 方式。 3、配置文件优化配置文件优化其实就是对 server.xml 优化，可以提大大提高 Tomcat 的处理请求的能力，下面我们来看 Tomcat 容器内的优化。 默认配置下，Tomcat 会为每个连接器创建一个绑定的线程池（最大线程数 200），服务启动时，默认创建了 5 个空闲线程随时等待用户请求。 首先，打开 ${TOMCAT_HOME}/conf/server.xml，搜索【&lt;Executor name=&quot;tomcatThreadPool&quot;】，开启并调整为123456&lt;Executor name=\"tomcatThreadPool\" namePrefix=\"catalina-exec-\" maxThreads=\"500\" minSpareThreads=\"20\" maxSpareThreads=\"50\" maxIdleTime=\"60000\"/&gt; 注意， Tomcat 7 在开启线程池前，一定要安装好 Apr 库，并可以启用，否则会有错误报出，shutdown.sh 脚本无法关闭进程。 然后，修改&lt;Connector …&gt;节点，增加 executor 属性，搜索【port=&quot;8080&quot;】，调整为1234567891011121314&lt;Connector executor=\"tomcatThreadPool\" port=\"8080\" protocol=\"HTTP/1.1\" URIEncoding=\"UTF-8\" connectionTimeout=\"30000\" enableLookups=\"false\" disableUploadTimeout=\"false\" connectionUploadTimeout=\"150000\" acceptCount=\"300\" keepAliveTimeout=\"120000\" maxKeepAliveRequests=\"1\" compression=\"on\" compressionMinSize=\"2048\" compressableMimeType=\"text/html,text/xml,text/javascript,text/css,text/plain,image/gif,image/jpg,image/png\" redirectPort=\"8443\" /&gt; maxThreads :Tomcat 使用线程来处理接收的每个请求，这个值表示 Tomcat 可创建的最大的线程数，默认值是 200 minSpareThreads：最小空闲线程数，Tomcat 启动时的初始化的线程数，表示即使没有人使用也开这么多空线程等待，默认值是 10。 maxSpareThreads：最大备用线程数，一旦创建的线程超过这个值，Tomcat 就会关闭不再需要的 socket 线程。 上边配置的参数，最大线程 500（一般服务器足以），要根据自己的实际情况合理设置，设置越大会耗费内存和 CPU，因为 CPU 疲于线程上下文切换，没有精力提供请求服务了，最小空闲线程数 20，线程最大空闲时间 60 秒，当然允许的最大线程连接数还受制于操作系统的内核参数设置，设置多大要根据自己的需求与环境。当然线程可以配置在“tomcatThreadPool”中，也可以直接配置在“Connector”中，但不可以重复配置。 URIEncoding：指定 Tomcat 容器的 URL 编码格式，语言编码格式这块倒不如其它 WEB 服务器软件配置方便，需要分别指定。 connnectionTimeout： 网络连接超时，单位：毫秒，设置为 0 表示永不超时，这样设置有隐患的。通常可设置为 30000 毫秒，可根据检测实际情况，适当修改。 enableLookups： 是否反查域名，以返回远程主机的主机名，取值为：true 或 false，如果设置为false，则直接返回IP地址，为了提高处理能力，应设置为 false。 disableUploadTimeout：上传时是否使用超时机制。 connectionUploadTimeout：上传超时时间，毕竟文件上传可能需要消耗更多的时间，这个根据你自己的业务需要自己调，以使Servlet有较长的时间来完成它的执行，需要与上一个参数一起配合使用才会生效。 acceptCount：指定当所有可以使用的处理请求的线程数都被使用时，可传入连接请求的最大队列长度，超过这个数的请求将不予处理，默认为100个。 keepAliveTimeout：长连接最大保持时间（毫秒），表示在下次请求过来之前，Tomcat 保持该连接多久，默认是使用 connectionTimeout 时间，-1 为不限制超时。 maxKeepAliveRequests：表示在服务器关闭之前，该连接最大支持的请求数。超过该请求数的连接也将被关闭，1表示禁用，-1表示不限制个数，默认100个，一般设置在100~200之间。 compression：是否对响应的数据进行 GZIP 压缩，off：表示禁止压缩；on：表示允许压缩（文本将被压缩）、force：表示所有情况下都进行压缩，默认值为off，压缩数据后可以有效的减少页面的大小，一般可以减小1/3左右，节省带宽。 compressionMinSize：表示压缩响应的最小值，只有当响应报文大小大于这个值的时候才会对报文进行压缩，如果开启了压缩功能，默认值就是2048。 compressableMimeType：压缩类型，指定对哪些类型的文件进行数据压缩。 noCompressionUserAgents=”gozilla, traviata”： 对于以下的浏览器，不启用压缩。 如果已经对代码进行了动静分离，静态页面和图片等数据就不需要 Tomcat 处理了，那么也就不需要配置在 Tomcat 中配置压缩了。 以上是一些常用的配置参数属性，当然还有好多其它的参数设置，还可以继续深入的优化，HTTP Connector 与 AJP Connector 的参数属性值，可以参考官方文档的详细说明： https://tomcat.apache.org/tomcat-7.0-doc/config/http.html https://tomcat.apache.org/tomcat-7.0-doc/config/ajp.html 二、JVM 优化Tomcat 启动命令行中的优化参数，就是 JVM 的优化 。Tomcat 首先跑在 JVM 之上的，因为它的启动其实也只是一个 java 命令行，首先我们需要对这个 JAVA 的启动命令行进行调优。不管是 YGC 还是 Full GC，GC 过程中都会对导致程序运行中中断，正确的选择不同的 GC 策略，调整 JVM、GC 的参数，可以极大的减少由于 GC 工作，而导致的程序运行中断方面的问题，进而适当的提高 Java 程序的工作效率。但是调整 GC 是以个极为复杂的过程，由于各个程序具备不同的特点，如：web 和 GUI 程序就有很大区别（Web可以适当的停顿，但GUI停顿是客户无法接受的），而且由于跑在各个机器上的配置不同（主要 cup 个数，内存不同），所以使用的 GC 种类也会不同。 1、JVM 参数配置方法Tomcat 的启动参数位于安装目录 ${JAVA_HOME}/bin目录下，Linux 操作系统就是 catalina.sh 文件。JAVA_OPTS，就是用来设置 JVM 相关运行参数的变量，还可以在 CATALINA_OPTS 变量中设置。关于这 2 个变量，还是多少有些区别的： JAVA_OPTS：用于当 Java 运行时选项“start”、“stop”或“run”命令执行。 CATALINA_OPTS：用于当 Java 运行时选项“start”或“run”命令执行。 为什么有两个不同的变量？它们之间都有什么区别呢？ 首先，在启动 Tomcat 时，任何指定变量的传递方式都是相同的，可以传递到执行“start”或“run”命令中，但只有设定在 JAVA_OPTS 变量里的参数被传递到“stop”命令中。对于 Tomcat 运行过程，可能没什么区别，影响的是结束程序，而不是启动程序。 第二个区别是更微妙，其他应用程序也可以使用 JAVA_OPTS 变量，但只有在 Tomcat 中使用 CATALINA_OPTS 变量。如果你设置环境变量为只使用 Tomcat，最好你会建议使用 CATALINA_OPTS 变量，而如果你设置环境变量使用其它的 Java 应用程序，例如 JBoss，你应该把你的设置放在JAVA_OPTS 变量中。 2、JVM 参数属性32 位系统下 JVM 对内存的限制：不能突破 2GB ，那么这时你的 Tomcat 要优化，就要讲究点技巧了，而在 64 位操作系统上无论是系统内存还是 JVM 都没有受到 2GB 这样的限制。 针对于 JMX 远程监控也是在这里设置，以下为 64 位系统环境下的配置，内存加入的参数如下：12345678910111213141516171819202122CATALINA_OPTS=\"-server -Xms6000M -Xmx6000M -Xss512k -XX:NewSize=2250M -XX:MaxNewSize=2250M -XX:PermSize=128M-XX:MaxPermSize=256M -XX:+AggressiveOpts -XX:+UseBiasedLocking -XX:+DisableExplicitGC -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:MaxTenuringThreshold=31 -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:LargePageSizeInBytes=128m -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly-Duser.timezone=Asia/Shanghai -Djava.awt.headless=true\" 为了看着方便，将每个参数单独写一行。上面参数好多啊，可能有人写到现在都没见过一个在 Tomcat 的启动命令里加了这么多参数，当然，这些参数只是我机器上的，不一定适合你，尤其是参数后的 value（值）是需要根据你自己的实际情况来设置的。 上述这样的配置，基本上可以达到： 系统响应时间增快； JVM回收速度增快同时又不影响系统的响应率； JVM内存最大化利用； 线程阻塞情况最小化。 JVM 常用参数详解： -server：一定要作为第一个参数，在多个 CPU 时性能佳，还有一种叫 -client 的模式，特点是启动速度比较快，但运行时性能和内存管理效率不高，通常用于客户端应用程序或开发调试，在 32 位环境下直接运行 Java 程序默认启用该模式。Server 模式的特点是启动速度比较慢，但运行时性能和内存管理效率很高，适用于生产环境，在具有 64 位能力的 JDK 环境下默认启用该模式，可以不配置该参数。 -Xms：表示 Java 初始化堆的大小，-Xms 与-Xmx 设成一样的值，避免 JVM 反复重新申请内存，导致性能大起大落，默认值为物理内存的 1/64，默认（MinHeapFreeRatio参数可以调整）空余堆内存小于 40% 时，JVM 就会增大堆直到 -Xmx 的最大限制。 -Xmx：表示最大 Java 堆大小，当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃，因此一般建议堆的最大值设置为可用内存的最大值的80%。如何知道我的 JVM 能够使用最大值，使用 java -Xmx512M -version 命令来进行测试，然后逐渐的增大 512 的值,如果执行正常就表示指定的内存大小可用，否则会打印错误信息，默认值为物理内存的 1/4，默认（MinHeapFreeRatio参数可以调整）空余堆内存大于 70% 时，JVM 会减少堆直到-Xms 的最小限制。 -Xss：表示每个 Java 线程堆栈大小，JDK 5.0 以后每个线程堆栈大小为 1M，以前每个线程堆栈大小为 256K。根据应用的线程所需内存大小进行调整，在相同物理内存下，减小这个值能生成更多的线程，但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在 3000~5000 左右。一般小的应用， 如果栈不是很深， 应该是128k 够用的，大的应用建议使用 256k 或 512K，一般不易设置超过 1M，要不然容易出现out ofmemory。这个选项对性能影响比较大，需要严格的测试。 -XX:NewSize：设置新生代内存大小。 -XX:MaxNewSize：设置最大新生代新生代内存大小 -XX:PermSize：设置持久代内存大小 -XX:MaxPermSize：设置最大值持久代内存大小，永久代不属于堆内存，堆内存只包含新生代和老年代。 -XX:+AggressiveOpts：作用如其名（aggressive），启用这个参数，则每当 JDK 版本升级时，你的 JVM 都会使用最新加入的优化技术（如果有的话）。 -XX:+UseBiasedLocking：启用一个优化了的线程锁，我们知道在我们的appserver，每个http请求就是一个线程，有的请求短有的请求长，就会有请求排队的现象，甚至还会出现线程阻塞，这个优化了的线程锁使得你的appserver内对线程处理自动进行最优调配。 -XX:+DisableExplicitGC：在 程序代码中不允许有显示的调用“System.gc()”。每次在到操作结束时手动调用 System.gc() 一下，付出的代价就是系统响应时间严重降低，就和关于 Xms，Xmx 里的解释的原理一样，这样去调用 GC 导致系统的 JVM 大起大落。 -XX:+UseConcMarkSweepGC：设置年老代为并发收集，即 CMS gc，这一特性只有 jdk1.5后续版本才具有的功能，它使用的是 gc 估算触发和 heap 占用触发。我们知道频频繁的 GC 会造面 JVM的大起大落从而影响到系统的效率，因此使用了 CMS GC 后可以在 GC 次数增多的情况下，每次 GC 的响应时间却很短，比如说使用了 CMSGC 后经过 jprofiler 的观察，GC 被触发次数非常多，而每次 GC 耗时仅为几毫秒。 -XX:+UseParNewGC：对新生代采用多线程并行回收，这样收得快，注意最新的 JVM 版本，当使用 -XX:+UseConcMarkSweepGC 时，-XX:UseParNewGC 会自动开启。因此，如果年轻代的并行 GC 不想开启，可以通过设置 -XX：-UseParNewGC 来关掉。 -XX:MaxTenuringThreshold：设置垃圾最大年龄。如果设置为0的话，则新生代对象不经过 Survivor 区，直接进入老年代。对于老年代比较多的应用（需要大量常驻内存的应用），可以提高效率。如果将此值设置为一 个较大值，则新生代对象会在 Survivor 区进行多次复制，这样可以增加对象在新生代的存活时间，增加在新生代即被回收的概率，减少Full GC的频率，这样做可以在某种程度上提高服务稳定性。该参数只有在串行 GC 时才有效，这个值的设置是根据本地的 jprofiler 监控后得到的一个理想的值，不能一概而论原搬照抄。 -XX:+CMSParallelRemarkEnabled：在使用 UseParNewGC 的情况下，尽量减少 mark 的时间。 -XX:+UseCMSCompactAtFullCollection：在使用 concurrent gc 的情况下，防止 memoryfragmention，对 live object 进行整理，使 memory 碎片减少。 -XX:LargePageSizeInBytes：指定 Java heap 的分页页面大小，内存页的大小不可设置过大， 会影响 Perm 的大小。 -XX:+UseFastAccessorMethods：使用 get，set 方法转成本地代码，原始类型的快速优化。 -XX:+UseCMSInitiatingOccupancyOnly：只有在 oldgeneration 在使用了初始化的比例后 concurrent collector 启动收集。 -Duser.timezone=Asia/Shanghai：设置用户所在时区。 -Djava.awt.headless=true：这个参数一般我们都是放在最后使用的，这全参数的作用是这样的，有时我们会在我们的 J2EE 工程中使用一些图表工具如：jfreechart，用于在 web 网页输出 GIF/JPG 等流，在 winodws 环境下，一般我们的 app server 在输出图形时不会碰到什么问题，但是在linux/unix 环境下经常会碰到一个 exception 导致你在 winodws 开发环境下图片显示的好好可是在 linux/unix 下却显示不出来，因此加上这个参数以免避这样的情况出现。 -Xmn：新生代的内存空间大小，注意：此处的大小是（eden+ 2 survivor space)。与 jmap -heap 中显示的 New gen 是不同的。整个堆大小 = 新生代大小 + 老生代大小 + 永久代大小。在保证堆大小不变的情况下，增大新生代后，将会减小老生代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的 3/8。 -XX:CMSInitiatingOccupancyFraction：当堆满之后，并行收集器便开始进行垃圾收集，例如，当没有足够的空间来容纳新分配或提升的对象。对于 CMS 收集器，长时间等待是不可取的，因为在并发垃圾收集期间应用持续在运行（并且分配对象）。因此，为了在应用程序使用完内存之前完成垃圾收集周期，CMS 收集器要比并行收集器更先启动。因为不同的应用会有不同对象分配模式，JVM 会收集实际的对象分配（和释放）的运行时数据，并且分析这些数据，来决定什么时候启动一次 CMS 垃圾收集周期。这个参数设置有很大技巧，基本上满足(Xmx-Xmn)(100-CMSInitiatingOccupancyFraction)/100 &gt;= Xmn 就不会出现 promotion failed。例如在应用中 Xmx 是6000，Xmn 是 512，那么 Xmx-Xmn 是 5488M，也就是老年代有 5488M，CMSInitiatingOccupancyFraction=90 说明老年代到 90% 满的时候开始执行对老年代的并发垃圾回收（CMS），这时还 剩 10% 的空间是 548810% = 548M，所以即使 Xmn（也就是新生代共512M）里所有对象都搬到老年代里，548M 的空间也足够了，所以只要满足上面的公式，就不会出现垃圾回收时的 promotion failed，因此这个参数的设置必须与 Xmn 关联在一起。 -XX:+CMSIncrementalMode：该标志将开启 CMS 收集器的增量模式。增量模式经常暂停 CMS 过程，以便对应用程序线程作出完全的让步。因此，收集器将花更长的时间完成整个收集周期。因此，只有通过测试后发现正常 CMS 周期对应用程序线程干扰太大时，才应该使用增量模式。由于现代服务器有足够的处理器来适应并发的垃圾收集，所以这种情况发生得很少，用于但 CPU情况。 -XX:NewRatio：年轻代（包括 Eden 和两个 Survivor 区）与年老代的比值（除去持久代），-XX:NewRatio=4 表示年轻代与年老代所占比值为 1:4，年轻代占整个堆栈的 1/5，Xms=Xmx 并且设置了 Xmn 的情况下，该参数不需要进行设置。 -XX:SurvivorRatio：Eden 区与 Survivor 区的大小比值，设置为 8，表示 2 个 Survivor 区（JVM 堆内存年轻代中默认有 2 个大小相等的 Survivor 区）与 1 个 Eden 区的比值为 2:8，即 1 个 Survivor 区占整个年轻代大小的 1/10。 -XX:+UseSerialGC：设置串行收集器。 -XX:+UseParallelGC：设置为并行收集器。此配置仅对年轻代有效。即年轻代使用并行收集，而年老代仍使用串行收集。 -XX:+UseParallelOldGC：配置年老代垃圾收集方式为并行收集，JDK6.0 开始支持对年老代并行收集。 -XX:ConcGCThreads：早期 JVM 版本也叫-XX:ParallelCMSThreads，定义并发 CMS 过程运行时的线程数。比如 value=4 意味着 CMS 周期的所有阶段都以 4 个线程来执行。尽管更多的线程会加快并发 CMS 过程，但其也会带来额外的同步开销。因此，对于特定的应用程序，应该通过测试来判断增加 CMS 线程数是否真的能够带来性能的提升。如果还标志未设置，JVM 会根据并行收集器中的 -XX:ParallelGCThreads 参数的值来计算出默认的并行 CMS 线程数。 -XX:ParallelGCThreads：配置并行收集器的线程数，即：同时有多少个线程一起进行垃圾回收，此值建议配置与 CPU 数目相等。 -XX:OldSize：设置 JVM 启动分配的老年代内存大小，类似于新生代内存的初始大小 -XX:NewSize。 以上就是一些常用的配置参数，有些参数是可以被替代的，配置思路需要考虑的是 Java 提供的垃圾回收机制。虚拟机的堆大小决定了虚拟机花费在收集垃圾上的时间和频度。收集垃圾能够接受的速度和应用有关，应该通过分析实际的垃圾收集的时间和频率来调整。假如堆的大小很大，那么完全垃圾收集就会很慢，但是频度会降低。假如您把堆的大小和内存的需要一致，完全收集就很快，但是会更加频繁。调整堆大小的的目的是最小化垃圾收集的时间，以在特定的时间内最大化处理客户的请求。在基准测试的时候，为确保最好的性能，要把堆的大小设大，确保垃圾收集不在整个基准测试的过程中出现。 假如系统花费很多的时间收集垃圾，请减小堆大小。一次完全的垃圾收集应该不超过 3-5 秒。假如垃圾收集成为瓶颈，那么需要指定代的大小，检查垃圾收集的周详输出，研究垃圾收集参数对性能的影响。当增加处理器时，记得增加内存，因为分配能够并行进行，而垃圾收集不是并行的。 3、设置系统属性之前说过，Tomcat 的语言编码，配置起来很慢，要经过多次设置才可以了，否则中文很有可能出现乱码情况。譬如汉字“中”，以 UTF-8 编码后得到的是 3 字节的值 %E4%B8%AD，然后通过 GET 或者 POST 方式把这 3 个字节提交到 Tomcat 容器，如果你不告诉 Tomcat 我的参数是用 UTF-8编码的，那么 Tomcat 就认为你是用 ISO-8859-1 来编码的，而 ISO8859-1（兼容 URI 中的标准字符集 US-ASCII）是兼容 ASCII 的单字节编码并且使用了单字节内的所有空间，因此 Tomcat 就以为你传递的用 ISO-8859-1 字符集编码过的 3 个字符，然后它就用 ISO-8859-1 来解码。 设置起来不难使用“ -D&lt;名称&gt;=&lt;值&gt; ”来设置系统属性： -Djavax.servlet.request.encoding=UTF-8 -Djavax.servlet.response.encoding=UTF-8 -Dfile.encoding=UTF-8 -Duser.country=CN -Duser.language=zh 4、常见的 Java 内存溢出有以下三种1) java.lang.OutOfMemoryError: Java heap space —-JVM Heap（堆）溢出 JVM 在启动的时候会自动设置 JVM Heap 的值，其初始空间（即-Xms）是物理内存的1/64，最大空间（-Xmx）不可超过物理内存。可以利用 JVM提供的 -Xmn -Xms -Xmx 等选项可进行设置。Heap 的大小是 Young Generation 和 Tenured Generaion 之和。在 JVM 中如果 98％ 的时间是用于 GC，且可用的 Heap size 不足 2％ 的时候将抛出此异常信息。 解决方法：手动设置 JVM Heap（堆）的大小。 2) java.lang.OutOfMemoryError: PermGen space —- PermGen space溢出。 PermGen space 的全称是 Permanent Generation space，是指内存的永久保存区域。为什么会内存溢出，这是由于这块内存主要是被 JVM 存放Class 和 Meta 信息的，Class 在被 Load 的时候被放入 PermGen space 区域，它和存放 Instance 的 Heap 区域不同，sun 的 GC 不会在主程序运行期对 PermGen space 进行清理，所以如果你的 APP 会载入很多 CLASS 的话，就很可能出现 PermGen space 溢出。 解决方法： 手动设置 MaxPermSize 大小 3) java.lang.StackOverflowError —- 栈溢出 栈溢出了，JVM 依然是采用栈式的虚拟机，这个和 C 与 Pascal 都是一样的。函数的调用过程都体现在堆栈和退栈上了。调用构造函数的 “层”太多了，以致于把栈区溢出了。通常来讲，一般栈区远远小于堆区的，因为函数调用过程往往不会多于上千层，而即便每个函数调用需要 1K 的空间（这个大约相当于在一个 C 函数内声明了 256 个 int 类型的变量），那么栈区也不过是需要 1MB 的空间。通常栈的大小是 1－2MB 的。通常递归也不要递归的层次过多，很容易溢出。 解决方法：修改程序。 更多信息，请参考以下文章：JVM 垃圾回收调优总结 JVM调优总结：典型配置举例 JVM基础：JVM参数设置、分析 JVM 堆内存相关的启动参数：年轻代、老年代和永久代的内存分配 Java 虚拟机–新生代与老年代GC JVM（Java虚拟机）优化大全和案例实战 JVM内存区域划分Eden Space、Survivor Space、Tenured Gen，Perm Gen解释","categories":[{"name":"tomcat调优","slug":"tomcat调优","permalink":"http://github.com/b2stry/categories/tomcat调优/"}],"tags":[{"name":"tomcat调优","slug":"tomcat调优","permalink":"http://github.com/b2stry/tags/tomcat调优/"}]},{"title":"Map中的hash()","slug":"Map中的hash","date":"2018-03-13T11:08:46.000Z","updated":"2018-03-13T11:27:09.563Z","comments":true,"path":"2018/03/13/Map中的hash/","link":"","permalink":"http://github.com/b2stry/2018/03/13/Map中的hash/","excerpt":"知道HashMap中hash方法的具体实现吗？ 你知道HashTable、ConcurrentHashMap中hash方法的实现以及原因吗？ 你知道为什么要这么实现吗？ 你知道为什么JDK 7和JDK 8中hash方法实现的不同以及区别吗？ 如果你不能很好的回答这些问题，那么你需要好好看看这篇文章。文中涉及到大量代码和计算机底层原理知识。绝对的干货满满。整个互联网，把hash()分析的如此透彻的，别无二家了。","text":"知道HashMap中hash方法的具体实现吗？ 你知道HashTable、ConcurrentHashMap中hash方法的实现以及原因吗？ 你知道为什么要这么实现吗？ 你知道为什么JDK 7和JDK 8中hash方法实现的不同以及区别吗？ 如果你不能很好的回答这些问题，那么你需要好好看看这篇文章。文中涉及到大量代码和计算机底层原理知识。绝对的干货满满。整个互联网，把hash()分析的如此透彻的，别无二家了。 哈希Hash，一般翻译做“散列”，也有直接音译为“哈希”的，就是把任意长度的输入，通过散列算法，变换成固定长度的输出，该输出就是散列值。这种转换是一种压缩映射，也就是，散列值的空间通常远小于输入的空间，不同的输入可能会散列成相同的输出，所以不可能从散列值来唯一的确定输入值。简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。 根据同一散列函数计算出的散列值如果不同，那么输入值肯定也不同。但是，根据同一散列函数计算出的散列值如果相同，输入值不一定相同。 两个不同的输入值，根据同一散列函数计算出的散列值相同的现象叫做碰撞。 常见的Hash函数有以下几个： 直接定址法：直接以关键字k或者k加上某个常数（k+c）作为哈希地址。 数字分析法：提取关键字中取值比较均匀的数字作为哈希地址。 除留余数法：用关键字k除以某个不大于哈希表长度m的数p，将所得余数作为哈希表地址。 分段叠加法：按照哈希表地址位数将关键字分成位数相等的几部分，其中最后一部分可以比较短。然后将这几部分相加，舍弃最高进位后的结果就是该关键字的哈希地址。 平方取中法：如果关键字各个部分分布都不均匀的话，可以先求出它的平方值，然后按照需求取中间的几位作为哈希地址。 伪随机数法：采用一个伪随机数当作哈希函数。 上面介绍过碰撞。衡量一个哈希函数的好坏的重要指标就是发生碰撞的概率以及发生碰撞的解决方案。任何哈希函数基本都无法彻底避免碰撞，常见的解决碰撞的方法有以下几种： 开放定址法开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。 链地址法将哈希表的每个单元作为链表的头结点，所有哈希地址为i的元素构成一个同义词链表。即发生冲突时就把该关键字链在以该单元为头结点的链表的尾部。 再哈希法当哈希地址发生冲突用其他的函数计算另一个哈希函数地址，直到冲突不再产生为止。 建立公共溢出区将哈希表分为基本表和溢出表两部分，发生冲突的元素都放入溢出表中。 HashMap 的数据结构在Java中，保存数据有两种比较简单的数据结构：数组和链表。数组的特点是：寻址容易，插入和删除困难；而链表的特点是：寻址困难，插入和删除容易。上面我们提到过，常用的哈希函数的冲突解决办法中有一种方法叫做链地址法，其实就是将数组和链表组合在一起，发挥了两者的优势，我们可以将其理解为链表的数组。 我们可以从上图看到，左边很明显是个数组，数组的每个成员是一个链表。该数据结构所容纳的所有元素均包含一个指针，用于元素间的链接。我们根据元素的自身特征把元素分配到不同的链表中去，反过来我们也正是通过这些特征找到正确的链表，再从链表中找出正确的元素。其中，根据元素特征计算元素数组下标的方法就是哈希算法，即本文的主角hash()函数（当然，还包括indexOf()函数）。 hash方法我们拿JDK 1.7的HashMap为例，其中定义了一个final int hash(Object k) 方法，其主要被以下方法引用。 上面的方法主要都是增加和删除方法，这不难理解，当我们要对一个链表数组中的某个元素进行增删的时候，首先要知道他应该保存在这个链表数组中的哪个位置，即他在这个数组中的下标。而hash()方法的功能就是根据Key来定位其在HashMap中的位置。HashTable、ConcurrentHashMap同理。 源码解析首先，在同一个版本的Jdk中，HashMap、HashTable以及ConcurrentHashMap里面的hash方法的实现是不同的。再不同的版本的JDK中（Java7 和 Java8）中也是有区别的。我会尽量全部介绍到。相信，看文这篇文章，你会彻底理解hash方法。 在上代码之前，我们先来做个简单分析。我们知道，hash方法的功能是根据Key来定位这个K-V在链表数组中的位置的。也就是hash方法的输入应该是个Object类型的Key，输出应该是个int类型的数组下标。如果让你设计这个方法，你会怎么做？ 其实简单，我们只要调用Object对象的hashCode()方法，该方法会返回一个整数，然后用这个数对HashMap或者HashTable的容量进行取模就行了。没错，其实基本原理就是这个，只不过，在具体实现上，由两个方法int hash(Object k)和 int indexFor(int h, int length)来实现。但是考虑到效率等问题，HashMap的实现会稍微复杂一点。 hash ：该方法主要是将Object转换成一个整型。 indexFor ：该方法主要是将hash生成的整型转换成链表数组中的下标。 HashMap In Java 71234567891011121314final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125;static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 前面我说过，indexFor方法其实主要是将hash生成的整型转换成链表数组中的下标。那么 return h &amp; (length-1);是什么意思呢？其实，他就是取模。Java之所有使用位运算(&amp;)来代替取模运算(%)，最主要的考虑就是效率。 位运算(&amp;)效率要比代替取模运算(%)高很多，主要原因是位运算直接对内存数据进行操作，不需要转成十进制，因此处理速度非常快。 那么，为什么可以使用位运算(&amp;)来实现取模运算(%)呢？这实现的原理如下： X % 2^n = X &amp; (2^n - 1) 2^n表示2的n次方，也就是说，一个数对2^n取模 == 一个数和(2^n - 1)做按位与运算 。 假设n为3，则2^3 = 8，表示成2进制就是1000。2^3 -1 = 7 ，即0111。 此时X &amp; (2^3 - 1) 就相当于取X的2进制的最后三位数。 从2进制角度来看，X / 8相当于 X &gt;&gt; 3，即把X右移3位，此时得到了X / 8的商，而被移掉的部分(后三位)，则是X % 8，也就是余数。 上面的解释不知道你有没有看懂，没看懂的话其实也没关系，你只需要记住这个技巧就可以了。或者你可以找几个例子试一下。 6 % 8 = 6 ，6 &amp; 7 = 6 10 % 8 = 2 ，10 &amp; 7 = 2 所以，return h &amp; (length-1);只要保证length的长度是 2^n的话，就可以实现取模运算了。而HashMap中的length也确实是2的倍数，初始值是16，之后每次扩充为原来的2倍。 分析完indexFor方法后，我们接下来准备分析 hash方法的具体原理和实现。在深入分析之前，至此，先做个总结。 HashMap的数据是存储在链表数组里面的。在对HashMap进行插入/删除等操作时，都需要根据K-V对的键值定位到他应该保存在数组的哪个下标中。而这个通过键值求取下标的操作就叫做哈希。 HashMap的数组是有长度的，Java中规定这个长度只能是2的倍数，初始值为16。 求哈希简单的做法是先求取出键值的hashcode，然后在将hashcode得到的int值对数组长度进行取模。为了考虑性能，Java总采用按位与操作实现取模操作。 以上，就是目前能够得到的结论，但是，由于HashMap使用位运算代替了取模运算，这就带来了另外一个问题，那就是有可能发生冲突。比如：CA11 1000和 0001 1000在对0000 1111进行按位与运算后的值是相等的。 两个不同的键值，在对数组长度进行按位与运算后得到的结果相同，这不就发生了冲突吗。那么如何解决这种冲突呢，来看下Java是如何做的。 其中的主要代码部分如下：123h ^= k.hashCode();h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12);return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); 这段代码是为了对key的hashCode进行扰动计算，防止不同hashCode的高位不同但低位相同导致的hash冲突。简单点说，就是为了把高位的特征和低位的特征组合起来，降低哈希冲突的概率，也就是说，尽量做到任何一位的变化都能对最终得到的结果产生影响。 举个例子来说，我们现在想向一个HashMap中put一个K-V对，Key的值为“hollischuang”，经过简单的获取hashcode后，得到的值为“1011000110101110011111010011011”，如果当前HashTable的大小为16，即在不进行扰动计算的情况下，他最终得到的index结果值为11。由于16的二进制扩展到32位为“00000000000000000000000000001111”，所以，一个数字在和他进行按位与操作的时候，前28位无论是什么，计算结果都一样（因为0和任何数做与，结果都为0）。如下图所示。 可以看到，后面的两个hashcode经过位运算之后得到的值也是11 ，虽然我们不知道哪个key的hashcode是上面例子中的那两个，但是肯定存在这样的key，这就产生了冲突。 那么，接下来，我看看一下经过扰动的算法最终的计算结果会如何。 从上面图中可以看到，之前会产生冲突的两个hashcode，经过扰动计算之后，最终得到的index的值不一样了，这就很好的避免了冲突。 其实，使用位运算代替取模运算，除了性能之外，还有一个好处就是可以很好的解决负数的问题。因为我们知道，hashcode的结果是int类型，而int的取值范围是-2^31 ~ 2^31 - 1，即[ -2147483648, 2147483647]；这里面是包含负数的，我们知道，对于一个负数取模还是有些麻烦的。如果使用二进制的位运算的话就可以很好的避免这个问题。首先，不管hashcode的值是正数还是负数。 length-1这个值一定是个正数。那么，他的二进制的第一位一定是0（有符号数用最高位作为符号位，“0”代表“+”，“1”代表“-”），这样里两个数做按位与运算之后，第一位一定是个0，也就是，得到的结果一定是个正数。 HashTable In Java 7上面是Java 7中HashMap的hash方法以及 indexOf方法的实现，那么接下来我们要看下，线程安全的HashTable是如何实现的，和HashMap有何不同，并试着分析下不同的原因。以下是Java 7中HashTable的hash方法的实现。1234private int hash(Object k) &#123; // hashSeed will be zero if alternative hashing is disabled. return hashSeed ^ k.hashCode();&#125; 我们可以发现，很简单，相当于只是对k做了个简单的hash，取了一下其hashCode。而HashTable中也没有indexOf方法，取而代之的是这段代码： int index = (hash &amp; 0x7FFFFFFF) % tab.length; 也就是说，HashMap和HashTable对于计算数组下标这件事，采用了两种方法。HashMap采用的是位运算，而HashTable采用的是直接取模。 为啥要把hash值和0x7FFFFFFF做一次按位与操作呢，主要是为了保证得到的index的的二进制的第一位为0（一个32位的有符号数和 0x7FFFFFFF做按位与操作，其实就是在取绝对值。），也就是为了得到一个正数。因为有符号数第一位0代表正数，1代表负数。 我们前面说过，HashMap之所以不用取模的原因是为了提高效率。有人认为，因为HashTable是个线程安全的类，本来就慢，所以Java并没有考虑效率问题，就直接使用取模算法了呢？但是其实并不完全是，Java这样设计还是有一定的考虑在的，虽然这样效率确实是会比HashMap慢一些。 其实，HashMap采用简单的取模是有一定的考虑在的。这就要涉及到HashTable的构造函数和扩容函数了。由于篇幅有限，这里就不贴代码了，直接给出结论： HashTable默认的初始大小为11，之后每次扩充为原来的2n+1。 也就是说，HashTable的链表数组的默认大小是一个素数、奇数。之后的每次扩充结果也都是奇数。 由于HashTable会尽量使用素数、奇数作为容量的大小。当哈希表的大小为素数时，简单的取模哈希的结果会更加均匀。（这个是可以证明出来的，由于不是本文重点，暂不详细介绍，可参考：http://zhaox.github.io/algorithm/2015/06/29/hash） 至此，我们看完了Java 7中HashMap和HashTable中对于hash的实现，我们来做个简单的总结。 HashMap默认的初始化大小为16，之后每次扩充为原来的2倍。 HashTable默认的初始大小为11，之后每次扩充为原来的2n+1。 当哈希表的大小为素数时，简单的取模哈希的结果会更加均匀，所以单从这一点上看，HashTable的哈希表大小选择，似乎更高明些。因为hash结果越分散效果越好。 在取模计算时，如果模数是2的幂，那么我们可以直接使用位运算来得到结果，效率要大大高于做除法。所以从hash计算的效率上，又是HashMap更胜一筹。 但是，HashMap为了提高效率使用位运算代替哈希，这又引入了哈希分布不均匀的问题，所以HashMap为解决这问题，又对hash算法做了一些改进，进行了扰动计算。 ConcurrentHashMap In Java 7123456789101112131415161718private int hash(Object k) &#123; int h = hashSeed; if ((0 != h) &amp;&amp; (k instanceof String)) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); // Spread bits to regularize both segment and index locations, // using variant of single-word Wang/Jenkins hash. h += (h &lt;&lt; 15) ^ 0xffffcd7d; h ^= (h &gt;&gt;&gt; 10); h += (h &lt;&lt; 3); h ^= (h &gt;&gt;&gt; 6); h += (h &lt;&lt; 2) + (h &lt;&lt; 14); return h ^ (h &gt;&gt;&gt; 16);&#125;int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; 上面这段关于ConcurrentHashMap的hash实现其实和HashMap如出一辙。都是通过位运算代替取模，然后再对hashcode进行扰动。区别在于，ConcurrentHashMap 使用了一种变种的Wang/Jenkins 哈希算法，其主要目的也是为了把高位和低位组合在一起，避免发生冲突。至于为啥不和HashMap采用同样的算法进行扰动，我猜这只是程序员自由意志的选择吧。至少我目前没有办法证明哪个更优。 HashMap In Java 8在Java 8 之前，HashMap和其他基于map的类都是通过链地址法解决冲突，它们使用单向链表来存储相同索引值的元素。在最坏的情况下，这种方式会将HashMap的get方法的性能从O(1)降低到 O(n)。 为了解决在频繁冲突时hashmap性能降低的问题，Java 8中使用平衡树来替代链表存储冲突的元素。这意味着我们可以将最坏情况下的性能从O(n)提高到 O(logn)。关于HashMap在Java 8中的优化，我后面会有文章继续深入介绍。 如果恶意程序知道我们用的是Hash算法，则在纯链表情况下，它能够发送大量请求导致哈希碰撞，然后不停访问这些key导致HashMap忙于进行线性查找，最终陷入瘫痪，即形成了拒绝服务攻击（DoS）。 关于Java 8中的hash函数，原理和Java 7中基本类似。Java 8中这一步做了优化，只做一次16位右位移异或混合，而不是四次，但原理是不变的。1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的。以上方法得到的int的hash值，然后再通过h &amp; (table.length -1)来得到该对象在数据中保存的位置。 HashTable In Java 8在Java 8的HashTable中，已经不在有hash方法了。但是哈希的操作还是在的，比如在put方法中就有如下实现： 12int hash = key.hashCode();int index = (hash &amp; 0x7FFFFFFF) % tab.length; 这其实和Java 7中的实现几乎无差别，就不做过多的介绍了。 ConcurrentHashMap In Java 8Java 8 里面的求hash的方法从hash改为了spread。实现方式如下： 123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; Java 8的ConcurrentHashMap同样是通过Key的哈希值与数组长度取模确定该Key在数组中的索引。 不同的是，Java 8的ConcurrentHashMap作者认为引入红黑树后，即使哈希冲突比较严重，寻址效率也足够高，所以作者并未在哈希值的计算上做过多设计，只是将Key的hashCode值与其高16位作异或并保证最高位为0（从而保证最终结果为正整数）。 总结至此，我们已经分析完了HashMap、HashTable以及ConcurrentHashMap分别在Jdk 1.7 和 Jdk 1.8中的实现。我们可以发现，为了保证哈希的结果可以分散、为了提高哈希的效率，JDK在一个小小的hash方法上就有很多考虑，做了很多事情。当然，我希望我们不仅可以深入了解背后的原理，还要学会这种对代码精益求精的态度。 Jdk的源代码，每一行都很有意思，都值得花时间去钻研、推敲。","categories":[{"name":"hash","slug":"hash","permalink":"http://github.com/b2stry/categories/hash/"}],"tags":[{"name":"hash","slug":"hash","permalink":"http://github.com/b2stry/tags/hash/"}]},{"title":"Spring Boot集成Swagger2，构建优雅的Restful API","slug":"springboot集成swagger2，构建优雅的Restful-API","date":"2018-03-05T08:20:46.000Z","updated":"2018-03-05T08:38:24.623Z","comments":true,"path":"2018/03/05/springboot集成swagger2，构建优雅的Restful-API/","link":"","permalink":"http://github.com/b2stry/2018/03/05/springboot集成swagger2，构建优雅的Restful-API/","excerpt":"Swagger,中文“拽”的意思。它是一个功能强大的api框架，它的集成非常简单，不仅提供了在线文档的查阅，而且还提供了在线文档的测试。另外Swagger很容易构建Restful风格的api，简单优雅帅气，正如它的名字。","text":"Swagger,中文“拽”的意思。它是一个功能强大的api框架，它的集成非常简单，不仅提供了在线文档的查阅，而且还提供了在线文档的测试。另外Swagger很容易构建Restful风格的api，简单优雅帅气，正如它的名字。 一、引入依赖1234567891011&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt;&lt;/dependency&gt; 二、写配置类12345678910111213141516171819202122@Configuration@EnableSwagger2public class Swagger2 &#123; @Bean public Docket createRestApi() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.basePackage(\"com.forezp.controller\")) .paths(PathSelectors.any()) .build(); &#125; private ApiInfo apiInfo() &#123; return new ApiInfoBuilder() .title(\"springboot利用swagger构建api文档\") .description(\"简单优雅的restfun风格，http://blog.csdn.net/forezp\") .termsOfServiceUrl(\"http://blog.csdn.net/forezp\") .version(\"1.0\") .build(); &#125;&#125; 通过@Configuration注解，表明它是一个配置类，@EnableSwagger2开启swagger2。apiINfo()配置一些基本的信息。apis()指定扫描的包会生成文档。 三、写生产文档的注解Swagger通过注解表明该接口会生成文档，包括接口名、请求方法、参数、返回信息的等等。 @Api：修饰整个类，描述Controller的作用 @ApiOperation：描述一个类的一个方法，或者说一个接口 @ApiParam：单个参数描述 @ApiModel：用对象来接收参数 @ApiProperty：用对象接收参数时，描述对象的一个字段 @ApiResponse：HTTP响应其中1个描述 @ApiResponses：HTTP响应整体描述 @ApiIgnore：使用该注解忽略这个API @ApiError ：发生错误返回的信息 @ApiParamImplicitL：一个请求参数 @ApiParamsImplicit 多个请求参数现在通过一个栗子来说明：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.forezp.controller;import com.forezp.entity.Book;import io.swagger.annotations.ApiImplicitParam;import io.swagger.annotations.ApiImplicitParams;import io.swagger.annotations.ApiOperation;import org.springframework.ui.ModelMap;import org.springframework.web.bind.annotation.*;import springfox.documentation.annotations.ApiIgnore;import java.util.*;/** * 用户创建某本图书 POST /books/ * 用户修改对某本图书 PUT /books/:id/ * 用户删除对某本图书 DELETE /books/:id/ * 用户获取所有的图书 GET /books * 用户获取某一图书 GET /Books/:id * Created by fangzhipeng on 2017/4/17. * 官方文档：http://swagger.io/docs/specification/api-host-and-base-path/ */@RestController@RequestMapping(value = \"/books\")public class BookContrller &#123; Map&lt;Long, Book&gt; books = Collections.synchronizedMap(new HashMap&lt;Long, Book&gt;()); @ApiOperation(value=\"获取图书列表\", notes=\"获取图书列表\") @RequestMapping(value=&#123;\"\"&#125;, method= RequestMethod.GET) public List&lt;Book&gt; getBook() &#123; List&lt;Book&gt; book = new ArrayList&lt;&gt;(books.values()); return book; &#125; @ApiOperation(value=\"创建图书\", notes=\"创建图书\") @ApiImplicitParam(name = \"book\", value = \"图书详细实体\", required = true, dataType = \"Book\") @RequestMapping(value=\"\", method=RequestMethod.POST) public String postBook(@RequestBody Book book) &#123; books.put(book.getId(), book); return \"success\"; &#125; @ApiOperation(value=\"获图书细信息\", notes=\"根据url的id来获取详细信息\") @ApiImplicitParam(name = \"id\", value = \"ID\", required = true, dataType = \"Long\",paramType = \"path\") @RequestMapping(value=\"/&#123;id&#125;\", method=RequestMethod.GET) public Book getBook(@PathVariable Long id) &#123; return books.get(id); &#125; @ApiOperation(value=\"更新信息\", notes=\"根据url的id来指定更新图书信息\") @ApiImplicitParams(&#123; @ApiImplicitParam(name = \"id\", value = \"图书ID\", required = true, dataType = \"Long\",paramType = \"path\"), @ApiImplicitParam(name = \"book\", value = \"图书实体book\", required = true, dataType = \"Book\") &#125;) @RequestMapping(value=\"/&#123;id&#125;\", method= RequestMethod.PUT) public String putUser(@PathVariable Long id, @RequestBody Book book) &#123; Book book1 = books.get(id); book1.setName(book.getName()); book1.setPrice(book.getPrice()); books.put(id, book1); return \"success\"; &#125; @ApiOperation(value=\"删除图书\", notes=\"根据url的id来指定删除图书\") @ApiImplicitParam(name = \"id\", value = \"图书ID\", required = true, dataType = \"Long\",paramType = \"path\") @RequestMapping(value=\"/&#123;id&#125;\", method=RequestMethod.DELETE) public String deleteUser(@PathVariable Long id) &#123; books.remove(id); return \"success\"; &#125; @ApiIgnore//使用该注解忽略这个API @RequestMapping(value = \"/hi\", method = RequestMethod.GET) public String jsonTest() &#123; return \" hi you!\"; &#125;&#125; 通过相关注解，就可以让Swagger2生成相应的文档。如果你不需要某接口生成文档，只需要在加@ApiIgnore注解即可。需要说明的是，如果请求参数在url上，@ApiImplicitParam 上加paramType = “path” 。 启动工程，访问：http://localhost:8080/swagger-ui.html ，就看到swagger-ui: 整个集成过程非常简单，但是我看了相关的资料，Swagger没有做安全方面的防护，可能需要我们自己做相关的工作。 四、参考资料swagger.io Spring Boot中使用Swagger2构建强大的RESTful API文档","categories":[{"name":"swagger","slug":"swagger","permalink":"http://github.com/b2stry/categories/swagger/"}],"tags":[{"name":"swagger","slug":"swagger","permalink":"http://github.com/b2stry/tags/swagger/"}]},{"title":"消息队列之选型对比","slug":"消息队列之选型对比","date":"2018-02-22T04:30:12.000Z","updated":"2018-02-22T04:36:42.926Z","comments":true,"path":"2018/02/22/消息队列之选型对比/","link":"","permalink":"http://github.com/b2stry/2018/02/22/消息队列之选型对比/","excerpt":"对ActiveMQ、RabbitMQ、RocketMQ、Kafka对比","text":"对ActiveMQ、RabbitMQ、RocketMQ、Kafka对比 综合选择RabbitMq","categories":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://github.com/b2stry/categories/消息中间件/"}],"tags":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://github.com/b2stry/tags/消息中间件/"}]},{"title":"10个有趣但毫无用处的Linux命令","slug":"10个有趣但毫无用处的Linux命令","date":"2018-02-07T09:43:26.000Z","updated":"2018-02-07T10:20:03.184Z","comments":true,"path":"2018/02/07/10个有趣但毫无用处的Linux命令/","link":"","permalink":"http://github.com/b2stry/2018/02/07/10个有趣但毫无用处的Linux命令/","excerpt":"以下Linux命令对中，正确的是( ) 1、ls和sl 2、cat和tac 3、more和erom 4、exit和tixe 刚看到这题真的是一脸懵逼，这真的有正确的吗，于是抱着怀疑的态度百度了一下，结果发现了下面的东西。","text":"以下Linux命令对中，正确的是( ) 1、ls和sl 2、cat和tac 3、more和erom 4、exit和tixe 刚看到这题真的是一脸懵逼，这真的有正确的吗，于是抱着怀疑的态度百度了一下，结果发现了下面的东西。 一、sl (Steam Locomotive)蒸汽机看清楚了，不是ls，是‘sl’。ls是linux命令最常用的一个命令，用来列表文件目录等。因为用的频繁，难免有着急打错的时候，一旦你敲成了 ‘sl’，会出现什么结果？后果很严重，是否还记得《盗梦空间》里突然一辆火车出现在梦境里的场景吗？这个命令的效果就是让你的屏幕上隆隆的驶过一辆蒸汽 机火车。有趣吧。 安装sl：1~#sudo apt-get install sl 执行效果：1~# sl 二、oneko如果你认为linux终端只是字符和光标，没有什么其它更丰富的表现形式了，那你就大错特错了，“oneko”命令就是一个很好的例子，不要以为oneko只是一种小猫的图形，移动你的鼠标，它会和你一起玩耍的。 安装oneko：1~#sudo apt-get install oneko 执行效果：1~# oneko 三、aafire有没有想过在你的黑白命令终端上燃起一团火将是什么样子，你只需要输入aafire命令，回车，奇迹就会出现。 安装aafire：1~#sudo apt-get install libaa-bin 执行效果：1~# aafire 四、ASCIIquarium这真是一个不可思议的杰作，你的linux终端窗口竟然成了水族馆，里面有水，有石、有鱼、有兽。不多说，自己欣赏一下吧。 安装ASCIIquarium：123456~# cd /tmp~# wget http://www.robobunny.com/projects/asciiquarium/asciiquarium.tar.gz~# tar -zxvf asciiquarium.tar.gz~# cd asciiquarium_1.1/~# cp asciiquarium /usr/local/bin~# chmod 0755 /usr/local/bin/asciiquarium 执行效果：1~# asciiquarium 五、toilet(厕所)有没有搞错，还有叫这个名字的命令？尽管这个名字本身就已经够搞笑了。那这个命令有什么特殊功能呢？肯定不是执行这个命令后你家马桶就能自动冲洗 了。它是能用字母拼写出更大字母的工具，具体拼出什么字由命令后面的参数决定，不仅如此，它还能打印出各种风格的效果，比如彩色，金属光泽等。 安装toilet：1~# sudo apt-get install toilet 运行效果：12~# toilet www.aqee.net~# toilet -f mono12 -F metal www.aqee.net 六、cmatrix你应该看过好莱坞大片《骇客帝国》，相信你会对电影中那些神奇的场景着迷。在Neo的眼里任何东西都能以计算机字节流的形式展现，你是否也想做一个很有黑客范儿的数据流的桌面呢？ 安装cmatrix：1~#sudo apt-get install cmatrix 执行效果：1~# cmatrix 七、Cowsay从这个命令的名字上，你就应该猜到，牛要说话了。的确，牛有话要说，而且是奶牛。这个命令有个加强版，叫做xcowsay，效果更好。 安装Cowsay和xCowsay：12~#sudo apt-get install cowsay~#sudo apt-get install xcowsay 运行效果：12~# cowsay 你好，外刊IT评论网~# xcowsay 你好，外刊IT评论网 八、xeyes执行xeyes会在屏幕上出现一双大眼睛，而且眼珠会跟随你的鼠标转动。 安装xeyes：1~# sudo apt-get install xeyes 运行效果：1~# xeyes 九、moo你今天咩咩了没有？呵呵 不需要安装 执行效果：1~# apt-get moo 十、bb自己在命令行窗口里输入bb，看看会发生什么情况。 安装bb：1~# sudo apt-get install bb 执行效果：1~# bb","categories":[{"name":"linux","slug":"linux","permalink":"http://github.com/b2stry/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://github.com/b2stry/tags/linux/"}]},{"title":"消息队列之RabbitMQ","slug":"消息队列之RabbitMQ","date":"2018-02-03T07:14:07.000Z","updated":"2018-02-22T02:57:36.302Z","comments":true,"path":"2018/02/03/消息队列之RabbitMQ/","link":"","permalink":"http://github.com/b2stry/2018/02/03/消息队列之RabbitMQ/","excerpt":"什么叫消息队列消息（Message）是指在应用间传送的数据。消息可以非常简单，比如只包含文本字符串，也可以更复杂，可能包含嵌入对象。 消息队列（Message Queue）是一种应用间的通信方式，消息发送后可以立即返回，由消息系统来确保消息的可靠传递。消息发布者只管把消息发布到 MQ 中而不用管谁来取，消息使用者只管从 MQ 中取消息而不管是谁发布的。这样发布者和使用者都不用知道对方的存在。","text":"什么叫消息队列消息（Message）是指在应用间传送的数据。消息可以非常简单，比如只包含文本字符串，也可以更复杂，可能包含嵌入对象。 消息队列（Message Queue）是一种应用间的通信方式，消息发送后可以立即返回，由消息系统来确保消息的可靠传递。消息发布者只管把消息发布到 MQ 中而不用管谁来取，消息使用者只管从 MQ 中取消息而不管是谁发布的。这样发布者和使用者都不用知道对方的存在。 为何用消息队列从上面的描述中可以看出消息队列是一种应用间的异步协作机制，那什么时候需要使用 MQ 呢？ 以常见的订单系统为例，用户点击【下单】按钮之后的业务逻辑可能包括：扣减库存、生成相应单据、发红包、发短信通知。在业务发展初期这些逻辑可能放在一起同步执行，随着业务的发展订单量增长，需要提升系统服务的性能，这时可以将一些不需要立即生效的操作拆分出来异步执行，比如发放红包、发短信通知等。这种场景下就可以用 MQ ，在下单的主流程（比如扣减库存、生成相应单据）完成之后发送一条消息到 MQ 让主流程快速完结，而由另外的单独线程拉取MQ的消息（或者由 MQ 推送消息），当发现 MQ 中有发红包或发短信之类的消息时，执行相应的业务逻辑。 以上是用于业务解耦的情况，其它常见场景包括最终一致性、广播、错峰流控等等。 RabbitMQ 特点RabbitMQ 是一个由 Erlang 语言开发的 AMQP 的开源实现。 AMQP ：Advanced Message Queue，高级消息队列协议。它是应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，并不受产品、开发语言等条件的限制。 RabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。具体特点包括： 1.可靠性（Reliability） RabbitMQ 使用一些机制来保证可靠性，如持久化、传输确认、发布确认。 2.灵活的路由（Flexible Routing） 在消息进入队列之前，通过 Exchange 来路由消息的。对于典型的路由功能，RabbitMQ 已经提供了一些内置的 Exchange 来实现。针对更复杂的路由功能，可以将多个 Exchange 绑定在一起，也通过插件机制实现自己的 Exchange 。 3.消息集群（Clustering）多个 RabbitMQ 服务器可以组成一个集群，形成一个逻辑 Broker 。 4.高可用（Highly Available Queues）队列可以在集群中的机器上进行镜像，使得在部分节点出问题的情况下队列仍然可用。 5.多种协议（Multi-protocol） RabbitMQ 支持多种消息队列协议，比如 STOMP、MQTT 等等。 6.多语言客户端（Many Clients） RabbitMQ 几乎支持所有常用语言，比如 Java、.NET、Ruby 等等。 7.管理界面（Management UI） RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方面。 8.跟踪机制（Tracing） 如果消息异常，RabbitMQ 提供了消息跟踪机制，使用者可以找出发生了什么。 9.插件机制（Plugin System）RabbitMQ 提供了许多插件，来从多方面进行扩展，也可以编写自己的插件。 RabbitMQ 中的概念模型消息模型所有 MQ 产品从模型抽象上来说都是一样的过程：消费者（consumer）订阅某个队列。生产者（producer）创建消息，然后发布到队列（queue）中，最后将消息发送到监听的消费者。 RabbitMQ 基本概念上面只是最简单抽象的描述，具体到 RabbitMQ 则有更详细的概念需要解释。上面介绍过 RabbitMQ 是 AMQP 协议的一个开源实现，所以其内部实际上也是 AMQP 中的基本概念： 1.Message 消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 2.Publisher 消息的生产者，也是一个向交换器发布消息的客户端应用程序。3.Exchange 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。 4.Binding 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。 5.Queue 消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 6.Connection 网络连接，比如一个TCP连接。 7.Channel 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。 8.Consumer 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。 9.Virtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。 10.Broker 表示消息队列服务器实体。 AMQP 中的消息路由AMQP 中消息的路由过程和 Java 开发者熟悉的 JMS 存在一些差别，AMQP 中增加了 Exchange 和 Binding 的角色。生产者把消息发布到 Exchange 上，消息最终到达队列并被消费者接收，而 Binding 决定交换器的消息应该发送到那个队列。 Exchange 类型Exchange分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。headers 匹配 AMQP 消息的 header 而不是路由键，此外 headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型： 1.direct 消息中的路由键（routing key）如果和 Binding 中的 binding key 一致， 交换器就将消息发到对应的队列中。路由键与队列名完全匹配，如果一个队列绑定到交换机要求路由键为“dog”，则只转发 routing key 标记为“dog”的消息，不会转发“dog.puppy”，也不会转发“dog.guard”等等。它是完全匹配单播的模式。 2.fanout 每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout 交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息都会被转发到与该交换器绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。 3.topic topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符：符号#和符号*。#匹配0个或多个单词，*匹配不多不少一个单词。 RabbitMQ 安装一般来说安装 RabbitMQ 之前要安装 Erlang ，可以去Erlang官网下载。接着去RabbitMQ官网下载安装包，之后解压缩即可。根据操作系统不同官网提供了相应的安装说明：Windows、Debian / Ubuntu、RPM-based Linux、Mac 如果是Mac 用户，个人推荐使用 HomeBrew 来安装，安装前要先更新 brew：1brew update 接着安装 rabbitmq 服务器：1brew install rabbitmq 这样 RabbitMQ 就安装好了，安装过程中会自动其所依赖的 Erlang 。 RabbitMQ 运行和管理1.启动 启动很简单，找到安装后的 RabbitMQ 所在目录下的 sbin 目录，可以看到该目录下有6个以 rabbitmq 开头的可执行文件，直接执行 rabbitmq-server 即可，下面将 RabbitMQ 的安装位置以 . 代替，启动命令就是：1./sbin/rabbitmq-server 启动正常的话会看到一些启动过程信息和最后的 completed with 7 plugins，这也说明启动的时候默认加载了7个插件。 2.后台启动 如果想让 RabbitMQ 以守护程序的方式在后台运行，可以在启动的时候加上 -detached 参数：1./sbin/rabbitmq-server -detached 3.查询服务器状态 sbin 目录下有个特别重要的文件叫 rabbitmqctl ，它提供了 RabbitMQ 管理需要的几乎一站式解决方案，绝大部分的运维命令它都可以提供。 查询 RabbitMQ 服务器的状态信息可以用参数 status ：1./sbin/rabbitmqctl status 该命令将输出服务器的很多信息，比如 RabbitMQ 和 Erlang 的版本、OS 名称、内存等等 4.关闭 RabbitMQ 节点 我们知道 RabbitMQ 是用 Erlang 语言写的，在Erlang 中有两个概念：节点和应用程序。节点就是 Erlang 虚拟机的每个实例，而多个 Erlang 应用程序可以运行在同一个节点之上。节点之间可以进行本地通信（不管他们是不是运行在同一台服务器之上）。比如一个运行在节点A上的应用程序可以调用节点B上应用程序的方法，就好像调用本地函数一样。如果应用程序由于某些原因奔溃，Erlang 节点会自动尝试重启应用程序。 如果要关闭整个 RabbitMQ 节点可以用参数 stop ：1./sbin/rabbitmqctl stop 它会和本地节点通信并指示其干净的关闭，也可以指定关闭不同的节点，包括远程节点，只需要传入参数 -n ：1./sbin/rabbitmqctl -n rabbit@server.example.com stop -n node 默认 node 名称是 rabbit@server ，如果你的主机名是 server.example.com ，那么 node 名称就是 rabbit@server.example.com 。 5.关闭 RabbitMQ 应用程序 如果只想关闭应用程序，同时保持 Erlang 节点运行则可以用 stop_app：1./sbin/rabbitmqctl stop_app 这个命令在后面要讲的集群模式中将会很有用。 6.启动 RabbitMQ 应用程序1./sbin/rabbitmqctl start_app 7.重置 RabbitMQ 节点1./sbin/rabbitmqctl reset 该命令将清除所有的队列。 8.查看已声明的队列1./sbin/rabbitmqctl list_queues 9.查看交换器1./sbin/rabbitmqctl list_exchanges 该命令还可以附加参数，比如列出交换器的名称、类型、是否持久化、是否自动删除：1./sbin/rabbitmqctl list_exchanges name type durable auto_delete 10.查看绑定1./sbin/rabbitmqctl list_bindings Java 客户端访问RabbitMQ 支持多种语言访问，以 Java 为例看下一般使用 RabbitMQ 的步骤。 1.maven工程的pom文件中添加依赖12345&lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;4.1.0&lt;/version&gt;&lt;/dependency&gt; 2.消息生产者1234567891011121314151617181920212223242526272829303132package org.study.rabbitmq;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Producer &#123; public static void main(String[] args) throws IOException, TimeoutException &#123; //创建连接工厂 ConnectionFactory factory = new ConnectionFactory(); factory.setUsername(&quot;guest&quot;); factory.setPassword(&quot;guest&quot;); //设置 RabbitMQ 地址 factory.setHost(&quot;localhost&quot;); //建立到代理服务器到连接 Connection conn = factory.newConnection(); //获得信道 Channel channel = conn.createChannel(); //声明交换器 String exchangeName = &quot;hello-exchange&quot;; channel.exchangeDeclare(exchangeName, &quot;direct&quot;, true); String routingKey = &quot;hola&quot;; //发布消息 byte[] messageBodyBytes = &quot;quit&quot;.getBytes(); channel.basicPublish(exchangeName, routingKey, null, messageBodyBytes); channel.close(); conn.close(); &#125;&#125; 2.消息消费者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package org.study.rabbitmq;import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Consumer &#123; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory factory = new ConnectionFactory(); factory.setUsername(&quot;guest&quot;); factory.setPassword(&quot;guest&quot;); factory.setHost(&quot;localhost&quot;); //建立到代理服务器到连接 Connection conn = factory.newConnection(); //获得信道 final Channel channel = conn.createChannel(); //声明交换器 String exchangeName = &quot;hello-exchange&quot;; channel.exchangeDeclare(exchangeName, &quot;direct&quot;, true); //声明队列 String queueName = channel.queueDeclare().getQueue(); String routingKey = &quot;hola&quot;; //绑定队列，通过键 hola 将队列和交换器绑定起来 channel.queueBind(queueName, exchangeName, routingKey); while(true) &#123; //消费消息 boolean autoAck = false; String consumerTag = &quot;&quot;; channel.basicConsume(queueName, autoAck, consumerTag, new DefaultConsumer(channel) &#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; String routingKey = envelope.getRoutingKey(); String contentType = properties.getContentType(); System.out.println(&quot;消费的路由键：&quot; + routingKey); System.out.println(&quot;消费的内容类型：&quot; + contentType); long deliveryTag = envelope.getDeliveryTag(); //确认消息 channel.basicAck(deliveryTag, false); System.out.println(&quot;消费的消息体内容：&quot;); String bodyStr = new String(body, &quot;UTF-8&quot;); System.out.println(bodyStr); &#125; &#125;); &#125; &#125;&#125; 4.启动 RabbitMQ 服务器1./sbin/rabbitmq-server 5.运行 Consumer 先运行 Consumer ，这样当生产者发送消息的时候能在消费者后端看到消息记录。 6.运行 Producer 接着运行 Producer ,发布一条消息，在 Consumer 的控制台能看到接收的消息： RabbitMQ 集群RabbitMQ 最优秀的功能之一就是内建集群，这个功能设计的目的是允许消费者和生产者在节点崩溃的情况下继续运行，以及通过添加更多的节点来线性扩展消息通信吞吐量。RabbitMQ 内部利用 Erlang 提供的分布式通信框架 OTP 来满足上述需求，使客户端在失去一个 RabbitMQ 节点连接的情况下，还是能够重新连接到集群中的任何其他节点继续生产、消费消息。 RabbitMQ 集群中的一些概念RabbitMQ 会始终记录以下四种类型的内部元数据： 1.队列元数据 包括队列名称和它们的属性，比如是否可持久化，是否自动删除 2.交换器元数据 交换器名称、类型、属性 3.绑定元数据 内部是一张表格记录如何将消息路由到队列 4.vhost 元数据 为 vhost 内部的队列、交换器、绑定提供命名空间和安全属性 在单一节点中，RabbitMQ 会将所有这些信息存储在内存中，同时将标记为可持久化的队列、交换器、绑定存储到硬盘上。存到硬盘上可以确保队列和交换器在节点重启后能够重建。而在集群模式下同样也提供两种选择：存到硬盘上（独立节点的默认设置），存在内存中。 如果在集群中创建队列，集群只会在单个节点而不是所有节点上创建完整的队列信息（元数据、状态、内容）。结果是只有队列的所有者节点知道有关队列的所有信息，因此当集群节点崩溃时，该节点的队列和绑定就消失了，并且任何匹配该队列的绑定的新消息也丢失了。还好RabbitMQ 2.6.0之后提供了镜像队列以避免集群节点故障导致的队列内容不可用。 RabbitMQ 集群中可以共享 user、vhost、exchange等，所有的数据和状态都是必须在所有节点上复制的，例外就是上面所说的消息队列。RabbitMQ 节点可以动态的加入到集群中。 当在集群中声明队列、交换器、绑定的时候，这些操作会直到所有集群节点都成功提交元数据变更后才返回。集群中有内存节点和磁盘节点两种类型，内存节点虽然不写入磁盘，但是它的执行比磁盘节点要好。内存节点可以提供出色的性能，磁盘节点能保障配置信息在节点重启后仍然可用，那集群中如何平衡这两者呢？ RabbitMQ 只要求集群中至少有一个磁盘节点，所有其他节点可以是内存节点，当节点加入火离开集群时，它们必须要将该变更通知到至少一个磁盘节点。如果只有一个磁盘节点，刚好又是该节点崩溃了，那么集群可以继续路由消息，但不能创建队列、创建交换器、创建绑定、添加用户、更改权限、添加或删除集群节点。换句话说集群中的唯一磁盘节点崩溃的话，集群仍然可以运行，但知道该节点恢复，否则无法更改任何东西。 RabbitMQ 集群配置和启动如果是在一台机器上同时启动多个 RabbitMQ 节点来组建集群的话，只用上面介绍的方式启动第二、第三个节点将会因为节点名称和端口冲突导致启动失败。所以在每次调用 rabbitmq-server 命令前，设置环境变量 RABBITMQ_NODENAME 和 RABBITMQ_NODE_PORT 来明确指定唯一的节点名称和端口。下面的例子端口号从5672开始，每个新启动的节点都加1，节点也分别命名为test_rabbit_1、test_rabbit_2、test_rabbit_3。 启动第1个节点：1RABBITMQ_NODENAME=test_rabbit_1 RABBITMQ_NODE_PORT=5672 ./sbin/rabbitmq-server -detached 启动第2个节点：1RABBITMQ_NODENAME=test_rabbit_2 RABBITMQ_NODE_PORT=5673 ./sbin/rabbitmq-server -detached 启动第2个节点前建议将 RabbitMQ 默认激活的插件关掉，否则会存在使用了某个插件的端口号冲突，导致节点启动不成功。 现在第2个节点和第1个节点都是独立节点，它们并不知道其他节点的存在。集群中除第一个节点外后加入的节点需要获取集群中的元数据，所以要先停止 Erlang 节点上运行的 RabbitMQ 应用程序，并重置该节点元数据，再加入并且获取集群的元数据，最后重新启动 RabbitMQ 应用程序。 停止第2个节点的应用程序：1./sbin/rabbitmqctl -n test_rabbit_2 stop_app 重置第2个节点元数据：1./sbin/rabbitmqctl -n test_rabbit_2 reset 第2节点加入第1个节点组成的集群：1./sbin/rabbitmqctl -n test_rabbit_2 join_cluster test_rabbit_1@localhost 启动第2个节点的应用程序1./sbin/rabbitmqctl -n test_rabbit_2 start_app 第3个节点的配置过程和第2个节点类似：123456789RABBITMQ_NODENAME=test_rabbit_3 RABBITMQ_NODE_PORT=5674 ./sbin/rabbitmq-server -detached./sbin/rabbitmqctl -n test_rabbit_3 stop_app./sbin/rabbitmqctl -n test_rabbit_3 reset./sbin/rabbitmqctl -n test_rabbit_3 join_cluster test_rabbit_1@localhost./sbin/rabbitmqctl -n test_rabbit_3 start_app RabbitMQ 集群运维停止某个指定的节点，比如停止第2个节点：1RABBITMQ_NODENAME=test_rabbit_2 ./sbin/rabbitmqctl stop 查看节点3的集群状态：1./sbin/rabbitmqctl -n test_rabbit_3 cluster_status 作者：预流链接：https://www.jianshu.com/p/79ca08116d57來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://github.com/b2stry/categories/rabbitmq/"}],"tags":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://github.com/b2stry/tags/消息中间件/"}]},{"title":"JSR303使用说明文档","slug":"JSR303使用说明文档","date":"2018-01-31T15:32:04.000Z","updated":"2018-01-31T15:44:56.350Z","comments":true,"path":"2018/01/31/JSR303使用说明文档/","link":"","permalink":"http://github.com/b2stry/2018/01/31/JSR303使用说明文档/","excerpt":"1.引言参数校验是我们程序开发中必不可少的过程。用户在前端页面上填写表单时，前端js程序会校验参数的合法性，当数据到了后端，为了防止恶意操作，保持程序的健壮性，后端同样需要对数据进行校验。后端参数校验最简单的做法是直接在业务方法里面进行判断，当判断成功之后再继续往下执行。但这样带给我们的是代码的耦合，冗余。当我们多个地方需要校验时，我们就需要在每一个地方调用校验程序,导致代码很冗余，且不美观。 那么如何优雅的对参数进行校验呢？JSR303就是为了解决这个问题出现的，本篇文章主要是介绍 JSR303，Hibernate Validator 等校验工具的使用，以及自定义校验注解的使用。","text":"1.引言参数校验是我们程序开发中必不可少的过程。用户在前端页面上填写表单时，前端js程序会校验参数的合法性，当数据到了后端，为了防止恶意操作，保持程序的健壮性，后端同样需要对数据进行校验。后端参数校验最简单的做法是直接在业务方法里面进行判断，当判断成功之后再继续往下执行。但这样带给我们的是代码的耦合，冗余。当我们多个地方需要校验时，我们就需要在每一个地方调用校验程序,导致代码很冗余，且不美观。 那么如何优雅的对参数进行校验呢？JSR303就是为了解决这个问题出现的，本篇文章主要是介绍 JSR303，Hibernate Validator 等校验工具的使用，以及自定义校验注解的使用。 2. 校验框架介绍JSR303 是一套JavaBean参数校验的标准，它定义了很多常用的校验注解，我们可以直接将这些注解加在我们JavaBean的属性上面，就可以在需要校验的时候进行校验了。注解如下：12345678910111213@NotNull 注解元素必须是非空@Null 注解元素必须是空@Digits 验证数字构成是否合法@Future 验证是否在当前系统时间之后@Past 验证是否在当前系统时间之前@Max 验证值是否小于等于最大指定整数值@Min 验证值是否大于等于最小指定整数值@Pattern 验证字符串是否匹配指定的正则表达式@Size 验证元素大小是否在指定范围内@DecimalMax 验证值是否小于等于最大指定小数值@DecimalMin 验证值是否大于等于最小指定小数值@AssertTrue 被注释的元素必须为true@AssertFalse 被注释的元素必须为false Hibernate validator 在JSR303的基础上对校验注解进行了扩展，扩展注解如下：1234@Email 被注释的元素必须是电子邮箱地址@Length 被注释的字符串的大小必须在指定的范围内@NotEmpty 被注释的字符串的必须非空@Range 被注释的元素必须在合适的范围内 3.代码实现3.1添加JAR包依赖在pom.xml中添加如下依赖:123456789101112&lt;!--jsr 303--&gt; &lt;dependency&gt; &lt;groupId&gt;javax.validation&lt;/groupId&gt; &lt;artifactId&gt;validation-api&lt;/artifactId&gt; &lt;version&gt;1.1.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;!-- hibernate validator--&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;5.2.0.Final&lt;/version&gt; &lt;/dependency&gt; 3.2最简单的参数校验3.2.1、Model 中添加校验注解123456789101112public class Book &#123; private long id; @NotEmpty(message = \"书名不能为空\") private String bookName; @NotNull(message = \"ISBN号不能为空\") private String bookIsbn; @DecimalMin(value = \"0.1\",message = \"单价最低为0.1\") private doubleprice; // getter setter .......&#125; 3.2.2、在controller中使用此校验1234@RequestMapping(value = \"/book\",method = RequestMethod.POST) public void addBook(@RequestBody @Valid Book book) &#123; System.out.println(book.toString());&#125; 当访问这个post接口时，如果参数不符合Model中定义的话，程序中就回抛出400异常，并提示错误信息。 3.3分组验证对同一个Model，我们在增加和修改时对参数的校验也是不一样的，这个时候我们就需要定义分组验证，步骤如下 3.3.1、定义两个空接口，分别代表Person对象的增加校验规则和修改校验规则1234567891011/** * 可以在一个Model上面添加多套参数验证规则，此接口定义添加Person模型新增时的参数校验规则 */public interface PersonAddView &#123;&#125;/** * 可以在一个Model上面添加多套参数验证规则，此接口定义添加Person模型修改时的参数校验规则 */public interface PersonModifyView &#123;&#125; 3.3.2、Model上添加注解时使用指明所述的分组12345678910111213141516171819public class Person &#123; private long id; /** * 添加groups 属性，说明只在特定的验证规则里面起作用，不加则表示在使用Deafault规则时起作用 */ @NotNull(groups = &#123;PersonAddView.class, PersonModifyView.class&#125;, message= \"添加、修改用户时名字不能为空\",payload = ValidateErrorLevel.Info.class) @ListNotHasNull.List(&#123; @ListNotHasNull(groups = &#123;PersonAddView.class&#125;, message = \"添加上Name不能为空\"), @ListNotHasNull(groups = &#123;PersonModifyView.class&#125;, message = \"修改时Name不能为空\")&#125;) private String name; @NotNull(groups = &#123;PersonAddView.class&#125;, message = \"添加用户时地址不能为空\") private String address; @Min(value = 18, groups = &#123;PersonAddView.class&#125;, message = \"姓名不能低于18岁\") @Max(value = 30, groups = &#123;PersonModifyView.class&#125;, message = \"姓名不能超过30岁\") private int age; //getter setter 方法......&#125; 3.3.3、启用校验此时启用校验和之前的不同,需要指明启用哪一组规则12345678910111213141516171819 /** * 添加一个Person对象 * 此处启用PersonAddView这个验证规则 * 备注：此处@Validated(PersonAddView.class)表示使用PersonAndView这套校验规则，若使用@Valid 则表示使用默认校验规则， * 若两个规则同时加上去，则只有第一套起作用 */ @RequestMapping(value = &quot;/person&quot;, method =RequestMethod.POST) public void addPerson(@RequestBody @Validated(&#123;PersonAddView.class,Default.class&#125;) Person person) &#123; System.out.println(person.toString()); &#125; /** * 修改Person对象 * 此处启用PersonModifyView这个验证规则 */ @RequestMapping(value = &quot;/person&quot;, method = RequestMethod.PUT) public void modifyPerson(@RequestBody @Validated(value =&#123;PersonModifyView.class&#125;) Person person) &#123; System.out.println(person.toString());&#125; 3.4 Spring validator 方法级别的校验JSR和Hibernate validator的校验只能对Object的属性进行校验，不能对单个的参数进行校验，spring 在此基础上进行了扩展，添加了MethodValidationPostProcessor拦截器，可以实现对方法参数的校验，实现如下: 3.4.1、实例化MethodValidationPostProcessor1234@Beanpublic MethodValidationPostProcessormethodValidationPostProcessor() &#123; return new MethodValidationPostProcessor();&#125; 3.4.2、在所要实现方法参数校验的类上面添加@Validated，如下1234@RestController@Validatedpublic class ValidateController &#123;&#125; 3.4.3、在方法上面添加校验规则:12345@RequestMapping(value = \"/test\",method = RequestMethod.GET) public String paramCheck(@Length(min = 10) @RequestParam String name) &#123; System.out.println(name); return null;&#125; 3.5使用BindingResult对象来保存验证结果每一个模型对象后边都需要跟一个Errors或BindingResult对象来保存验证结果，其方法体内部可以使用这两个验证结果对象来选择出错时跳转的页面或处理的逻辑。1234567891011@RequestMapping(\"/validate/multi\")public String multi(@Valid@ModelAttribute(\"a\") A a, BindingResult aErrors, @Valid@ModelAttribute(\"b\") B b, BindingResult bErrors) &#123; if (aErrors.hasErrors()) &#123; //如果a模型对象验证失败 return \"validate/error\"; &#125; if (bErrors.hasErrors()) &#123; //如果a模型对象验证失败 return \"validate/error\"; &#125; return \"redirect:/success\";&#125;","categories":[{"name":"jsr303","slug":"jsr303","permalink":"http://github.com/b2stry/categories/jsr303/"}],"tags":[{"name":"jsr303","slug":"jsr303","permalink":"http://github.com/b2stry/tags/jsr303/"}]},{"title":"Redis基础","slug":"Redis基础","date":"2018-01-30T13:29:30.000Z","updated":"2018-02-22T03:03:03.119Z","comments":true,"path":"2018/01/30/Redis基础/","link":"","permalink":"http://github.com/b2stry/2018/01/30/Redis基础/","excerpt":"Redis典型使用场景 缓存系统 计数器 消息队列系统 排行榜 社交网络 实时系统","text":"Redis典型使用场景 缓存系统 计数器 消息队列系统 排行榜 社交网络 实时系统 Redis API的使用和理解通用命令、字符串类型、哈希类型、列表类型、集合类型、有序集合类型 通用命令 keys [pattern]：遍历所有key (keys命令一般不在生产环境使用，keys* 怎么用？1.热备从节点2.scan) —-O(n) dbsize：计算key的总数—-O(1) exists keys：检查key是否存在(存在返回1，不存在返回0)—-O(1) del key [key …]：删除指定key-value—-O(1) expire key seconds：key在seconds秒后过期—-O(1) ttl key：查看key剩余的过期时间—-O(1) persist key：去掉key的过期时间—-O(1) type key：返回key的类型—-O(1) 字符串字符串键值结构(不能超过512MB)1234key valuehello worldcounter 1bits 10111101 命令 get key：获取key对应的value—-O(1) set key value：设置—-O(1) del key：删除—-O(1) incr key：key自增1，如果key不存在，自增后get(key)=1—-O(1) decr key：key自减1，如果key不存在，自减后get(key)=-1—-O(1) incrby key k：key自增k，如果key不存在，自增后get(key)=k—-O(1) decrby key k：key自减k，如果key不存在，自减后get(key)=-k—-O(1) 实战：1.记录网站每个用户个人主页的访问量？ incr userid:pageview(单线程：无竞争) 2.缓存视频的基本信息（数据源在MySQL中）伪代码？1234567891011public VideoInfo get(long id) &#123; String redisKey = redisPrefix + id; VideoInfo videoInfo = redis.get(redisKey); if(videoInfo == null)&#123; videoInfo = mysql.get(id); if(videoInfo != null)&#123; //序列化 redis.set(redisKey,serialize(videoInfo)); &#125; &#125;&#125; 3.分布式id生成器？ incr id 重要API set key value:不管key是否存在，都设置—-O(1) setnx key value:key不存在才设置—-O(1) set key value xx: key存在 才设置—-O(1) mget key1 key2 key3:批量获取key，原子操作—-O(n) 12n次get = n次网络时间 + n次命令时间1次mget = 1次网络时间 + n次命令时间 mset key1 value1 key2 value2 key3 value3:批量设置key-value，原子操作—-O(n) getset key newvalue：set key newvalue并返回旧的value—-O(1) append key value：将value追加到旧的value—-O(1) strlen key：返回字符串的长度(注意中文占2个字节)—-O(1) incrbyfloat key 3.5：增加key对应的值3.5—-O(1) getrange key start end：获取字符串指定下标所有的值—-O(1) setrange key index value：设置指定下标所有对应的值—-O(1) 哈希哈希键值结构12345 key field value name Shallowanuser:1:info age 21 Date 201 viewCounter 50 特点 Mapmap？ Small redis field不能相同，value可以相同 重要API hget key field：获取hash key对应的field的value—-O(1) hset key field value：设置hash key对应field的value—-O(1) hdel key field：删除hash key对应field的value—-O(1) hexists key field：判断hash key是否有field—-O(1) hlen key：获取hash key field的数量—-O(1) hmget key field1 field2 … fieldN：批量获取hash key的一批field对应的值—-O(n) hmset key field1 value1 field2 value2 … fieldN valueN：批量设置hash key的一批field value—-O(n) 实战：1.记录网站每个用户个人主页的访问量？ hincrby user:1:info pageview count 2.缓存视频的基本信息（数据源在MySQL中）伪代码？123456789101112public VideoInfo get(long id) &#123; String redisKey = redisPrefix + id; Map&lt;String, String&gt; hashMap = redis.hgetAll(redisKey); VideoInfo videoInfo = transferMapToVider(hashMap); if(videoInfo == null)&#123; videoInfo = mysql.get(id); if(videoInfo != null)&#123; //序列化 redis.hmset(redisKey,transferVideoToMap(videoInfo)); &#125; &#125;&#125; hgetall key:返回hash key对应所有的field和value—-O(n) hvals key:返回hash key对应所有field的value—-O(n) hkeys key:返回hash key对应所有的field—-O(n) 相似的API12345678hash stringget hgetset setnx hset hsetnxdel hdelincr incrby decr decrby hincrbymset hmsetmget hmget 用户信息(String 实现)123key value(serializable:json,xml,protobuf)user:1 &#123;&quot;xxx&quot;:&quot;xxx&quot;&#125; 1234 方案 优点 缺点string 编程简单、可能节约内存 1.序列化开销 2.设置属性要操作整个数据hash 直观、节省空间、可以部分更新 1.编程稍微复杂 2.ttl不好控制 hsetnx key field value:设置hash key对应field的value(如field已经存在，则失败)—-O(1) hincrby key field intCounter：hash key对应的field的value自增intCounter—-O(1) hincrbyfloat key field floatCounter:hincrby浮点数版—-O(1) 列表列表结构12 key elementsuser:1:message a-b-c-d-e-f 特点 有序 可以重复 左右两边插入弹出 重要API增 rpush key value1 value2…valueN:从列表右端插入—-O(1~n) lpush key value1 value2…valueN:从列表左端插入 —-O(1~n) linsert key before|after value newValue：在list指定的值前|后插入newValue—-O(n) 删 lpop key:从列表左侧弹出一个item—-O(1) rpop key:从列表右侧弹出一个item—-O(1) lrem key count value:根据count值，从列表中删除所有value相等的项—-O(n) 1231.count&gt;0,从左到右，删除最多count个value相等的项----O(1)2.count&lt;0,从右到左，删除最多Math.abs(count)删除最多count个value相等的项----O(1)3.count=0,删除所有value相等的项----O(1) ltrim key start end :按照索引范围修剪列表—-O(n) 查 lrange key start end(包含end)：获取列表指定索引范围所有item —-O(n) lindex key index:获取列表指定索引的item —-O(n) llen key:获取列表长度 改 lset key index newValue:设置列表指定索引值为newValue—-O(n) EXT blpop key timeout：lpop阻塞版本，timeout是阻塞超时时间，timeout=0为永远不阻塞 brpop key timeout：rpop阻塞版本，timeout是阻塞超时时间，timeout=0为永远不阻塞 TIPS1.LRUSH+LPOP=Stack 2.LPUSH+RPOP=Queue 3.LPUSH+LTRIM=Capped Collection 4.LPUSH+BRPOP=Message Queue 集合集合结构12 key valuesuser:1:follow it、music、movie、his、sports 特点无序、无重复、集合间操作 集合内API sadd key element:向集合key添加element(如果element已经存在，添加失败)—-O(1) srem key element:将集合key中的element移除掉—-O(1) scard key :计算集合大小—-O(1) sismember key value:判断value是否存在集合中—-O(1) srandmember key counter :从集合中随机挑count个元素—-O(1) spop key:从集合中随机弹出一个元素—-O(1) smembers key :获取集合所有元素 (无序、小心使用)—-O(1) 1spop从集合弹出、srandmember不会破坏集合 集合间API sdiff key1 key2 :差集—-O(1) sinter key1 key2 :交集—-O(1) sunion key1 key2 :并集—-O(1) sdiff|sinter|sunion + store destkey :将结果保存在destkey中—-O(1) TIPS1.SADD = Tagging 2.SPOP/SRANDMEMBER = Random item 3.SADD+SINTER = Social Graph 有序集合有序集合结构1234567 key score value 1 krisuser:ranking 91 mike 200 frank 220 chris 250 martin 251 tom 集合 Vs 有序集合 无重复元素 - 无重复元素 无序 - 有序 element - element + score 列表 Vs 有序集合 可以有重复元素 - 无重复元素 无序 - 有序 element - element + score 重要API zadd key score element(可以是多对)：添加score和element —-O(logN) zrem key element(可以是多个)：删除元素—-O(1) zscore key element：返回元素的分数—-O(1) zincrby key increScore element：增加或减少元素的分数—-O(1) zcard key：返回元素的总个数—-O(1) zrange key start end [WITHSCORES]：返回指定索引范围内的升序元素[分值]—-O(log(n)+m) zrangbyscore key minScore maxScore [WITHSCORES]：返回指定分数范围内的升序元素[分值]—-O(log(n)+m) zcount key minScore maxScore：返回有序集合内在指定分数范围内的个数—-O(log(n)+m) zremrangebyrank key start end：删除指定排名内的升序元素—-O(log(n)+m) zremrangebyscore key minScore maxScore：删除指定分数内的升序元素—-O(log(n)+m)","categories":[{"name":"redis","slug":"redis","permalink":"http://github.com/b2stry/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://github.com/b2stry/tags/redis/"}]},{"title":"分布式和集群区别？什么是云计算平台？分布式的应用场景？","slug":"分布式和集群区别？什么是云计算平台？分布式的应用场景？","date":"2018-01-24T15:23:56.000Z","updated":"2018-02-28T08:48:23.864Z","comments":true,"path":"2018/01/24/分布式和集群区别？什么是云计算平台？分布式的应用场景？/","link":"","permalink":"http://github.com/b2stry/2018/01/24/分布式和集群区别？什么是云计算平台？分布式的应用场景？/","excerpt":"分布式是指将一个业务拆分不同的子业务，分布在不同的机器上执行，集群是指多台服务器集中在一起，实现同一业务，可以视为一台计算机，一个云计算平台，就是通过一套软件系统把分布式部署的资源集中调度使用。要应对大并发，要实现高可用，既需要分布式，也离不开集群。","text":"分布式是指将一个业务拆分不同的子业务，分布在不同的机器上执行，集群是指多台服务器集中在一起，实现同一业务，可以视为一台计算机，一个云计算平台，就是通过一套软件系统把分布式部署的资源集中调度使用。要应对大并发，要实现高可用，既需要分布式，也离不开集群。 分布式和集群区别？分布式分布式：是指将一个业务拆分不同的子业务，分布在不同的机器上执行。 常用的分布式就是在负载均衡服务器后加一堆web服务器，然后在上面搞一个缓存服务器来保存临时状态，后面共享一个数据库。 如图所示:这种环境下真正进行分布式的只是web server而已，并且web server之间没有任何联系，所以结构和实现都非常简单。 集群集群：是指多台服务器集中在一起，实现同一业务，可以视为一台计算机。 多台服务器组成的一组计算机，作为一个整体存在，向用户提供一组网络资源，这些单个的服务器就是集群的节点。 两个特点可扩展性：集群中的服务节点，可以动态的添加机器，从而增加集群的处理能力。 高可用性：如果集群某个节点发生故障，这台节点上面运行的服务，可以被其他服务节点接管，从而增强集群的高可用性。 集群分类常用的集群分类 1.高可用集群(High Availability Cluster) 高可用集群,普通两节点双机热备，多节点HA集群。 2.负载均衡集群(Load Balance Cluster) 常用的有 Nginx 把请求分发给后端的不同web服务器，还有就是数据库集群，负载均衡就是，为了保证服务器的高可用，高并发。 3.科学计算集群(High Performance Computing Cluster) 简称HPC集群。这类集群致力于提供单个计算机所不能提供的强大的计算能力。 两大能力负载均衡：负载均衡能把任务比较均衡地分布到集群环境下的计算和网络资源。 集群容错：当我们的系统中用到集群环境,因为各种原因在集群调用失败时，集群容错起到关键性的作用。 例如 Dubbo 的集群容错： Failover Cluster 失败自动切换，当出现失败，重试其它服务器，通常用于读操作，但重试会带来更长延迟。 Failfast Cluster 快速失败，只发起一次调用，失败立即报错，通常用于非幂等性的写操作，比如新增记录。 Failback Cluster 失败自动恢复，后台记录失败请求，定时重发，通常用于消息通知操作。 Forking Cluster 并行调用多个服务器，只要一个成功即返回，通常用于实时性要求较高的读操作，但需要浪费更多服务资源。 简单总结分布式，从狭义上理解，也与集群差不多，但是它的组织比较松散，不像集群，有一定组织性，一台服务器宕了，其他的服务器可以顶上来。 分布式的每一个节点，都完成不同的业务，一个节点宕了，这个业务就不可访问了。 1. 分布式是指将一个业务拆分不同的子业务，分布在不同的机器上执行。 2. 集群是指多台服务器集中在一起，实现同一业务，可以视为一台计算机。 分布式的每一个节点，都可以用来做集群。而集群不一定就是分布式了。 什么是云计算平台？一个云计算平台，就是通过一套软件系统把分布式部署的资源集中调度使用。要应对大并发，要实现高可用，既需要分布式，也离不开集群。 比如负载均衡，如果只是一台服务器，这台宕机了就完蛋了。 分布式的难点，就是很多机器做存在依赖关系的不同活儿，这些活儿需要的资源、时间区别可能很大，某些机器还可能罢工，要怎么样才能协调好，做到效率最高，消耗最少，不出错。 分布式的应用场景？平时接触到的分布式系统有很多种，比如分布式文件系统，分布式数据库，分布式WebService，分布式计算等等，面向的情景不同，但分布式的思路是否是一样的呢? 1.简单的例子假设我们有一台服务器，它可以承担1百万/秒的请求，这个请求可以的是通过http访问网页，通过tcp下载文件，jdbc执行sql，RPC调用接口…，现在我们有一条数据的请求是2百万/秒，很显然服务器hold不住了，会各种拒绝访问，甚至崩溃，宕机，怎么办呢。 一台机器解决不了的问题，那就两台。所以我们加一台机器，每台承担1百万。如果请求继续增加呢，两台解决不了的问题，那就三台呗。 这种方式我们称之为水平扩展。如何实现请求的平均分配便是负载均衡了。 另一个栗子，我们现在有两个数据请求，数据1 90万，数据2 80万，上面那台机器也hold不住，我们加一台机器来负载均衡一下，每台机器处理45万数据1和40万数据2，但是平分太麻烦，不如一台处理数据1，一台处理数据2，同样能解决问题，这种方式我们称之为垂直拆分。 水平扩展和垂直拆分是分布式架构的两种思路，但并不是一个二选一的问题，更多的是兼并合用。下面介绍一个实际的场景。这也是许多互联网的公司架构思路。 2.实际的例子我此时所在的公司的计算机系统很庞大，自然是一个整的分布式系统，为了方便组织管理，公司将整个技术部按业务和平台拆分为部门，订单的，会员的，商家的等等，每个部门有自己的web服务器集群，数据库服务器集群，通过同一个网站访问的链接可能来自于不同的服务器和数据库，对网站及底层对数据库的访问被分配到了不同的服务器集群,这个便是典型的按业务做的垂直拆分，每个部门的服务器在hold不住时，会有弹性的扩展，这便是水平扩展。 在数据库层，有些表非常大，数据量在亿级，如果只是纯粹的水平的扩展并不一定最好，如果对表进行拆分，比如可以按用户id进行水平拆表，通过对id取模的方式，将用户划分到多张表中，同时这些表也可以处在不同的服务器。按业务的垂直拆库和按用户水平拆表是分布式数据库中通用的解决方案。 比如 Mycat 开源分布式数据库中间件 http://www.mycat.io/ 3.分布式一致性分布式系统中，解决了负载均衡的问题后，另外一个问题就是数据的一致性了，这个就需要通过同步来保障。根据不同的场景和需求，同步的方式也是有选择的。 在分布式文件系统中，比如商品页面的图片，如果进行了修改，同步要求并不高，就算有数秒甚至数分钟的延迟都是可以接受的，因为一般不会产生损失性的影响，因此可以简单的通过文件修改的时间戳，隔一定时间扫描同步一次，可以牺牲一致性来提高效率。 但银行中的分布式数据库就不一样了，一丁点不同步就是无法接受的，甚至可以通过加锁等牺牲性能的方式来保障完全的一致。 在一致性算法中paxos算法是公认的最好的算法，Chubby、ZooKeeper 中Paxos是它保证一致性的核心。这个算法比较难懂，我目前也没弄懂，这里就不深入了。 作者：鹏磊出处：http://www.ymq.io/2018/01/23/Distributed-cluster/","categories":[{"name":"集群","slug":"集群","permalink":"http://github.com/b2stry/categories/集群/"},{"name":"分布式","slug":"集群/分布式","permalink":"http://github.com/b2stry/categories/集群/分布式/"},{"name":"云计算","slug":"集群/分布式/云计算","permalink":"http://github.com/b2stry/categories/集群/分布式/云计算/"}],"tags":[{"name":"集群","slug":"集群","permalink":"http://github.com/b2stry/tags/集群/"},{"name":"分布式","slug":"分布式","permalink":"http://github.com/b2stry/tags/分布式/"},{"name":"云计算","slug":"云计算","permalink":"http://github.com/b2stry/tags/云计算/"}]},{"title":"搜索引擎之Elasticsearch","slug":"全文搜索引擎-Elasticsearch-入门教程","date":"2018-01-24T04:24:00.000Z","updated":"2018-02-21T13:33:23.058Z","comments":true,"path":"2018/01/24/全文搜索引擎-Elasticsearch-入门教程/","link":"","permalink":"http://github.com/b2stry/2018/01/24/全文搜索引擎-Elasticsearch-入门教程/","excerpt":"全文搜索属于最常见的需求，开源的 Elasticsearch （以下简称 Elastic）是目前全文搜索引擎的首选。 它可以快速地储存、搜索和分析海量数据。维基百科、Stack Overflow、Github都采用它。","text":"全文搜索属于最常见的需求，开源的 Elasticsearch （以下简称 Elastic）是目前全文搜索引擎的首选。 它可以快速地储存、搜索和分析海量数据。维基百科、Stack Overflow、Github都采用它。Elastic 的底层是开源库 Lucene。但是，你没法直接用 Lucene，必须自己写代码去调用它的接口。Elastic 是 Lucene 的封装，提供了 REST API 的操作接口，开箱即用。 本文从零开始，讲解如何使用 Elastic 搭建自己的全文搜索引擎。每一步都有详细的说明，大家跟着做就能学会。 一、安装Elastic 需要 Java 8 环境。如果你的机器还没安装 Java，可以参考这篇文章，注意要保证环境变量JAVA_HOME正确设置。安装完 Java，就可以跟着官方文档安装 Elastic。直接下载压缩包比较简单。123$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.1.zip$ unzip elasticsearch-5.5.1.zip$ cd elasticsearch-5.5.1/ 接着，进入解压后的目录，运行下面的命令，启动Elastic。1$ ./bin/elasticsearch 如果这时报错”max virtual memory areas vm.maxmapcount [65530] is too low“，要运行下面的命令。1$ sudo sysctl -w vm.max_map_count=262144 如果一切正常，Elastic 就会在默认的9200端口运行。这时，打开另一个命令行窗口，请求该端口，会得到说明信息。1$ curl localhost:9200 12345678910111213&#123; \"name\" : \"atntrTf\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"tf9250XhQ6ee4h7YI11anA\", \"version\" : &#123; \"number\" : \"5.5.1\", \"build_hash\" : \"19c13d0\", \"build_date\" : \"2017-07-18T20:44:24.823Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.6.0\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 上面代码中，请求9200端口，Elastic 返回一个 JSON对象，包含当前节点、集群、版本等信息。 按下 Ctrl + C，Elastic 就会停止运行。 默认情况下，Elastic 只允许本机访问，如果需要远程访问，可以修改 Elastic 安装目录的config/elasticsearch.yml文件，去掉network.host的注释，将它的值改成0.0.0.0，然后重新启动 Elastic。1network.host: 0.0.0.0 上面代码中，设成0.0.0.0让任何人都可以访问。线上服务不要这样设置，要设成具体的 IP。 二、基本概念2.1 Node 与 ClusterElastic本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elastic 实例。单个 Elastic 实例称为一个节点（node）。一组节点构成一个集群（cluster）。 2.2 IndexElastic 会索引所有字段，经过处理后写入一个反向索引（Inverted Index）。查找数据的时候，直接查找该索引。 所以，Elastic 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。下面的命令可以查看当前节点的所有 Index。1$ curl -X GET &apos;http://localhost:9200/_cat/indices?v&apos; 2.3 DocumentIndex 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。Document 使用 JSON 格式表示，下面是一个例子。12345&#123; \"user\": \"张三\", \"title\": \"工程师\", \"desc\": \"数据库管理\"&#125; 同一个 Index 里面的 Document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。 2.4 TypeDocument 可以分组，比如weather这个 Index 里面，可以按城市分组（北京和上海），也可以按气候分组（晴天和雨天）。这种分组就叫做 Type，它是虚拟的逻辑分组，用来过滤 Document。 不同的 Type 应该有相似的结构（schema），举例来说，id字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如products和logs）应该存成两个 Index，而不是一个 Index 里面的两个 Type（虽然可以做到）。下面的命令可以列出每个 Index 所包含的 Type。 1$ curl &apos;localhost:9200/_mapping?pretty=true&apos; 根据规划，Elastic 6.x 版只允许每个Index 包含一个Type，7.x版将会彻底移除 Type。 三、新建和删除 Index新建Index，可以直接向Elastic服务器发出 PUT 请求。下面的例子是新建一个名叫weather的Index。1$ curl -X PUT &apos;localhost:9200/weather&apos; 服务器返回一个 JSON 对象，里面的acknowledged字段表示操作成功。1234&#123; \"acknowledged\":true, \"shards_acknowledged\":true&#125; 然后，我们发出 DELETE 请求，删除这个 Index。1$ curl -X DELETE &apos;localhost:9200/weather&apos; 四、中文分词设置首先，安装中文分词插件。这里使用的是 ik，也可以考虑其他插件（比如 smartcn）。1$ ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip 上面代码安装的是5.5.1版的插件，与 Elastic 5.5.1 配合使用。 接着，重新启动 Elastic，就会自动加载这个新安装的插件。 然后，新建一个 Index，指定需要分词的字段。这一步根据数据结构而异，下面的命令只针对本文。基本上，凡是需要搜索的中文字段，都要单独设置一下。123456789101112131415161718192021222324$ curl -X PUT &apos;localhost:9200/accounts&apos; -d &apos;&#123; &quot;mappings&quot;: &#123; &quot;person&quot;: &#123; &quot;properties&quot;: &#123; &quot;user&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;desc&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125;&apos; 上面代码中，首先新建一个名称为accounts的Index，里面有一个名称为person的 Type。person有三个字段。123usertitledesc 这三个字段都是中文，而且类型都是文本（text），所以需要指定中文分词器，不能使用默认的英文分词器。 Elastic 的分词器称为 analyzer。我们对每个字段指定分词器。12345\"user\": &#123; \"type\": \"text\", \"analyzer\": \"ik_max_word\", \"search_analyzer\": \"ik_max_word\"&#125; 上面代码中，analyzer是字段文本的分词器，search_analyzer是搜索词的分词器。ik_max_word分词器是插件ik提供的，可以对文本进行最大数量的分词。 五、数据操作5.1 新增记录向指定的 /Index/Type 发送PUT 请求，就可以在Index 里面新增一条记录。比如，向/accounts/person发送请求，就可以新增一条人员记录。123456$ curl -X PUT &apos;localhost:9200/accounts/person/1&apos; -d &apos;&#123; &quot;user&quot;: &quot;张三&quot;, &quot;title&quot;: &quot;工程师&quot;, &quot;desc&quot;: &quot;数据库管理&quot;&#125;&apos; 服务器返回的 JSON 对象，会给出 Index、Type、Id、Version 等信息。123456789&#123; \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"1\", \"_version\":1, \"result\":\"created\", \"_shards\":&#123;\"total\":2,\"successful\":1,\"failed\":0&#125;, \"created\":true&#125; 如果你仔细看，会发现请求路径是/accounts/person/1，最后的1是该条记录的Id。它不一定是数字，任意字符串（比如abc）都可以。 新增记录的时候，也可以不指定 Id，这时要改成 POST 请求。123456$ curl -X POST &apos;localhost:9200/accounts/person&apos; -d &apos;&#123; &quot;user&quot;: &quot;李四&quot;, &quot;title&quot;: &quot;工程师&quot;, &quot;desc&quot;: &quot;系统管理&quot;&#125;&apos; 上面代码中，向/accounts/person发出一个 POST 请求，添加一个记录。这时，服务器返回的 JSON 对象里面，_id字段就是一个随机字符串。123456789&#123; \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"AV3qGfrC6jMbsbXb6k1p\", \"_version\":1, \"result\":\"created\", \"_shards\":&#123;\"total\":2,\"successful\":1,\"failed\":0&#125;, \"created\":true&#125; 注意，如果没有先创建 Index（这个例子是accounts），直接执行上面的命令，Elastic 也不会报错，而是直接生成指定的 Index。所以，打字的时候要小心，不要写错 Index 的名称。5.2 查看记录向/Index/Type/Id发出 GET 请求，就可以查看这条记录。1$ curl &apos;localhost:9200/accounts/person/1?pretty=true&apos; 上面代码请求查看/accounts/person/1这条记录，URL 的参数pretty=true表示以易读的格式返回。 返回的数据中，found字段表示查询成功，_source字段返回原始记录。123456789101112&#123; \"_index\" : \"accounts\", \"_type\" : \"person\", \"_id\" : \"1\", \"_version\" : 1, \"found\" : true, \"_source\" : &#123; \"user\" : \"张三\", \"title\" : \"工程师\", \"desc\" : \"数据库管理\" &#125;&#125; 如果 Id 不正确，就查不到数据，found字段就是false。1$ curl &apos;localhost:9200/weather/beijing/abc?pretty=true&apos; 123456&#123; \"_index\" : \"accounts\", \"_type\" : \"person\", \"_id\" : \"abc\", \"found\" : false&#125; 5.3 删除记录删除记录就是发出 DELETE 请求。1$ curl -X DELETE &apos;localhost:9200/accounts/person/1&apos; 这里先不要删除这条记录，后面还要用到。 5.4 更新记录更新记录就是使用 PUT 请求，重新发送一次数据。123456$ curl -X PUT &apos;localhost:9200/accounts/person/1&apos; -d &apos;&#123; &quot;user&quot; : &quot;张三&quot;, &quot;title&quot; : &quot;工程师&quot;, &quot;desc&quot; : &quot;数据库管理，软件开发&quot;&#125;&apos; 123456789&#123; \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"1\", \"_version\":2, \"result\":\"updated\", \"_shards\":&#123;\"total\":2,\"successful\":1,\"failed\":0&#125;, \"created\":false&#125; 上面代码中，我们将原始数据从”数据库管理”改成”数据库管理，软件开发”。 返回结果里面，有几个字段发生了变化。123\"_version\" : 2,\"result\" : \"updated\",\"created\" : false 可以看到，记录的 Id 没变，但是版本（version）从1变成2，操作类型（result）从created变成updated，created字段变成false，因为这次不是新建记录。 六、数据查询6.1 返回所有记录使用 GET 方法，直接请求/Index/Type/_search，就会返回所有记录。1$ curl &apos;localhost:9200/accounts/person/_search&apos; 123456789101112131415161718192021222324252627282930313233&#123; \"took\":2, \"timed_out\":false, \"_shards\":&#123;\"total\":5,\"successful\":5,\"failed\":0&#125;, \"hits\":&#123; \"total\":2, \"max_score\":1.0, \"hits\":[ &#123; \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"AV3qGfrC6jMbsbXb6k1p\", \"_score\":1.0, \"_source\": &#123; \"user\": \"李四\", \"title\": \"工程师\", \"desc\": \"系统管理\" &#125; &#125;, &#123; \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"1\", \"_score\":1.0, \"_source\": &#123; \"user\" : \"张三\", \"title\" : \"工程师\", \"desc\" : \"数据库管理，软件开发\" &#125; &#125; ] &#125;&#125; 上面代码中，返回结果的 took字段表示该操作的耗时（单位为毫秒），timed_out字段表示是否超时，hits字段表示命中的记录，里面子字段的含义如下。123total：返回记录数，本例是2条。max_score：最高的匹配程度，本例是1.0。hits：返回的记录组成的数组。 返回的记录中，每条记录都有一个_score字段，表示匹配的程序，默认是按照这个字段降序排列。 6.2 全文搜索Elastic 的查询非常特别，使用自己的查询语法，要求 GET 请求带有数据体。1234$ curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;软件&quot; &#125;&#125;&#125;&apos; 上面代码使用 Match 查询，指定的匹配条件是desc字段里面包含”软件”这个词。返回结果如下。12345678910111213141516171819202122&#123; \"took\":3, \"timed_out\":false, \"_shards\":&#123;\"total\":5,\"successful\":5,\"failed\":0&#125;, \"hits\":&#123; \"total\":1, \"max_score\":0.28582606, \"hits\":[ &#123; \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"1\", \"_score\":0.28582606, \"_source\": &#123; \"user\" : \"张三\", \"title\" : \"工程师\", \"desc\" : \"数据库管理，软件开发\" &#125; &#125; ] &#125;&#125; Elastic 默认一次返回10条结果，可以通过size字段改变这个设置。12345$ curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;管理&quot; &#125;&#125;, &quot;size&quot;: 1&#125;&apos; 上面代码指定，每次只返回一条结果。 还可以通过from字段，指定位移。123456$ curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;管理&quot; &#125;&#125;, &quot;from&quot;: 1, &quot;size&quot;: 1&#125;&apos; 上面代码指定，从位置1开始（默认是从位置0开始），只返回一条结果。 6.3 逻辑运算如果有多个搜索关键字， Elastic 认为它们是or关系。1234$ curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;软件 系统&quot; &#125;&#125;&#125;&apos; 上面代码搜索的是软件 or 系统。 如果要执行多个关键词的and搜索，必须使用布尔查询。1234567891011$ curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;desc&quot;: &quot;软件&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;desc&quot;: &quot;系统&quot; &#125; &#125; ] &#125; &#125;&#125;&apos; ps:本地快速搭建集群1、./elasticsearch2、./elasticsearch -Ehttp.port=8200 -Epath.data=node23、./elasticsearch -Ehttp.port=7200 -Epath.data=node3七、参考链接ElasticSearch 官方手册 A Practical Introduction to Elasticsearch 作者： 阮一峰原文链接：http://www.ruanyifeng.com/blog/2017/08/elasticsearch.html","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://github.com/b2stry/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://github.com/b2stry/tags/Elasticsearch/"}]},{"title":"慕课网---Spring Boot企业级博客系统实战(数据持久化)","slug":"慕课网-Spring-Boot技术栈博客企业前后端-Spring Data JPA","date":"2018-01-16T10:06:48.000Z","updated":"2018-02-03T07:33:32.246Z","comments":true,"path":"2018/01/16/慕课网-Spring-Boot技术栈博客企业前后端-Spring Data JPA/","link":"","permalink":"http://github.com/b2stry/2018/01/16/慕课网-Spring-Boot技术栈博客企业前后端-Spring Data JPA/","excerpt":"课程目标通过本章节学习 JPA简介 Spring Data JPA 用法介绍 Spring Data JPA、Hibernate与Spring Boot集成 数据持久化实战","text":"课程目标通过本章节学习 JPA简介 Spring Data JPA 用法介绍 Spring Data JPA、Hibernate与Spring Boot集成 数据持久化实战 JPA简介什么是JPA?JPA(Java Persistence API)是用于管理Java EE 和Java SE环境中的持久化，以及对象/关系映射的Java API 实现：EclipseLink、Hibernate、Apache OpenJPA JPA核心概念实体实体表示关系数据库中的表每个实体实例对应于该表中的行类必须用javax.persistence.Entity注解类必须有一个public或protected的无参数的构造方法实体实例被当做值以分离对象方式进行传递（例如通过会话bean的远程业务接口），则该类必须实现Serializable接口唯一的对象标识符：简单主键(javax.persistence.Id)、复合主键(javax.persistence.EmbeddedId和javax.persistence.IdClass) 关系 一对一：@OneToOne 一对多：@OneToMany 多对一：@ManyToOne 多对多：@ManyToMany EntityManager接口 定义用于与持久性上下文进行交互的方法 创建和删除持久实体实例，通过实体的主键查找实体 允许在实体上运行查询 获取EntityManager实例1234567891011121314151617@PersistenceUnitEntityManagerFactory emf;EntityManager em;@ResourceUserTransaction utx;...em = emf.createEntityManager();try &#123; utx.begin(); em.persist(SomeEntity); em.merge(AnotherEntity); em.remove(ThirdEntity); utx.commit();&#125; catch (Exception e) &#123; utx.rollback();&#125; 查找实体1234567@PersistenceContextEntityManager em;public void enterOrder(int custID, CustomerOrder newOrder) &#123; Customer cust = em.find(Customer.class, custID); cust.getOrders().add(newOrder); newOrder.setCustomer(cust);&#125; Spring Data JPA 简介什么是Spring Data JPA 是更大的Spring Data家族的一部分 对基于JPA的数据访问层的增强支持 更容易构建基于使用Spring数据访问技术栈的应用程序 Spring Data JPA常用接口CrudRepository1234567891011121314151617public interface CrudRepository&lt;T, ID extends Serializable&gt; extends Repository&lt;T, ID&gt; &#123; &lt;S extends T&gt; S save(S entity); //(1) T findOne(ID primaryKey); //(2) Iterable&lt;T&gt; findAll(); //(3) Long count(); //(4) void delete(T entity); //(5) boolean exists(ID primaryKey); //(6) // ... more functionality omitted.&#125; PagingAndSortingRepository1234567public interface PagingAndSortingRepository&lt;T, ID extends Serializable&gt; extends CrudRepository&lt;T, ID&gt; &#123; Iterable&lt;T&gt; findAll(Sort sort); Page&lt;T&gt; findAll(Pageable pageable);&#125; Spring Data JPA自定义接口根据方法名创建查询123456789101112131415161718public interface PersonRepository extends Repository&lt;User, Long&gt; &#123; List&lt;Person&gt; findByEmailAddressAndLastname(EmailAddress emailAddress, String lastname); //启用 distinct 标志 List&lt;Person&gt; findDistinctPeopleByLastnameOrFirstname(String lastname, String firstname); List&lt;Person&gt; findPeopleDistinctByLastnameOrFirstname(String lastname, String firstname); //给独立的属性启用 ignore case List&lt;Person&gt; findByLastnameIgnoreCase(String lastname); //给所有合适的属性启用 ignore case List&lt;Person&gt; findByLastnameAndFirstnameAllIgnoreCase(String lastname, String firstname); //启用ORDER BY List&lt;Person&gt; findByLastnameOrderByFirstnameAsc(String lastname); List&lt;Person&gt; findByLastnameOrderByFirstnameDesc(String lastname);&#125; Spring Data JPA、Hibernate与Spring Boot集成修改build.gradle1234567891011//依赖关系dependencies &#123; ... // 添加 Spring Data JPA 的依赖 compile(&apos;org.springframework.boot:spring-boot-starter-data-jpa&apos;) // 添加 MySQL连接驱动 的依赖 compile(&apos;mysql:mysql-connector-java:6.0.5&apos;) ...&#125; 123456buildscript &#123; ... // 自定义 Hibernate 的版本 ext[&apos;hibernate.version&apos;] = &apos;5.2.8.Final&apos; ...&#125;","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://github.com/b2stry/categories/Spring-Boot/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://github.com/b2stry/tags/Spring-Boot/"}]},{"title":"Thymeleaf 3学习笔记","slug":"Thymeleaf-3学习笔记","date":"2018-01-16T06:17:08.000Z","updated":"2018-01-30T14:03:32.669Z","comments":true,"path":"2018/01/16/Thymeleaf-3学习笔记/","link":"","permalink":"http://github.com/b2stry/2018/01/16/Thymeleaf-3学习笔记/","excerpt":"Thymeleaf 目前最新版本3.0 Thymeleaf作为Spring-Boot官方推荐模板引擎，而且支持纯HTML浏览器展现(模板表达式在脱离运行环境下不污染html结构).是时候了解一番了。","text":"Thymeleaf 目前最新版本3.0 Thymeleaf作为Spring-Boot官方推荐模板引擎，而且支持纯HTML浏览器展现(模板表达式在脱离运行环境下不污染html结构).是时候了解一番了。 安装与初始化配置与Spring集成12345&lt;dependency&gt; &lt;groupId&gt;org.thymeleaf&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-spring4&lt;/artifactId&gt; &lt;version&gt;3.0.0.RELEASE&lt;/version&gt;&lt;/dependency&gt; 与Spring-Boot集成：1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 在Spring中进行配置：1234567891011121314151617181920212223242526272829303132333435@Configuration@EnableWebMvc@ComponentScan(\"com.thymeleafexamples\")public class ThymeleafConfig extends WebMvcConfigurerAdapter implements ApplicationContextAware &#123; private ApplicationContext applicationContext; public void setApplicationContext(ApplicationContext applicationContext) &#123; this.applicationContext = applicationContext; &#125; @Bean public ViewResolver viewResolver() &#123; ThymeleafViewResolver resolver = new ThymeleafViewResolver(); resolver.setTemplateEngine(templateEngine()); resolver.setCharacterEncoding(\"UTF-8\"); return resolver; &#125; @Bean public TemplateEngine templateEngine() &#123; SpringTemplateEngine engine = new SpringTemplateEngine(); engine.setEnableSpringELCompiler(true); engine.setTemplateResolver(templateResolver()); return engine; &#125; private ITemplateResolver templateResolver() &#123; SpringResourceTemplateResolver resolver = new SpringResourceTemplateResolver(); resolver.setApplicationContext(applicationContext); resolver.setPrefix(\"/WEB-INF/templates/\"); resolver.setTemplateMode(TemplateMode.HTML); return resolver; &#125;&#125; 在Spring-Boot中只需如下配置：1234567#thymeleaf startspring.thymeleaf.mode=HTML5spring.thymeleaf.encoding=UTF-8spring.thymeleaf.content-type=text/html#开发时关闭缓存,不然没法看到实时页面spring.thymeleaf.cache=false#thymeleaf end 具体可以配置的参数可以查看 org.springframework.boot.autoconfigure.thymeleaf.ThymeleafProperties这个类,上面的配置实际上就是注入到该类中的属性值. 基本语法1234567891011&lt;!DOCTYPE HTML&gt;&lt;html xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;title&gt;hello&lt;/title&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" /&gt;&lt;/head&gt;&lt;body&gt;&lt;!--/*@thymesVar id=\"name\" type=\"java.lang.String\"*/--&gt;&lt;p th:text=\"'Hello！, ' + $&#123;name&#125; + '!'\" &gt;3333&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 表达式 Variable Expressions: ${…} Selection Variable Expressions: *{…} Message Expressions: #{…} Link URL Expressions: @{…} Fragment Expressions: ~{…} 字符串操作: String concatenation: + Literal substitutions: |The name is ${name}| 条件操作： If-then: (if) ? (then) If-then-else: (if) ? (then) : (else) Default: (value) ?: (defaultvalue) No-Operation: _如：&#39;User is of type &#39; + (${user.isAdmin()} ? &#39;Administrator&#39; : (${user.type} ?: &#39;Unknown&#39;)) 1、获取变量值&lt;p th:text=&quot;&#39;Hello！, &#39; + ${name} + &#39;!&#39;&quot; &gt;3333&lt;/p&gt;可以看出获取变量值用$符号,对于javaBean的话使用变量名.属性名方式获取,这点和EL表达式一样.另外$表达式只能写在th标签内部,不然不会生效 #{}是国际化支持取值的符号 注意:th:text与th:utext的区别，输出中文时应该使用th:utext ${..}实际语法是:OGNL(非web),SpEL(web) ,支持的内置变量 便捷部分 ${x} will return a variable x stored into the Thymeleaf context or as a request attribute. ${param.x} will return a request parameter called x (which might be multivalued). ${session.x} will return a session attribute called x. ${application.x} will return a servlet context attribute called x.基本的1234567#ctx: the context object.#vars: the context variables.#locale: the context locale.#request: (only in Web Contexts) the HttpServletRequest object.#response: (only in Web Contexts) the HttpServletResponse object.#session: (only in Web Contexts) the HttpSession object.#servletContext: (only in Web Contexts) the ServletContext object. 工具对象 12345678910111213141516#execInfo: information about the template being processed.#messages: methods for obtaining externalized messages inside variables expressions, in the same way as they would be obtained using #&#123;…&#125; syntax.#uris: methods for escaping parts of URLs/URIs#conversions: methods for executing the configured conversion service (if any).#dates: methods for java.util.Date objects: formatting, component extraction, etc.#calendars: analogous to #dates, but for java.util.Calendar objects.#numbers: methods for formatting numeric objects.#strings: methods for String objects: contains, startsWith, prepending/appending, etc.#objects: methods for objects in general.#bools: methods for boolean evaluation.#arrays: methods for arrays.#lists: methods for lists.#sets: methods for sets.#maps: methods for maps.#aggregates: methods for creating aggregates on arrays or collections.#ids: methods for dealing with id attributes that might be repeated (for example, as a result of an iteration). 工具对象的使用方式见：http://www.thymeleaf.org/doc/...， 以下仅仅举几个例子 1234567891011121314151617181920$&#123;#dates.format(date, &apos;dd/MMM/yyyy HH:mm&apos;)&#125;$&#123;#dates.arrayFormat(datesArray, &apos;dd/MMM/yyyy HH:mm&apos;)&#125;$&#123;#dates.listFormat(datesList, &apos;dd/MMM/yyyy HH:mm&apos;)&#125;$&#123;#dates.setFormat(datesSet, &apos;dd/MMM/yyyy HH:mm&apos;)&#125;$&#123;#dates.createNow()&#125;$&#123;#dates.createToday()&#125; //time set to 00:00$&#123;#strings.isEmpty(name)&#125; //Check whether a String is empty (or null)$&#123;#strings.arrayIsEmpty(nameArr)&#125;$&#123;#strings.listIsEmpty(nameList)&#125;$&#123;#strings.setIsEmpty(nameSet)&#125;$&#123;#strings.startsWith(name,&apos;Don&apos;)&#125; // also array*, list* and set*$&#123;#strings.endsWith(name,endingFragment)&#125; // also array*, list* and set*$&#123;#strings.length(str)&#125;$&#123;#strings.equals(str)&#125;$&#123;#strings.equalsIgnoreCase(str)&#125;$&#123;#strings.concat(str)&#125;$&#123;#strings.concatReplaceNulls(str)&#125; 用*{…} 选择对象里的变量，如123456789101112&lt;div th:object=\"$&#123;session.user&#125;\"&gt; &lt;p&gt;Name: &lt;span th:text=\"*&#123;firstName&#125;\"&gt;Sebastian&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Surname: &lt;span th:text=\"*&#123;lastName&#125;\"&gt;Pepper&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Nationality: &lt;span th:text=\"*&#123;nationality&#125;\"&gt;Saturn&lt;/span&gt;.&lt;/p&gt; &lt;/div&gt;//等价于&lt;div&gt; &lt;p&gt;Name: &lt;span th:text=\"$&#123;session.user.firstName&#125;\"&gt;Sebastian&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Surname: &lt;span th:text=\"$&#123;session.user.lastName&#125;\"&gt;Pepper&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Nationality: &lt;span th:text=\"$&#123;session.user.nationality&#125;\"&gt;Saturn&lt;/span&gt;.&lt;/p&gt;&lt;/div&gt; 字面量 Text literals: ‘one text’, ‘Another one!’,… Number literals: 0, 34, 3.0, 12.3,… Boolean literals: true, false Null literal: null字符串一般需要包围在’单引号内，但也有几种变通方式12345&lt;div th:class=\"'content'\"&gt;...&lt;/div&gt;&lt;span th:text=\"|Welcome to our application, $&#123;user.name&#125;!|\"&gt;//Which is equivalent to:&lt;span th:text=\"'Welcome to our application, ' + $&#123;user.name&#125; + '!'\"&gt;&lt;span th:text=\"$&#123;onevar&#125; + ' ' + |$&#123;twovar&#125;, $&#123;threevar&#125;|\"&gt; 定义模板本地变量1234567891011121314&lt;div th:with=\"firstPer=$&#123;persons[0]&#125;\"&gt; &lt;p&gt; The name of the first person is &lt;span th:text=\"$&#123;firstPer.name&#125;\"&gt;Julius Caesar&lt;/span&gt;. &lt;/p&gt;&lt;/div&gt;&lt;div th:with=\"firstPer=$&#123;persons[0]&#125;,secondPer=$&#123;persons[1]&#125;\"&gt; &lt;p&gt; The name of the first person is &lt;span th:text=\"$&#123;firstPer.name&#125;\"&gt;Julius Caesar&lt;/span&gt;. &lt;/p&gt; &lt;p&gt; But the name of the second person is &lt;span th:text=\"$&#123;secondPer.name&#125;\"&gt;Marcus Antonius&lt;/span&gt;. &lt;/p&gt;&lt;/div&gt; 2.引入URLThymeleaf对于URL的处理是通过语法@{…}来处理的123&lt;a th:href=\"@&#123;http://blog.csdn.net/u012706811&#125;\"&gt;绝对路径&lt;/a&gt;&lt;a th:href=\"@&#123;/&#125;\"&gt;相对路径&lt;/a&gt;&lt;a th:href=\"@&#123;css/bootstrap.min.css&#125;\"&gt;Content路径,默认访问static下的css文件夹&lt;/a&gt; 类似的标签有:th:href和th:src1234567891011121314&lt;!-- Will produce 'http://localhost:8080/gtvg/order/details?orderId=3' (plus rewriting) --&gt;&lt;a href=\"details.html\" th:href=\"@&#123;http://localhost:8080/gtvg/order/details(orderId=$&#123;o.id&#125;)&#125;\"&gt;view&lt;/a&gt;&lt;!-- Will produce '/gtvg/order/details?orderId=3' (plus rewriting) --&gt;&lt;a href=\"details.html\" th:href=\"@&#123;/order/details(orderId=$&#123;o.id&#125;)&#125;\"&gt;view&lt;/a&gt;&lt;!-- Will produce '/gtvg/order/3/details' (plus rewriting) --&gt;&lt;a href=\"details.html\" th:href=\"@&#123;/order/&#123;orderId&#125;/details(orderId=$&#123;o.id&#125;)&#125;\"&gt;view&lt;/a&gt;&lt;a th:href=\"@&#123;$&#123;url&#125;(orderId=$&#123;o.id&#125;)&#125;\"&gt;view&lt;/a&gt;&lt;a th:href=\"@&#123;'/details/'+$&#123;user.login&#125;(orderId=$&#123;o.id&#125;)&#125;\"&gt;view&lt;/a&gt;Server root relative URLsAn additional syntax can be used to create server-root-relative (instead of context-root-relative) URLs in order to link to different contexts in the same server. These URLs will be specified like @&#123;~/path/to/something&#125; 3、运算符在表达式中可以使用各类算术运算符，例如+, -, *, /, %1th:with=\"isEven=($&#123;prodStat.count&#125; % 2 == 0)\" 逻辑运算符&gt;, &lt;, &gt;=, &lt;=，==,!= (gt, lt, ge, le,eq,ne)都可以使用，唯一需要注意的是使用&lt;,&gt;时需要用它的HTML转义符：12th:if=\"$&#123;prodStat.count&#125; &amp;gt; 1\"th:text=\"'Execution mode is ' + ( ($&#123;execMode&#125; == 'dev')? 'Development' : 'Production')\" 布尔运算符: and or not/! 4.条件if/unlessThymeleaf中使用th:if和th:unless属性进行条件判断，标签只有在th:if中条件成立时才显示,th:unless于th:if恰好相反，只有表达式中的条件不成立，才会显示其内容。1&lt;a th:href=\"@&#123;/login&#125;\" th:unless=$&#123;session.user != null&#125;&gt;Login&lt;/a&gt; SwitchThymeleaf同样支持多路选择Switch结构,默认属性default可以用*表示:12345&lt;div th:switch=\"$&#123;user.role&#125;\"&gt; &lt;p th:case=\"'admin'\"&gt;User is an administrator&lt;/p&gt; &lt;p th:case=\"#&#123;roles.manager&#125;\"&gt;User is a manager&lt;/p&gt; &lt;p th:case=\"*\"&gt;User is some other thing&lt;/p&gt;&lt;/div&gt; 5.循环12345&lt;tr th:each=\"prod : $&#123;prods&#125;\"&gt; &lt;td th:text=\"$&#123;prod.name&#125;\"&gt;Onions&lt;/td&gt; &lt;td th:text=\"$&#123;prod.price&#125;\"&gt;2.41&lt;/td&gt; &lt;td th:text=\"$&#123;prod.inStock&#125;? #&#123;true&#125; : #&#123;false&#125;\"&gt;yes&lt;/td&gt; &lt;/tr&gt; 迭代对象必须为 Any object implementing java.util.Iterable、 java.util.Enumeration、java.util.Iterator Any object implementing java.util.Map. When iterating maps, iter variables will be of class java.util.Map.Entry. Any array. Any other object will be treated as if it were a single-valued list containing the object itself.1234567891011&lt;tr th:each=\"prod,iterStat : $&#123;prods&#125;\" th:class=\"$&#123;iterStat.odd&#125;? 'odd'\"&gt; &lt;td th:text=\"$&#123;prod.name&#125;\"&gt;Onions&lt;/td&gt; &lt;td th:text=\"$&#123;prod.price&#125;\"&gt;2.41&lt;/td&gt; &lt;td th:text=\"$&#123;prod.inStock&#125;? #&#123;true&#125; : #&#123;false&#125;\"&gt;yes&lt;/td&gt; &lt;/tr&gt;//不过也可以直接加Stat后缀访问状态变量&lt;tr th:each=\"prod : $&#123;prods&#125;\" th:class=\"$&#123;prodStat.odd&#125;? 'odd'\"&gt; &lt;td th:text=\"$&#123;prod.name&#125;\"&gt;Onions&lt;/td&gt; &lt;td th:text=\"$&#123;prod.price&#125;\"&gt;2.41&lt;/td&gt; &lt;td th:text=\"$&#123;prod.inStock&#125;? #&#123;true&#125; : #&#123;false&#125;\"&gt;yes&lt;/td&gt; &lt;/tr&gt; th:each内置迭代状态属性： index ,当前索引，从0开始。 count,当前数目，从1开始。 size，总大小 current，当前值 even/odd boolean properties. first boolean property. last boolean property.6、设置html标签属性12345678&lt;img src=\"../../images/gtvglogo.png\" th:attr=\"src=@&#123;/images/gtvglogo.png&#125;,title=#&#123;logo&#125;,alt=#&#123;logo&#125;\" /&gt;//which is equivalent:&lt;img src=\"../../images/gtvglogo.png\" th:src=\"@&#123;/images/gtvglogo.png&#125;\" th:title=\"#&#123;logo&#125;\" th:alt=\"#&#123;logo&#125;\" /&gt;//append&lt;tr th:each=\"prod : $&#123;prods&#125;\" class=\"row\" th:classappend=\"$&#123;prodStat.odd&#125;? 'odd'\"&gt; Thymeleaf 3中的一些变化和特性 模板变化 推荐你去掉模板中的 th:inline=“text” 属性。因为在HTML或XML模板中，不再需要该属性去支持文本中内联表达式的特性。 完整的HTML5 标记支持 不在强制要求标签闭合，属性加引号等等 模板类型 Thymeleaf 3 移除了之前版本的模板类型，新的模板类型为：HTML、XML、TEXT、JAVASCRIPT、CSS、RAW 文本型模板文本型模板使得Thymeleaf可以支持输出CSS、Javascript和文本文件。在你想要在CSS或Javascript文件中使用服务端的变量时；或者想要输出纯文本的内容时。 在文本模式中使用Thymeleaf的特性，你需要使用一种新的语法，例如：1234[# th:each=&quot;item : $&#123;items&#125;&quot;] - [# th:utext=&quot;$&#123;item&#125;&quot; /][/]var a = [# th:text=&quot;$&#123;msg&#125;&quot;/]; 增强的内联机制现在可无需额外的标签，直接在文本中输出数据：12This product is called [[$&#123;product.name&#125;]] and it&apos;s great!var a = [[$&#123;msg&#125;]]; 4、片段（Fragment）表达式Thymeleaf 3.0 引入了一个新的片段表达式。形如：~{commons::footer}。该特性十分有用（比如解决定义通用的header和footer的问题）base.html123456789101112&lt;head th:fragment=\"common_header(title,links)\"&gt; &lt;title th:replace=\"$&#123;title&#125;\"&gt;The awesome application&lt;/title&gt; &lt;!-- Common styles and scripts --&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" media=\"all\" th:href=\"@&#123;/css/awesomeapp.css&#125;\"&gt; &lt;link rel=\"shortcut icon\" th:href=\"@&#123;/images/favicon.ico&#125;\"&gt; &lt;script type=\"text/javascript\" th:src=\"@&#123;/sh/scripts/codebase.js&#125;\"&gt;&lt;/script&gt; &lt;!--/* Per-page placeholder for additional links */--&gt; &lt;th:block th:replace=\"$&#123;links&#125;\" /&gt;&lt;/head&gt; main.html12345&lt;head th:replace=\"base :: common_header(~&#123;::title&#125;,~&#123;::link&#125;)\"&gt; &lt;title&gt;Awesome - Main&lt;/title&gt; &lt;link rel=\"stylesheet\" th:href=\"@&#123;/css/bootstrap.min.css&#125;\"&gt; &lt;link rel=\"stylesheet\" th:href=\"@&#123;/themes/smoothness/jquery-ui.css&#125;\"&gt;&lt;/head&gt; 片段经常和th:insert or th:replace一起使用12345&lt;div th:insert=\"~&#123;commons :: main&#125;\"&gt;...&lt;/div&gt;&lt;div th:with=\"frag=~&#123;footer :: #main/text()&#125;\"&gt; &lt;p th:insert=\"$&#123;frag&#125;\"&gt;&lt;/div&gt; ~{::selector} or ~{this::selector}引用本模板内的片段 不使用th:fragment定义的片段的情况：1234&lt;div id=\"copy-section\"&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/div&gt; &lt;div th:insert=\"~&#123;footer :: #copy-section&#125;\"&gt;&lt;/div&gt; th:insert and th:replace (and th:include)的区别： th:insert 插入片段本身 th:replace actually replaces its host tag with the specified fragment. th:include 与th:insert不同的是，它插入的是片段解析后的内容5、无操作标记（token）Thymeleaf 3.0 另一个新的特性就是无操作（NO-OP no-operation）标记，下划线””，代表什么也不做。例如：`&lt;span th:text=”${user.name} ?: “&gt;no user authenticated`当user.name 为空的时候，直接输出标签体中的内容 注释普通html注释：Thymeleaf 注释：123456789101112131、&lt;!--/* This code will be removed at Thymeleaf parsing time! */--&gt;2、&lt;!--/*--&gt; &lt;div&gt; you can see me only before Thymeleaf processes me! &lt;/div&gt;&lt;!--*/--&gt;3、&lt;!--/*/ &lt;div th:text=&quot;$&#123;...&#125;&quot;&gt; ... &lt;/div&gt;/*/--&gt; html内联123456789101112131415161718192021222324252627//不会转义时&lt;p&gt;The message is \"[($&#123;msg&#125;)]\"&lt;/p&gt;//等价于&lt;p&gt;The message is \"This is &lt;b&gt;great!&lt;/b&gt;\"&lt;/p&gt;//转义时&lt;p&gt;The message is \"[[$&#123;msg&#125;]]\"&lt;/p&gt;//等价于&lt;p&gt;The message is \"This is &amp;lt;b&amp;gt;great!&amp;lt;/b&amp;gt;\"&lt;/p&gt;//禁用内联&lt;p th:inline=\"none\"&gt;A double array looks like this: [[1, 2, 3], [4, 5]]!&lt;/p&gt;//js内联&lt;script th:inline=\"javascript\"&gt; ... var username = [[$&#123;session.user.name&#125;]]; ...&lt;/script&gt;//css内联&lt;style th:inline=\"css\"&gt; .[[$&#123;classname&#125;]] &#123; text-align: [[$&#123;align&#125;]]; &#125;&lt;/style&gt; Markup Selector Syntaxhttp://www.thymeleaf.org/doc/tutorials/3.0/usingthymeleaf.html#appendix-c-markup-selector-syntax demo: https://github.com/jmiguelsamper/thymeleaf3-spring-helloworld https://github.com/jmiguelsamper/thymeleaf3-spring-xml-helloworld https://github.com/jmiguelsamper/thymeleaf3-servlet-helloworld https://github.com/tengj/SpringBootDemo参考： http://www.thymeleaf.org/doc/articles/thymeleaf3migration.html http://www.thymeleaf.org/doc/tutorials/3.0/usingthymeleaf.html http://blog.csdn.net/u012706811/article/details/52185345 https://www.tianmaying.com/tutorial/using-thymeleaf http://www.thymeleaf.org/doc/tutorials/3.0/usingthymeleaf.html#appendix-a-expression-basic-objects 原文链接：https://segmentfault.com/a/1190000009903821","categories":[{"name":"thymeleaf","slug":"thymeleaf","permalink":"http://github.com/b2stry/categories/thymeleaf/"}],"tags":[{"name":"thymeleaf","slug":"thymeleaf","permalink":"http://github.com/b2stry/tags/thymeleaf/"}]},{"title":"消息队列之Kafka","slug":"消息队列之kafka","date":"2018-01-15T09:58:50.000Z","updated":"2018-02-03T07:36:43.342Z","comments":true,"path":"2018/01/15/消息队列之kafka/","link":"","permalink":"http://github.com/b2stry/2018/01/15/消息队列之kafka/","excerpt":"Kafka 特点Kafka 最早是由 LinkedIn 公司开发一种分布式的基于发布/订阅的消息系统，之后成为 Apache 的顶级项目。主要特点如下：","text":"Kafka 特点Kafka 最早是由 LinkedIn 公司开发一种分布式的基于发布/订阅的消息系统，之后成为 Apache 的顶级项目。主要特点如下： 1) 同时为发布和订阅提供高吞吐量 Kafka 的设计目标是以时间复杂度为 O(1) 的方式提供消息持久化能力，即使对TB 级以上数据也能保证常数时间的访问性能。即使在非常廉价的商用机器上也能做到单机支持每秒 100K 条消息的传输。 2) 消息持久化 将消息持久化到磁盘，因此可用于批量消费，例如 ETL 以及实时应用程序。通过将数据持久化到硬盘以及 replication 防止数据丢失。 3) 分布式 支持 Server 间的消息分区及分布式消费，同时保证每个 partition 内的消息顺序传输。这样易于向外扩展，所有的producer、broker 和 consumer 都会有多个，均为分布式的。无需停机即可扩展机器。 4) 消费消息采用 pull 模式 消息被处理的状态是在 consumer 端维护，而不是由 server 端维护，broker 无状态，consumer 自己保存 offset。 支持 online 和 offline 的场景。 同时支持离线数据处理和实时数据处理。 Kafka 中的基本概念 1) BrokerKafka 集群中的一台或多台服务器统称为 Broker 2) Topic每条发布到 Kafka 的消息都有一个类别，这个类别被称为 Topic 。（物理上不同Topic 的消息分开存储。逻辑上一个 Topic 的消息虽然保存于一个或多个broker上，但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处） 3) PartitionTopic 物理上的分组，一个 Topic 可以分为多个 Partition ，每个 Partition 是一个有序的队列。Partition 中的每条消息都会被分配一个有序的 id（offset） 4) Producer消息和数据的生产者，可以理解为往 Kafka 发消息的客户端 5) Consumer消息和数据的消费者，可以理解为从 Kafka 取消息的客户端 6) Consumer Group每个 Consumer 属于一个特定的 Consumer Group（可为每个 Consumer 指定Group Name，若不指定 Group Name 则属于默认的 Group）。 这是 Kafka 用来实现一个 Topic 消息的广播（发给所有的 Consumer ）和单播（发给任意一个 Consumer ）的手段。一个 Topic 可以有多个 Consumer Group。Topic 的消息会复制（不是真的复制，是概念上的）到所有的 Consumer Group，但每个 Consumer Group 只会把消息发给该 Consumer Group 中的一个 Consumer。如果要实现广播，只要每个 Consumer 有一个独立的 Consumer Group 就可以了。如果要实现单播只要所有的 Consumer 在同一个 Consumer Group 。用 Consumer Group 还可以将 Consumer 进行自由的分组而不需要多次发送消息到不同的 Topic 。 Kafka 安装Mac 用户用 HomeBrew 来安装，安装前要先更新 brew1brew update 接着安装 kafka1brew install kafka 安装完成之后可以查看 kafka 的配置文件1cd /usr/local/etc/kafka 我的电脑通过 HomeBrew 安装的 kafka 位置在 /usr/local/Cellar/kafka/0.11.0.1/bin ，可以看到 HomeBrew 安装下来的 kafka 的版本已经是 0.11.0.1 的了。 kafka 需要用到 zookeeper，HomeBrew 安装kafka 的时候会同时安装 zookeeper。下面先启动 zookeeper：1zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties 接着启动 kafka12cd /usr/local/Cellar/kafka/0.11.0.1./bin/kafka-server-start /usr/local/etc/kafka/server.properties 创建 topic，设置 partition 数量为2，topic 的名字叫 test-topic，下面的例子都用这个 topic12cd /usr/local/Cellar/kafka/0.11.0.1./bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 2 --topic test-topic 查看创建的 topic12cd /usr/local/Cellar/kafka/0.11.0.1./bin/kafka-topics --list --zookeeper localhost:2181 Kafka 命令行测试发送消息12cd /usr/local/Cellar/kafka/0.11.0.1/binkafka-console-producer --broker-list localhost:9092 --topic test-topic 消费消息12cd /usr/local/Cellar/kafka/0.11.0.1/binkafka-console-consumer --bootstrap-server localhost:9092 --topic test-topic --from-beginning 删除 topic12cd /usr/local/Cellar/kafka/0.11.0.1/bin./bin/kafka-topics --delete --zookeeper localhost:2181 --topic test-topic 如果 kafka 启动时加载的配置文件中 server.properties 没有配置delete.topic.enable=true，那么此时的删除并不是真正的删除，而是把 topic 标记为：marked for deletion 可以通过命令来查看所有 topic12cd /usr/local/Cellar/kafka/0.11.0.1/bin./bin/kafka-topics --zookeeper localhost:2181 --list 如果想真正删除它，可以如下操作123登录zookeeper客户端：/usr/local/Cellar/zookeeper/3.4.10/bin/zkCli找到topic所在的目录：ls /brokers/topics找到要删除的topic，执行命令：rmr /brokers/topics/test-topic 即可，此时topic被彻底删除 Java 客户端访问1) maven工程的pom文件中添加依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.1&lt;/version&gt;&lt;/dependency&gt; 2) 消息生产者123456789101112131415161718192021222324252627282930package org.study.kafka;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.HashMap;import java.util.Map;public class ProducerSample &#123; public static void main(String[] args) &#123; Map&lt;String, Object&gt; props = new HashMap&lt;String, Object&gt;(); props.put(&quot;zk.connect&quot;, &quot;127.0.0.1:2181&quot;);//zookeeper 的地址 props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//用于建立与 kafka 集群连接的 host/port 组。 props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); String topic = &quot;test-topic&quot;; Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); producer.send(new ProducerRecord&lt;String, String&gt;(topic, &quot;idea-key2&quot;, &quot;java-message 1&quot;)); producer.send(new ProducerRecord&lt;String, String&gt;(topic, &quot;idea-key2&quot;, &quot;java-message 2&quot;)); producer.send(new ProducerRecord&lt;String, String&gt;(topic, &quot;idea-key2&quot;, &quot;java-message 3&quot;)); producer.close(); &#125;&#125; 3) 消息消费者1234567891011121314151617181920212223242526272829303132package org.study.kafka;import org.apache.kafka.clients.consumer.Consumer;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.Arrays;import java.util.Properties;public class ConsumerSample &#123; public static void main(String[] args) &#123; String topic = &quot;test-topic&quot;;// topic name Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);//用于建立与 kafka 集群连接的 host/port 组。 props.put(&quot;group.id&quot;, &quot;testGroup1&quot;);// Consumer Group Name props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);// Consumer 的 offset 是否自动提交 props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);// 自动提交 offset 到 zookeeper 的时间间隔，时间是毫秒 props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); Consumer&lt;String, String&gt; consumer = new KafkaConsumer(props); consumer.subscribe(Arrays.asList(topic)); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) System.out.printf(&quot;partition = %d, offset = %d, key = %s, value = %s%n&quot;, record.partition(), record.offset(), record.key(), record.value()); &#125; &#125;&#125; 4) 启动 zookeeper1zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties 5) 启动 kafka 服务器1kafka-server-start /usr/local/etc/kafka/server.properties 6) 运行 Consumer先运行 Consumer ，这样当生产者发送消息的时候能在消费者后端看到消息记录。 7) 运行 Producer运行 Producer，发布几条消息，在 Consumer 的控制台能看到接收的消息 Kafka 集群配置kafka 的集群配置一般有三种，即： single node - single broker ，single node - multiple broker ，multiple node - multiple broker 前两种实际上官网有介绍。 single node - single broker 1) 启动 zookeeper1zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties 2) 启动 kafka broker1kafka-server-start /usr/local/etc/kafka/server.properties 3) 创建一个 kafka topic1kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 2 --topic topic-singlenode-singlebroker 4) 启动 producer 发送信息1kafka-console-producer --broker-list localhost:9092 --topic topic-singlenode-singlebroker broker-list 和 topic 这两个参数是必须的，broker-list 指定要连接的 broker 的地址，格式为 node_address:port 。topic 是必须的，因为需要发送消息给订阅了该topic 的 consumer group 。 现在可以在命令行里输入一些信息，每一行会被作为一个消息。 5) 启动 consumer 消费消息1kafka-console-consumer --bootstrap-server localhost:9092 --topic topic-singlenode-singlebroker 在不同的终端窗口里分别启动 zookeeper、broker、producer、consumer 后，在producer 终端里输入消息，消息就会在 consumer 终端中显示了。 single node - multiple broker 1) 启动 zookeeper1zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties 2) 启动broker 如果需要在单个节点（即一台机子）上面启动多个 broker（这里作为例子启动三个 broker），需要准备多个server.properties文件即可，所以需要复制 /usr/local/etc/kafka/server.properties 文件。因为需要为每个 broker 指定单独的属性配置文件，其中 broker.id 、 port 、 log.dir 这三个属性必须是不同的。 新建一个 kafka-example 目录和三个存放日志的目录1234mkdir kafka-examplemkdir kafka-logs-1mkdir kafka-logs-2mkdir kafka-logs-3 复制 /usr/local/etc/kafka/server.properties 文件三份123cp server.properties /Users/niwei/Downloads/kafka-example/server-1.propertiescp server.properties /Users/niwei/Downloads/kafka-example/server-2.propertiescp server.properties /Users/niwei/Downloads/kafka-example/server-3.properties 在 broker1 的配置文件 server-1.properties 中，相关要修改的参数为：123broker.id=1port=9093log.dirs=/Users/niwei/Downloads/kafka-example/kafka-logs-1 broker2 的配置文件 server-2.properties 中，相关要修改的参数为：123broker.id=2port=9094log.dirs=/Users/niwei/Downloads/kafka-example/kafka-logs-2 broker3 的配置文件 server-3.properties 中，相关要修改的参数为：123broker.id=3port=9095log.dirs=/Users/niwei/Downloads/kafka-example/kafka-logs-3 启动每个 broker1234cd /Users/niwei/Downloads/kafka-examplekafka-server-start server-1.propertieskafka-server-start server-2.propertieskafka-server-start server-3.properties 3) 创建 topic 创建一个名为 topic-singlenode-multiplebroker 的topic1kafka-topics --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic topic-singlenode-multiplebroker 4) 启动 producer 发送信息如果一个 producer 需要连接多个 broker 则需要传递参数 broker-list1kafka-console-producer --broker-list localhost:9093, localhost:9094, localhost:9095 --topic topic-singlenode-multiplebroker 5) 启动 consumer 消费消息1kafka-console-consumer --zookeeper localhost:2181 --topic topic-singlenode-multiplebroker multiple node - multiple broker 在多节点多 broker 集群中，每个节点都需要安装 Kafka，且所有的 broker 都连接到同一个 zookeeper 。这里 zookeeper 当然也是可以配置成集群方式的，具体步骤参见我之前写的搭建 zookeeper 集群。 1) Kafka 的集群配置12345678910111213141516171819broker.id=1 #当前机器在集群中的唯一标识port=9093 #当前 kafka 对外提供服务的端口，默认是 9092host.name=192.168.121.101 #这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。log.dirs=/Users/niwei/Downloads/kafka-example/kafka-logs-1 #消息存放的目录，这个目录可以配置为逗号分割的表达式zookeeper.connect=192.168.120.101:2181,192.168.120.102:2181,192.168.120.103:2181 #设置 zookeeper 集群的连接端口num.network.threads=3 #这个是 borker 进行网络处理的线程数num.io.threads=5 #这个是 borker 进行 IO 处理的线程数socket.send.buffer.bytes=102400 #发送缓冲区的大小，数据先回存储到缓冲区了到达一定的大小后在发送能提高性能socket.receive.buffer.bytes=102400 #接收缓冲区的大小，当数据到达一定大小后在序列化到磁盘socket.request.max.bytes=104857600 #这个参数是向 kafka 请求消息或者向 kafka 发送消息的请求的最大数，这个值不能超过 jvm 的堆栈大小num.partitions=1 #默认的分区数，一个 topic 默认1个分区数log.retention.hours=24 #默认消息的最大持久化时间，24小时message.max.byte=5242880 #消息保存的最大值5Mdefault.replication.factor=2 #kafka 保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务replica.fetch.max.bytes=5242880 #取消息的最大直接数log.segment.bytes=1073741824 #这个参数是因为 kafka 的消息是以追加的形式落地到文件，当超过这个值的时候，kafka 会新建一个文件log.retention.check.interval.ms=300000 #每隔 300000 毫秒去检查上面配置的 log 失效时间（log.retention.hours=24 ），到目录查看是否有过期的消息如果有则删除log.cleaner.enable=false #是否启用 log 压缩，一般不用启用，启用的话可以提高性能 由于是多节点多 broker 的，所以每个 broker 的配置文件 server.properties 都要按以上说明修改 2) producer 的配置修改1kafka-console-producer --broker-list 192.168.21.1:9092,192.168.21.2:9092,192.168.21.3:9092 --topic topic-multiplenode-multiplebroker 3) consumer 的配置修改1kafka-console-consumer --zookeeper 192.168.120.101:2181,192.168.120.102:2181,192.168.120.103:2181 --topic topic-multiplenode-multiplebroker Kafka 高可靠性配置Kafka 提供了很高的数据冗余弹性，对于需要数据高可靠性的场景可以增加数据冗余备份数（replication.factor），调高最小写入副本数的个数（min.insync.replicas）等等，但是这样会影响性能。反之，性能提高而可靠性则降低，用户需要自身业务特性在彼此之间做一些权衡性选择。 要保证数据写入到 Kafka 是安全的、高可靠的，需要如下的配置： 1) topic 的配置replication.factor&gt;=3，即副本数至少是3个2&lt;=min.insync.replicas&lt;=replication.factor2) broker 的配置leader 的选举条件 unclean.leader.election.enable=false3) producer 的配置request.required.acks=-1，producer.type=sync Kafka 高吞吐量的秘诀消息中间件从功能上看就是写入数据、读取数据两大类，优化也可以从这两方面来看。 为了优化写入速度 Kafak 采用以下技术： 1) 顺序写入 磁盘大多数都还是机械结构（SSD不在讨论的范围内），如果将消息以随机写的方式存入磁盘，就需要按柱面、磁头、扇区的方式寻址，缓慢的机械运动（相对内存）会消耗大量时间，导致磁盘的写入速度与内存写入速度差好几个数量级。为了规避随机写带来的时间消耗，Kafka 采取了顺序写的方式存储数据，如下图所示： 每条消息都被append 到该 partition 中，属于顺序写磁盘，因此效率非常高。 但这种方法有一个缺陷：没有办法删除数据。所以Kafka是不会删除数据的，它会把所有的数据都保留下来，每个消费者（Consumer）对每个 Topic 都有一个 offset 用来表示读取到了第几条数据。 上图中有两个消费者，Consumer1 有两个 offset 分别对应 Partition0、Partition1（假设每一个 Topic 一个 Partition ）。Consumer2 有一个 offset 对应Partition2 。这个 offset 是由客户端 SDK 保存的，Kafka 的 Broker 完全无视这个东西的存在，一般情况下 SDK 会把它保存到 zookeeper 里面。 如果不删除消息，硬盘肯定会被撑满，所以 Kakfa 提供了两种策略来删除数据。一是基于时间，二是基于 partition 文件大小，具体配置可以参看它的配置文档。 即使是顺序写，过于频繁的大量小 I/O 操作一样会造成磁盘的瓶颈，所以 Kakfa 在此处的处理是把这些消息集合在一起批量发送，这样减少对磁盘 I/O 的过度操作，而不是一次发送单个消息。 2) 内存映射文件即便是顺序写入硬盘，硬盘的访问速度还是不可能追上内存。所以 Kafka 的数据并不是实时的写入硬盘，它充分利用了现代操作系统分页存储来利用内存提高I/O效率。Memory Mapped Files （后面简称mmap）也被翻译成内存映射文件，在64位操作系统中一般可以表示 20G 的数据文件，它的工作原理是直接利用操作系统的 Page 来实现文件到物理内存的直接映射。完成映射之后对物理内存的操作会被同步到硬盘上（由操作系统在适当的时候）。 通过 mmap 进程像读写硬盘一样读写内存，也不必关心内存的大小，有虚拟内存为我们兜底。使用这种方式可以获取很大的 I/O 提升，因为它省去了用户空间到内核空间复制的开销（调用文件的 read 函数会把数据先放到内核空间的内存中，然后再复制到用户空间的内存中） 但这样也有一个很明显的缺陷——不可靠，写到 mmap 中的数据并没有被真正的写到硬盘，操作系统会在程序主动调用 flush 的时候才把数据真正的写到硬盘。所以 Kafka 提供了一个参数—— producer.type 来控制是不是主动 flush，如果Kafka 写入到 mmap 之后就立即 flush 然后再返回 Producer 叫同步(sync)；如果写入 mmap 之后立即返回，Producer 不调用 flush ，就叫异步(async)。 3) 标准化二进制消息格式为了避免无效率的字节复制，尤其是在负载比较高的情况下影响是显著的。为了避免这种情况，Kafka 采用由 Producer，Broker 和 Consumer 共享的标准化二进制消息格式，这样数据块就可以在它们之间自由传输，无需转换，降低了字节复制的成本开销。 而在读取速度的优化上 Kafak 采取的主要是零拷贝 1) 零拷贝（Zero Copy）的技术：传统模式下我们从硬盘读取一个文件是这样的 （1）操作系统将数据从磁盘读到内核空间的页缓存区 （2）应用将数据从内核空间读到用户空间的缓存中 （3）应用将数据写会内核空间的套接字缓存中 （4）操作系统将数据从套接字缓存写到网卡缓存中，以便将数据经网络发出 这样做明显是低效的，这里有四次拷贝，两次系统调用。 针对这种情况 Unix 操作系统提供了一个优化的路径，用于将数据从页缓存区传输到 socket。在 Linux 中，是通过 sendfile 系统调用来完成的。Java提供了访问这个系统调用的方法：FileChannel.transferTo API。这种方式只需要一次拷贝：操作系统将数据直接从页缓存发送到网络上，在这个优化的路径中，只有最后一步将数据拷贝到网卡缓存中是需要的。 这个技术其实非常普遍，The C10K problem 里面也有很详细的介绍，Nginx 也是用的这种技术，稍微搜一下就能找到很多资料。 Kafka 速度的秘诀在于它把所有的消息都变成一个的文件。通过 mmap 提高 I/O 的速度，写入数据的时候是末尾添加所以速度最优；读取数据的时候配合sendfile 直接暴力输出。所以单纯的去测试 MQ 的速度没有任何意义，Kafka 的这种暴力的做法已经脱了 MQ 的底裤，更像是一个暴力的数据传送器。 作者：预流链接：https://www.jianshu.com/p/2c4caed49343來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"kafka","slug":"kafka","permalink":"http://github.com/b2stry/categories/kafka/"}],"tags":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://github.com/b2stry/tags/消息中间件/"}]},{"title":"SpringMVC 测试 mockMVC","slug":"SpringMVC-测试-mockMVC","date":"2018-01-15T05:53:44.000Z","updated":"2018-01-15T06:05:34.899Z","comments":true,"path":"2018/01/15/SpringMVC-测试-mockMVC/","link":"","permalink":"http://github.com/b2stry/2018/01/15/SpringMVC-测试-mockMVC/","excerpt":"SpringMVC测试框架基于RESTful风格的SpringMVC的测试，我们可以测试完整的Spring MVC流程，即从URL请求到控制器处理，再到视图渲染都可以测试。","text":"SpringMVC测试框架基于RESTful风格的SpringMVC的测试，我们可以测试完整的Spring MVC流程，即从URL请求到控制器处理，再到视图渲染都可以测试。 一 MockMvcBuilderMockMvcBuilder是用来构造MockMvc的构造器，其主要有两个实现：StandaloneMockMvcBuilder和DefaultMockMvcBuilder，分别对应两种测试方式，即独立安装和集成Web环境测试（此种方式并不会集成真正的web环境，而是通过相应的Mock API进行模拟测试，无须启动服务器）。对于我们来说直接使用静态工厂MockMvcBuilders创建即可。 1.集成Web环境方式MockMvcBuilders.webAppContextSetup(WebApplicationContext context)：指定WebApplicationContext，将会从该上下文获取相应的控制器并得到相应的MockMvc；12345678910111213@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:config/IncotermsRestServiceTest-context.xml\")@WebAppConfigurationpublic class IncotermsRestServiceTest &#123; @Autowired private WebApplicationContext wac; private MockMvc mockMvc; @Before public void setup() &#123; this.mockMvc = MockMvcBuilders.webAppContextSetup(this.wac).build(); //构造MockMvc &#125; ...&#125; 注意： 1) @WebAppConfiguration：测试环境使用，用来表示测试环境使用的ApplicationContext将是WebApplicationContext类型的；value指定web应用的根； 2) 通过@Autowired WebApplicationContext wac：注入web环境的ApplicationContext容器； 3) 然后通过MockMvcBuilders.webAppContextSetup(wac).build()创建一个MockMvc进行测试； 2.独立测试方式MockMvcBuilders.standaloneSetup(Object… controllers)：通过参数指定一组控制器，这样就不需要从上下文获取了；1234567891011121314151617public class PricingExportResultsRestServiceTest &#123; @InjectMocks private PricingExportResultsRestService pricingExportResultsRestService; @Mock private ExportRateScheduleService exportRateScheduleService; @Mock private PricingUrlProvider pricingUrlProvider; private MockMvc mockMvc; @Before public void setup() &#123; MockitoAnnotations.initMocks(this); mockMvc = MockMvcBuilders.standaloneSetup(pricingExportResultsRestService).build(); //构造MockMvc &#125; ...&#125; 主要是两个步骤：1) 首先自己创建相应的控制器，注入相应的依赖2) 通过MockMvcBuilders.standaloneSetup模拟一个Mvc测试环境，通过build得到一个MockMvc 二 MockMvc先看一个测试例子1：123456789101112131415161718 @Testpublic void createIncotermSuccess() throws Exception &#123; IncotermTo createdIncoterm = new IncotermTo(); createdIncoterm.setId(new IncotermId(UUID.fromString(\"6305ff33-295e-11e5-ae37-54ee7534021a\"))); createdIncoterm.setCode(\"EXW\"); createdIncoterm.setDescription(\"code exw\"); createdIncoterm.setLocationQualifier(LocationQualifier.DEPARTURE); when(inventoryService.create(any(IncotermTo.class))).thenReturn(createdIncoterm); mockMvc.perform(post(\"/secured/resources/incoterms/create\").accept(MediaType.APPLICATION_JSON).contentType(MediaType.APPLICATION_JSON) .content(\"&#123;\\\"code\\\" : \\\"EXW\\\", \\\"description\\\" : \\\"code exw\\\", \\\"locationQualifier\\\" : \\\"DEPARTURE\\\"&#125;\".getBytes())) //.andDo(print()) .andExpect(status().isOk()) .andExpect(jsonPath(\"id.value\").exists()) .andExpect(jsonPath(\"id.value\").value(\"6305ff33-295e-11e5-ae37-54ee7534021a\")) .andExpect(jsonPath(\"code\").value(\"EXW\"));&#125; perform：执行一个RequestBuilder请求，会自动执行SpringMVC的流程并映射到相应的控制器执行处理； andExpect：添加ResultMatcher验证规则，验证控制器执行完成后结果是否正确； andDo：添加ResultHandler结果处理器，比如调试时打印结果到控制台； andReturn：最后返回相应的MvcResult；然后进行自定义验证/进行下一步的异步处理； 看一个具体的例子2：12345678910@Testpublic void testView() throws Exception &#123; MvcResult result = mockMvc.perform(MockMvcRequestBuilders.get(\"/user/1\")) .andExpect(MockMvcResultMatchers.view().name(\"user/view\")) .andExpect(MockMvcResultMatchers.model().attributeExists(\"user\")) .andDo(MockMvcResultHandlers.print()) .andReturn(); Assert.assertNotNull(result.getModelAndView().getModel().get(\"user\"));&#125; 整个过程：1) mockMvc.perform执行一个请求；2) MockMvcRequestBuilders.get(“/user/1”)构造一个请求3) ResultActions.andExpect添加执行完成后的断言4) ResultActions.andDo添加一个结果处理器，表示要对结果做点什么事情，比如此处使用MockMvcResultHandlers.print()输出整个响应结果信息。5) ResultActions.andReturn表示执行完成后返回相应的结果。 整个测试过程非常有规律：1) 准备测试环境2) 通过MockMvc执行请求3.1) 添加验证断言3.2) 添加结果处理器3.3) 得到MvcResult进行自定义断言/进行下一步的异步请求4) 卸载测试环境 三 RequestBuilder/MockMvcRequestBuilders从名字可以看出，RequestBuilder用来构建请求的，其提供了一个方法buildRequest(ServletContext servletContext)用于构建MockHttpServletRequest；其主要有两个子类MockHttpServletRequestBuilder和MockMultipartHttpServletRequestBuilder（如文件上传使用），即用来Mock客户端请求需要的所有数据。 1.MockMvcRequestBuilders主要APIMockHttpServletRequestBuilder get(String urlTemplate, Object… urlVariables)：根据uri模板和uri变量值得到一个GET请求方式的MockHttpServletRequestBuilder；如get(/user/{id}, 1L)； MockHttpServletRequestBuilder post(String urlTemplate, Object… urlVariables)：同get类似，但是是POST方法； MockHttpServletRequestBuilder put(String urlTemplate, Object… urlVariables)：同get类似，但是是PUT方法； MockHttpServletRequestBuilder delete(String urlTemplate, Object… urlVariables) ：同get类似，但是是DELETE方法； MockHttpServletRequestBuilder options(String urlTemplate, Object… urlVariables)：同get类似，但是是OPTIONS方法； MockHttpServletRequestBuilder request(HttpMethod httpMethod, String urlTemplate, Object… urlVariables)： 提供自己的Http请求方法及uri模板和uri变量，如上API都是委托给这个API； MockMultipartHttpServletRequestBuilder fileUpload(String urlTemplate, Object… urlVariables)：提供文件上传方式的请求，得到MockMultipartHttpServletRequestBuilder； RequestBuilder asyncDispatch(final MvcResult mvcResult)：创建一个从启动异步处理的请求的MvcResult进行异步分派的RequestBuilder； 2.MockHttpServletRequestBuilder和MockMultipartHttpServletRequestBuilder API(1)MockHttpServletRequestBuilder APIMockHttpServletRequestBuilder header(String name, Object… values)/MockHttpServletRequestBuilder headers(HttpHeaders httpHeaders)：添加头信息； MockHttpServletRequestBuilder contentType(MediaType mediaType)：指定请求的contentType头信息； MockHttpServletRequestBuilder accept(MediaType… mediaTypes)/MockHttpServletRequestBuilder accept(String… mediaTypes)：指定请求的Accept头信息； MockHttpServletRequestBuilder content(byte[] content)/MockHttpServletRequestBuilder content(String content)：指定请求Body体内容； MockHttpServletRequestBuilder cookie(Cookie… cookies)：指定请求的Cookie； MockHttpServletRequestBuilder locale(Locale locale)：指定请求的Locale； MockHttpServletRequestBuilder characterEncoding(String encoding)：指定请求字符编码； MockHttpServletRequestBuilder requestAttr(String name, Object value) ：设置请求属性数据； MockHttpServletRequestBuilder sessionAttr(String name, Object value)/MockHttpServletRequestBuilder sessionAttrs(Map sessionAttributes)：设置请求session属性数据； MockHttpServletRequestBuilder flashAttr(String name, Object value)/MockHttpServletRequestBuilder flashAttrs(Map flashAttributes)：指定请求的flash信息，比如重定向后的属性信息； MockHttpServletRequestBuilder session(MockHttpSession session) ：指定请求的Session； MockHttpServletRequestBuilder principal(Principal principal) ：指定请求的Principal； MockHttpServletRequestBuilder contextPath(String contextPath) ：指定请求的上下文路径，必须以“/”开头，且不能以“/”结尾； MockHttpServletRequestBuilder pathInfo(String pathInfo) ：请求的路径信息，必须以“/”开头； MockHttpServletRequestBuilder secure(boolean secure)：请求是否使用安全通道； MockHttpServletRequestBuilder with(RequestPostProcessor postProcessor)：请求的后处理器，用于自定义一些请求处理的扩展点； (2)MockMultipartHttpServletRequestBuilder继承自MockHttpServletRequestBuilder，又提供了如下APIMockMultipartHttpServletRequestBuilder file(String name, byte[] content)/MockMultipartHttpServletRequestBuilder file(MockMultipartFile file)：指定要上传的文件； 四 ResultActions调用MockMvc.perform(RequestBuilder requestBuilder)后将得到ResultActions，通过ResultActions完成如下三件事： ResultActions andExpect(ResultMatcher matcher) ：添加验证断言来判断执行请求后的结果是否是预期的； ResultActions andDo(ResultHandler handler) ：添加结果处理器，用于对验证成功后执行的动作，如输出下请求/结果信息用于调试； MvcResult andReturn() ：返回验证成功后的MvcResult；用于自定义验证/下一步的异步处理； 五 ResultMatcher/MockMvcResultMatchers1) ResultMatcher用来匹配执行完请求后的结果验证，其就一个match(MvcResult result)断言方法，如果匹配失败将抛出相应的异常；spring mvc测试框架提供了很多ResultMatchers来满足测试需求。注意这些ResultMatchers并不是ResultMatcher的子类，而是返回ResultMatcher实例的。Spring mvc测试框架为了测试方便提供了MockMvcResultMatchers静态工厂方法方便操作； 2) 具体的API如下：HandlerResultMatchers handler()：请求的Handler验证器，比如验证处理器类型/方法名；此处的Handler其实就是处理请求的控制器； RequestResultMatchers request()：得到RequestResultMatchers验证器； ModelResultMatchers model()：得到模型验证器； ViewResultMatchers view()：得到视图验证器； FlashAttributeResultMatchers flash()：得到Flash属性验证； StatusResultMatchers status()：得到响应状态验证器； HeaderResultMatchers header()：得到响应Header验证器； CookieResultMatchers cookie()：得到响应Cookie验证器； ContentResultMatchers content()：得到响应内容验证器； JsonPathResultMatchers jsonPath(String expression, Object … args)/ResultMatcher jsonPath(String expression, Matcher matcher)：得到Json表达式验证器； XpathResultMatchers xpath(String expression, Object… args)/XpathResultMatchers xpath(String expression, Map namespaces, Object… args)：得到Xpath表达式验证器； ResultMatcher forwardedUrl(final String expectedUrl)：验证处理完请求后转发的url（绝对匹配）； ResultMatcher forwardedUrlPattern(final String urlPattern)：验证处理完请求后转发的url（Ant风格模式匹配，@since spring4）； ResultMatcher redirectedUrl(final String expectedUrl)：验证处理完请求后重定向的url（绝对匹配）； ResultMatcher redirectedUrlPattern(final String expectedUrl)：验证处理完请求后重定向的url（Ant风格模式匹配，@since spring4）； 六 一些常用的测试1.测试普通控制器123456mockMvc.perform(get(\"/user/&#123;id&#125;\", 1)) //执行请求 .andExpect(model().attributeExists(\"user\")) //验证存储模型数据 .andExpect(view().name(\"user/view\")) //验证viewName .andExpect(forwardedUrl(\"/WEB-INF/jsp/user/view.jsp\"))//验证视图渲染时forward到的jsp .andExpect(status().isOk())//验证状态码 .andDo(print()); //输出MvcResult到控制台 2.得到MvcResult自定义验证123MvcResult result = mockMvc.perform(get(\"/user/&#123;id&#125;\", 1))//执行请求 .andReturn(); //返回MvcResultAssert.assertNotNull(result.getModelAndView().getModel().get(\"user\")); //自定义断言 3.验证请求参数绑定到模型数据及Flash属性123456mockMvc.perform(post(\"/user\").param(\"name\", \"zhang\")) //执行传递参数的POST请求(也可以post(\"/user?name=zhang\")) .andExpect(handler().handlerType(UserController.class)) //验证执行的控制器类型 .andExpect(handler().methodName(\"create\")) //验证执行的控制器方法名 .andExpect(model().hasNoErrors()) //验证页面没有错误 .andExpect(flash().attributeExists(\"success\")) //验证存在flash属性 .andExpect(view().name(\"redirect:/user\")); //验证视图 4.文件上传1234byte[] bytes = new byte[] &#123;1, 2&#125;;mockMvc.perform(fileUpload(\"/user/&#123;id&#125;/icon\", 1L).file(\"icon\", bytes)) //执行文件上传 .andExpect(model().attribute(\"icon\", bytes)) //验证属性相等性 .andExpect(view().name(\"success\")); //验证视图 5.JSON请求/响应验证123456789101112131415String requestBody = \"&#123;\\\"id\\\":1, \\\"name\\\":\\\"zhang\\\"&#125;\"; mockMvc.perform(post(\"/user\") .contentType(MediaType.APPLICATION_JSON).content(requestBody) .accept(MediaType.APPLICATION_JSON)) //执行请求 .andExpect(content().contentType(MediaType.APPLICATION_JSON)) //验证响应contentType .andExpect(jsonPath(\"$.id\").value(1)); //使用Json path验证JSON 请参考http://goessner.net/articles/JsonPath/ String errorBody = \"&#123;id:1, name:zhang&#125;\"; MvcResult result = mockMvc.perform(post(\"/user\") .contentType(MediaType.APPLICATION_JSON).content(errorBody) .accept(MediaType.APPLICATION_JSON)) //执行请求 .andExpect(status().isBadRequest()) //400错误请求 .andReturn(); Assert.assertTrue(HttpMessageNotReadableException.class.isAssignableFrom(result.getResolvedException().getClass()));//错误的请求内容体 6.异步测试12345678910 //CallableMvcResult result = mockMvc.perform(get(\"/user/async1?id=1&amp;name=zhang\")) //执行请求 .andExpect(request().asyncStarted()) .andExpect(request().asyncResult(CoreMatchers.instanceOf(User.class))) //默认会等10秒超时 .andReturn();mockMvc.perform(asyncDispatch(result)) .andExpect(status().isOk()) .andExpect(content().contentType(MediaType.APPLICATION_JSON)) .andExpect(jsonPath(\"$.id\").value(1)); 7.全局配置12345678mockMvc = webAppContextSetup(wac) .defaultRequest(get(\"/user/1\").requestAttr(\"default\", true)) //默认请求 如果其是Mergeable类型的，会自动合并的哦mockMvc.perform中的RequestBuilder .alwaysDo(print()) //默认每次执行请求后都做的动作 .alwaysExpect(request().attribute(\"default\", true)) //默认每次执行后进行验证的断言 .build(); mockMvc.perform(get(\"/user/1\")) .andExpect(model().attributeExists(\"user\")); 本文转载至 李月云博主的博客园，地址：http://www.cnblogs.com/lyy-2016/p/6122144.html","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://github.com/b2stry/categories/Spring-Boot/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://github.com/b2stry/tags/Spring-Boot/"}]},{"title":"慕课网---Spring Boot企业级博客系统实战(开篇)","slug":"慕课网-Spring-Boot技术栈博客企业前后端-开篇","date":"2018-01-14T15:06:48.000Z","updated":"2018-02-03T07:33:24.927Z","comments":true,"path":"2018/01/14/慕课网-Spring-Boot技术栈博客企业前后端-开篇/","link":"","permalink":"http://github.com/b2stry/2018/01/14/慕课网-Spring-Boot技术栈博客企业前后端-开篇/","excerpt":"课程概述 名称：基于Spring Boot的博客系统 功能：注册博主、发博客、评论、点赞、全文检索、文件上传…… 技术：前端、后端、数据库、NoSQL、文件存储、大数据…… 目的：掌握企业级应用开发的流程，提升市场核心竞争力！","text":"课程概述 名称：基于Spring Boot的博客系统 功能：注册博主、发博客、评论、点赞、全文检索、文件上传…… 技术：前端、后端、数据库、NoSQL、文件存储、大数据…… 目的：掌握企业级应用开发的流程，提升市场核心竞争力！ 核心功能博客系统： 用户管理：注册、登录、增加用户、修改用户、删除用户、搜索用户…… 安全设置：角色授权、权限设置…… 博客管理：发表博客、编辑博客、删除博客、博客分类、设置标签、上传图片、模糊查询、最新排序、最热排序、阅读量统计…… 评论管理：发表评论、删除评论、评论量统计…… 点赞管理：点赞、取消点赞、点赞量统计…… 分类管理：创建分类、编辑分类、删除分类、按分类查询…… 标签管理：创建标签、删除标签、按标签进行查询…… 首页搜索：全文检索、最新文章、最热文章、热门标签、热门用户、热门文章、最新发布…… ………… 核心技术 前端：Bootstarp、Thymeleaf、jQuery、HTML5、JavaScript、CSS 后端：Spring、Spring Boot、Spring MVC、Spring Data、Spring Security、Hibernate 数据存储：MySQL、H2、MongoDB 其他：ElasticSearch、Gradle…… 授课方式： 精讲技术 —-&gt; 各个攻破 功能累加 —-&gt; 步步为营 课程安排： 前面部分：实战入门阶段：Gradle、Gradle Wrapper、开发环境搭建及项目运行、Thymeleaf只是讲解及实战、数据持久化讲解及实战、全文搜索讲解及实战、Bootstarp讲解及实战 中间部分：实战进阶阶段：架构设计与分层、需求分析与设计、权限管理、整体框架搭建、API设计 后面部分：实战高级阶段：用户管理实现、角色管理实现、权限管理实现、博客管理实现、评价管理实现、点赞管理实现、分类管理实现、标签管理实现、首页搜索实现 学习收获 学会Spring Boot及周边技术栈 掌握如何运行上述技术进行整合，搭建框架的能力 熟悉完整企业级应用开发的流程 掌握了如何打造一款企业级应用产品 特色 技术涉及面广 技术点富有前瞻性，符合主流 知识点梳理+实战案例 真实可用的企业级应用—-博客系统 适合人群 Java开发者 对Spring Boot及企业级开发感兴趣的朋友 立志于成为架构师的朋友 注意事项 有过Spring开发经验 环境配置，尽量与讲师保持一致 Spring Boot是什么？ 为所有Spring开发提供一个更快更广泛的入门体验 开箱即用，不是合适时也可以快速抛弃 提供一系列大型项目常用的非功能特性 零配置(不需要XML配置，遵循”约定大于配置”) Spring Boot 简化开发抛弃了传统JavaEE项目繁琐的配置、学习过程，让企业级应用开发过程变得 So Easy！ 开启Spring Boot的第一个Web项目通过SpringInitializr初始化一个Web项目 打开 http://start.spring.io/ ，选择Gradle构建，依赖选择Web，然后下载下来。 下载Gradle https://gradle.org/releases/ ，解压后，将Gradle添加到环境变量里。 进入到项目目录中执行gradle build,构建完成后进入build/libs目录中，执行java -jar xxxxx.jar命令启动程序。 一个HelloWorld项目 通过本节学习 编写项目构建信息 编写程序代码 编写测试用例 配置Gradle Wrapper 新建一个helloworld项目修改Maven仓库将build.gradle文件中两处mavenCentral()修改为阿里云仓库(也可以使用其他仓库)：123maven&#123; url &apos;http://maven.aliyun.com/nexus/content/groups/public/&apos;&#125; 在src/main/java/xxx/xxx/xxx中新建controller包，然后新建HelloController.java,具体代码如下1234567891011121314import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * Hello World 控制器 */@RestControllerpublic class HelloController &#123; @RequestMapping(\"/hello\") public String hello() &#123; return \"Hello World!\"; &#125;&#125; 编写测试用例在src/test/java/xxx/xxx/xxx中新建controller包，然后新建HelloControllerTest.java,具体代码如下1234567891011121314151617181920212223242526272829303132import static org.hamcrest.Matchers.equalTo;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.content;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.web.servlet.AutoConfigureMockMvc;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.http.MediaType;import org.springframework.test.context.junit4.SpringRunner;import org.springframework.test.web.servlet.MockMvc;import org.springframework.test.web.servlet.request.MockMvcRequestBuilders;/** * Hello World 控制器测试类 */@RunWith(SpringRunner.class)@SpringBootTest@AutoConfigureMockMvcpublic class HelloControllerTest &#123; @Autowired private MockMvc mockMvc; @Test public void testHello() throws Exception &#123; mockMvc.perform(MockMvcRequestBuilders.get(\"/hello\").accept(MediaType.APPLICATION_JSON)) .andExpect(status().isOk()) .andExpect(content().string(equalTo(\"Hello World!\"))); &#125;&#125; 配置Gradle Wrappergradle/wrapper/gradle-wrapper.properties 可以通过修改distributionUrl修改gradle版本 使用Gradle Wrapper在项目根目录下执行gradlew build","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://github.com/b2stry/categories/Spring-Boot/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://github.com/b2stry/tags/Spring-Boot/"}]},{"title":"Java集合干货——ArrayList源码分析","slug":"Java集合干货——ArrayList源码分析","date":"2018-01-13T08:19:58.000Z","updated":"2018-02-03T07:32:26.190Z","comments":true,"path":"2018/01/13/Java集合干货——ArrayList源码分析/","link":"","permalink":"http://github.com/b2stry/2018/01/13/Java集合干货——ArrayList源码分析/","excerpt":"前言本篇主要通过一些对源码的分析，讲解几个ArrayList常见的方法，以及和Vector的区别。","text":"前言本篇主要通过一些对源码的分析，讲解几个ArrayList常见的方法，以及和Vector的区别。 ArrayList定义12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable ArrayList实际上是一个动态数组，容量可以动态的增长，其继承了AbstractList，实现了List, RandomAccess, Cloneable, java.io.Serializable这些接口。 RandomAccess接口，被List实现之后，为List提供了随机访问功能，也就是通过下标获取元素对象的功能。 实现了Cloneable, java.io.Serializable意味着可以被克隆和序列化。 初始化在使用中我们经常需要new出来各种泛型的ArrayList，那么在初始化过程是怎样的呢？ 如下一行代码，创建一个ArrayList对象1List&lt;Person&gt; list = new ArrayList&lt;&gt;(); 我们来看源码，是如何初始化的，找到构造方法12345//无参构造方法public ArrayList() &#123; super(); this.elementData = EMPTY_ELEMENTDATA;&#125; 看到这些代码的时候，我也是不解的，elementData和EMPTY_ELEMENTDATA是什么啊？但是很明显EMPTY_ELEMENTDATA是一个常量，追本溯源我们去看一下成员属性。12345678910//如果是无参构造方法创建对象的话，ArrayList的初始化长度为10，这是一个静态常量private static final int DEFAULT_CAPACITY = 10;//在这里可以看到我们不解的EMPTY_ELEMENTDATA实际上是一个空的对象数组 private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;//保存ArrayList数据的对象数组缓冲区 空的ArrayList的elementData = EMPTY_ELEMENTDATA 这就是为什么说ArrayList底层是数组实现的了。elementData的初始容量为10，大小会根据ArrayList容量的增长而动态的增长。 private transient Object[] elementData;//集合的长度 private int size; 执行完构造方法，如下图 可以说ArrayList的作者真的是很贴心，连缓存都处理好了，多次new出来的新对象，都执行同一个引用。 添加方法addadd(E e)123456789101112 /** * Appends the specified element to the end of this list. *///增加元素到集合的最后public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! //因为++运算符的特点 先使用后运算 这里实际上是 //elementData[size] = e //size+1 elementData[size++] = e; return true;&#125; 先不管ensureCapacityInternal的话，这个方法就是将一个元素增加到数组的size++位置上。 再说回ensureCapacityInternal，它是用来扩容的，准确说是用来进行扩容检查的。下面我们来看一下整个扩容的过程1234567891011121314151617181920212223242526272829303132333435363738394041424344//初始长度是10，size默认值0，假定添加的是第一个元素，那么minCapacity=1 这是最小容量 如果小于这个容量就会报错//如果底层数组就是默认数组，那么选一个大的值，就是10private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; //调用另一个方法ensureExplicitCapacity ensureExplicitCapacity(minCapacity); &#125; private void ensureExplicitCapacity(int minCapacity) &#123; //记录修改的次数 modCount++; // overflow-conscious code //如果传过来的值大于初始长度 执行grow方法（参数为传过来的值）扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity); &#125;//真正的扩容 private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; //新的容量是在原有的容量基础上+50% 右移一位就是二分之一 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //如果新容量小于最小容量，按照最小容量进行扩容 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: //这里是重点 调用工具类Arrays的copyOf扩容 elementData = Arrays.copyOf(elementData, newCapacity); &#125;//Arrayspublic static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) &#123; T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy;&#125; add(int index, E element)添加到指定的位置123456789101112131415161718public void add(int index, E element) &#123; //判断索引是否越界，如果越界就会抛出下标越界异常 rangeCheckForAdd(index);//扩容检查 ensureCapacityInternal(size + 1); // Increments modCount!! //将指定下标空出 具体作法就是index及其后的所有元素后移一位 System.arraycopy(elementData, index, elementData, index + 1,size - index); //将要添加元素赋值到空出来的指定下标处 elementData[index] = element; //长度加1 size++;&#125;//判断是否出现下标是否越界private void rangeCheckForAdd(int index) &#123; //如果下标超过了集合的尺寸 或者 小于0就是越界 if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; remove(int index)ArrayList支持两种删除元素的方式 remove(int index) 按照下标删除 常用 remove(Object o) 按照元素删除 会删除和参数匹配的第一个元素 下面我们看一下ArrayList的实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061 /** 移除list中指定位置的元素 * Removes the element at the specified position in this list. 所有后续元素左移 下标减1 * Shifts any subsequent elements to the left (subtracts one from their * indices). *参数是要移除元素的下标 * @param index the index of the element to be removed 返回值是被移除的元素 * @return the element that was removed from the list * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E remove(int index) &#123; //下标越界检查 rangeCheck(index);//修改次数统计 modCount++; //获取这个下标上的值 E oldValue = elementData(index);//计算出需要移动的元素个数 （因为需要将后续元素左移一位 此处计算出来的是后续元素的位数） int numMoved = size - index - 1; //如果这个值大于0 说明后续有元素需要左移 是0说明被移除的对象就是最后一位元素 if (numMoved &gt; 0) //索引index只有的所有元素左移一位 覆盖掉index位置上的元素 System.arraycopy(elementData, index+1, elementData, index,numMoved); // 将最后一个元素赋值为null 这样就可以被gc回收了 elementData[--size] = null; // clear to let GC do its work//返回index位置上的元素 return oldValue;&#125;//移除特定元素public boolean remove(Object o) &#123; //如果元素是null 遍历数组移除第一个null if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; //遍历找到第一个null元素的下标 调用下标移除元素的方法 fastRemove(index); return true; &#125; &#125; else &#123; //找到元素对应的下标 调用下标移除元素的方法 for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false;&#125;//按照下标移除元素private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work&#125; ArrayList总结 底层数组实现，使用默认构造方法初始化出来的容量是10 扩容的长度是在原长度基础上加二分之一 实现了RandomAccess接口，底层又是数组，get读取元素性能很好 线程不安全，所有的方法均不是同步方法也没有加锁，因此多线程下慎用 顺序添加很方便 删除和插入需要复制数组 性能很差（可以使用LinkindList） 为什么ArrayList的elementData是用transient修饰的？transient修饰的属性意味着不会被序列化，也就是说在序列化ArrayList的时候，不序列化elementData。 为什么要这么做呢？ elementData不总是满的，每次都序列化，会浪费时间和空间 重写了writeObject 保证序列化的时候虽然不序列化全部 但是有的元素都序列化 所以说不是不序列化 而是不全部序列化。1234567891011121314private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123;// Write out element count, and any hidden stuffint expectedModCount = modCount;s.defaultWriteObject(); // Write out array length s.writeInt(elementData.length); // Write out all elements in the proper order.for (int i=0; i&lt;size; i++) s.writeObject(elementData[i]); if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125; ArrayList和Vector的区别标准答案 ArrayList是线程不安全的，Vector是线程安全的 扩容时候ArrayList扩0.5倍，Vector扩1倍 那么问题来了 ArrayList有没有办法线程安全？Collections工具类有一个synchronizedList方法 可以把list变为线程安全的集合，但是意义不大，因为可以使用Vector Vector为什么是线程安全的？老实讲，抛开多线程 它俩区别没多大，但是多线程下就不一样了，那么Vector是如何实现线程安全的，我们来看几个关键方法123456789101112131415161718192021public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125;public synchronized E remove(int index) &#123; modCount++; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); int numMoved = elementCount - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--elementCount] = null; // Let gc do its work return oldValue;&#125; 就代码实现上来说，和ArrayList并没有很多逻辑上的区别，但是在Vector的关键方法都使用了synchronized修饰。 作者：冰洋链接：https://juejin.im/post/5a58aa62f265da3e4d72a51b来源：掘金","categories":[{"name":"Java","slug":"Java","permalink":"http://github.com/b2stry/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://github.com/b2stry/tags/Java/"}]},{"title":"Javap指令集","slug":"javap指令集","date":"2017-12-26T14:08:36.000Z","updated":"2018-02-03T07:32:11.091Z","comments":true,"path":"2017/12/26/javap指令集/","link":"","permalink":"http://github.com/b2stry/2017/12/26/javap指令集/","excerpt":"Javap指令查询。","text":"Javap指令查询。 栈和局部变量操作 将常量压入栈的指令 aconst_null 将null对象引用压入栈 iconst_m1 将int类型常量-1压入栈 iconst_0 将int类型常量0压入栈 iconst_1 将int类型常量1压入栈 iconst_2 将int类型常量2压入栈 iconst_3 将int类型常量3压入栈 iconst_4 将int类型常量4压入栈 iconst_5 将int类型常量5压入栈 lconst_0 将long类型常量0压入栈 lconst_1 将long类型常量1压入栈 fconst_0 将float类型常量0压入栈 fconst_1 将float类型常量1压入栈 dconst_0 将double类型常量0压入栈 dconst_1 将double类型常量1压入栈 bipush 将一个8位带符号整数压入栈 sipush 将16位带符号整数压入栈 ldc 把常量池中的项压入栈 ldc_w 把常量池中的项压入栈（使用宽索引） ldc2_w 把常量池中long类型或者double类型的项压入栈（使用宽索引） 从栈中的局部变量中装载值的指令 iload 从局部变量中装载int类型值 lload 从局部变量中装载long类型值 fload 从局部变量中装载float类型值 dload 从局部变量中装载double类型值 aload 从局部变量中装载引用类型值（refernce） iload_0 从局部变量0中装载int类型值 iload_1 从局部变量1中装载int类型值 iload_2 从局部变量2中装载int类型值 iload_3 从局部变量3中装载int类型值 lload_0 从局部变量0中装载long类型值 lload_1 从局部变量1中装载long类型值 lload_2 从局部变量2中装载long类型值 lload_3 从局部变量3中装载long类型值 fload_0 从局部变量0中装载float类型值 fload_1 从局部变量1中装载float类型值 fload_2 从局部变量2中装载float类型值 fload_3 从局部变量3中装载float类型值 dload_0 从局部变量0中装载double类型值 dload_1 从局部变量1中装载double类型值 dload_2 从局部变量2中装载double类型值 dload_3 从局部变量3中装载double类型值 aload_0 从局部变量0中装载引用类型值 aload_1 从局部变量1中装载引用类型值 aload_2 从局部变量2中装载引用类型值 aload_3 从局部变量3中装载引用类型值 iaload 从数组中装载int类型值 laload 从数组中装载long类型值 faload 从数组中装载float类型值 daload 从数组中装载double类型值 aaload 从数组中装载引用类型值 baload 从数组中装载byte类型或boolean类型值 caload 从数组中装载char类型值 saload 从数组中装载short类型值 将栈中的值存入局部变量的指令 istore 将int类型值存入局部变量 lstore 将long类型值存入局部变量 fstore 将float类型值存入局部变量 dstore 将double类型值存入局部变量 astore 将将引用类型或returnAddress类型值存入局部变量 istore_0 将int类型值存入局部变量0 istore_1 将int类型值存入局部变量1 istore_2 将int类型值存入局部变量2 istore_3 将int类型值存入局部变量3 lstore_0 将long类型值存入局部变量0 lstore_1 将long类型值存入局部变量1 lstore_2 将long类型值存入局部变量2 lstore_3 将long类型值存入局部变量3 fstore_0 将float类型值存入局部变量0 fstore_1 将float类型值存入局部变量1 fstore_2 将float类型值存入局部变量2 fstore_3 将float类型值存入局部变量3 dstore_0 将double类型值存入局部变量0 dstore_1 将double类型值存入局部变量1 dstore_2 将double类型值存入局部变量2 dstore_3 将double类型值存入局部变量3 astore_0 将引用类型或returnAddress类型值存入局部变量0 astore_1 将引用类型或returnAddress类型值存入局部变量1 astore_2 将引用类型或returnAddress类型值存入局部变量2 astore_3 将引用类型或returnAddress类型值存入局部变量3 iastore 将int类型值存入数组中 lastore 将long类型值存入数组中 fastore 将float类型值存入数组中 dastore 将double类型值存入数组中 aastore 将引用类型值存入数组中 bastore 将byte类型或者boolean类型值存入数组中 castore 将char类型值存入数组中 sastore 将short类型值存入数组中 wide指令 wide 使用附加字节扩展局部变量索引 通用(无类型）栈操作 nop 不做任何操作 pop 弹出栈顶端一个字长的内容 pop2 弹出栈顶端两个字长的内容 dup 复制栈顶部一个字长内容 dup_x1 复制栈顶部一个字长的内容，然后将复制内容及原来弹出的两个字长的内容压入栈 dup_x2 复制栈顶部一个字长的内容，然后将复制内容及原来弹出的三个字长的内容压入栈 dup2 复制栈顶部两个字长内容 dup2_x1 复制栈顶部两个字长的内容，然后将复制内容及原来弹出的三个字长的内容压入栈 dup2_x2 复制栈顶部两个字长的内容，然后将复制内容及原来弹出的四个字长的内容压入栈 swap 交换栈顶部两个字长内容 类型转换 i2l 把int类型的数据转化为long类型 i2f 把int类型的数据转化为float类型 i2d 把int类型的数据转化为double类型 l2i 把long类型的数据转化为int类型 l2f 把long类型的数据转化为float类型 l2d 把long类型的数据转化为double类型 f2i 把float类型的数据转化为int类型 f2l 把float类型的数据转化为long类型 f2d 把float类型的数据转化为double类型 d2i 把double类型的数据转化为int类型 d2l 把double类型的数据转化为long类型 d2f 把double类型的数据转化为float类型 i2b 把int类型的数据转化为byte类型 i2c 把int类型的数据转化为char类型 i2s 把int类型的数据转化为short类型 整数运算 iadd 执行int类型的加法 ladd 执行long类型的加法 isub 执行int类型的减法 lsub 执行long类型的减法 imul 执行int类型的乘法 lmul 执行long类型的乘法 idiv 执行int类型的除法 ldiv 执行long类型的除法 irem 计算int类型除法的余数 lrem 计算long类型除法的余数 ineg 对一个int类型值进行取反操作 lneg 对一个long类型值进行取反操作 iinc 把一个常量值加到一个int类型的局部变量上 逻辑运算 移位操作 ishl 执行int类型的向左移位操作 lshl 执行long类型的向左移位操作 ishr 执行int类型的向右移位操作 lshr 执行long类型的向右移位操作 iushr 执行int类型的向右逻辑移位操作 lushr 执行long类型的向右逻辑移位操作 按位布尔运算 iand 对int类型值进行“逻辑与”操作 land 对long类型值进行“逻辑与”操作 ior 对int类型值进行“逻辑或”操作 lor 对long类型值进行“逻辑或”操作 ixor 对int类型值进行“逻辑异或”操作 lxor 对long类型值进行“逻辑异或”操作 浮点运算 fadd 执行float类型的加法 dadd 执行double类型的加法 fsub 执行float类型的减法 dsub 执行double类型的减法 fmul 执行float类型的乘法 dmul 执行double类型的乘法 fdiv 执行float类型的除法 ddiv 执行double类型的除法 frem 计算float类型除法的余数 drem 计算double类型除法的余数 fneg 将一个float类型的数值取反 dneg 将一个double类型的数值取反 对象和数组 对象操作指令 new 创建一个新对象 checkcast 确定对象为所给定的类型 getfield 从对象中获取字段 putfield 设置对象中字段的值 getstatic 从类中获取静态字段 putstatic 设置类中静态字段的值 instanceof 判断对象是否为给定的类型 数组操作指令 newarray 分配数据成员类型为基本上数据类型的新数组 anewarray 分配数据成员类型为引用类型的新数组 arraylength 获取数组长度 multianewarray 分配新的多维数组 控制流 条件分支指令 ifeq 如果等于0，则跳转 ifne 如果不等于0，则跳转 iflt 如果小于0，则跳转 ifge 如果大于等于0，则跳转 ifgt 如果大于0，则跳转 ifle 如果小于等于0，则跳转 if_icmpcq 如果两个int值相等，则跳转 if_icmpne 如果两个int类型值不相等，则跳转 if_icmplt 如果一个int类型值小于另外一个int类型值，则跳转 if_icmpge 如果一个int类型值大于或者等于另外一个int类型值，则跳转 if_icmpgt 如果一个int类型值大于另外一个int类型值，则跳转 if_icmple 如果一个int类型值小于或者等于另外一个int类型值，则跳转 ifnull 如果等于null，则跳转 ifnonnull 如果不等于null，则跳转 if_acmpeq 如果两个对象引用相等，则跳转 if_acmpnc 如果两个对象引用不相等，则跳转 比较指令 lcmp 比较long类型值 fcmpl 比较float类型值（当遇到NaN时，返回-1） fcmpg 比较float类型值（当遇到NaN时，返回1） dcmpl 比较double类型值（当遇到NaN时，返回-1） dcmpg 比较double类型值（当遇到NaN时，返回1） 无条件转移指令 goto 无条件跳转 goto_w 无条件跳转（宽索引） 表跳转指令 tableswitch 通过索引访问跳转表，并跳转 lookupswitch 通过键值匹配访问跳转表，并执行跳转操作 异常 athrow 抛出异常或错误 finally子句 jsr 跳转到子例程 jsr_w 跳转到子例程（宽索引） rct 从子例程返回 方法调用与返回 方&gt;法调用指令 invokcvirtual 运行时按照对象的类来调用实例方法 invokespecial 根据编译时类型来调用实例方法 invokestatic 调用类（静态）方法 invokcinterface 调用接口方法 方法返回指令 ireturn 从方法中返回int类型的数据 lreturn 从方法中返回long类型的数据 freturn 从方法中返回float类型的数据 dreturn 从方法中返回double类型的数据 areturn 从方法中返回引用类型的数据 return 从方法中返回，返回值为void 线程同步 montiorenter 进入并获取对象监视器 monitorexit 释放并退出对象监视器 JVM指令助记符 变量到操作数栈：iload,iload,lload,lload,fload,fload,dload,dload,aload,aload_ 操作数栈到变量：istore,istore,lstore,lstore,fstore,fstore,dstore,dstor,astore,astore_ 常数到操作数栈：bipush,sipush,ldc,ldc_w,ldc2_w,aconst_null,iconstml,iconst,lconst,fconst,dconst_ 加：iadd,ladd,fadd,dadd 减：isub,lsub,fsub,dsub 乘：imul,lmul,fmul,dmul 除：idiv,ldiv,fdiv,ddiv 余数：irem,lrem,frem,drem 取负：ineg,lneg,fneg,dneg 移位：ishl,lshr,iushr,lshl,lshr,lushr 按位或：ior,lor 按位与：iand,land 按位异或：ixor,lxor 类型转换：i2l,i2f,i2d,l2f,l2d,f2d(放宽数值转换) i2b,i2c,i2s,l2i,f2i,f2l,d2i,d2l,d2f(缩窄数值转换) 创建类实便：new 创建新数组：newarray,anewarray,multianwarray 访问类的域和类实例域：getfield,putfield,getstatic,putstatic 把数据装载到操作数栈：baload,caload,saload,iaload,laload,faload,daload,aaload 从操作数栈存存储到数组：bastore,castore,sastore,iastore,lastore,fastore,dastore,aastore 获取数组长度：arraylength 检相类实例或数组属性：instanceof,checkcast 操作数栈管理：pop,pop2,dup,dup2,dup_xl,dup2_xl,dup_x2,dup2_x2,swap 有条件转移：ifeq,iflt,ifle,ifne,ifgt,ifge,ifnull,ifnonnull,if_icmpeq,if_icmpene, if_icmplt,if_icmpgt,if_icmple,if_icmpge,if_acmpeq,if_acmpne,lcmp,fcmpl fcmpg,dcmpl,dcmpg 复合条件转移：tableswitch,lookupswitch 无条件转移：goto,goto_w,jsr,jsr_w,ret 调度对象的实便方法：invokevirtual 调用由接口实现的方法：invokeinterface 调用需要特殊处理的实例方法：invokespecial 调用命名类中的静态方法：invokestatic 方法返回：ireturn,lreturn,freturn,dreturn,areturn,return 异常：athrow finally关键字的实现使用：jsr,jsr_w,ret","categories":[{"name":"Java","slug":"Java","permalink":"http://github.com/b2stry/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://github.com/b2stry/tags/Java/"}]},{"title":"用Python给头像加上圣诞帽","slug":"用Python给头像加上圣诞帽","date":"2017-12-23T13:53:36.000Z","updated":"2018-01-13T13:30:05.114Z","comments":true,"path":"2017/12/23/用Python给头像加上圣诞帽/","link":"","permalink":"http://github.com/b2stry/2017/12/23/用Python给头像加上圣诞帽/","excerpt":"引言随着圣诞的到来，大家纷纷@官微给自己的头像加上一顶圣(yuan)诞(liang)帽。当然这种事情用很多P图软件都可以做到。但是作为一个学习图像处理的技术人，还是觉得我们有必要写一个程序来做这件事情。而且这完全可以作为一个练手的小项目，工作量不大，而且很有意思。","text":"引言随着圣诞的到来，大家纷纷@官微给自己的头像加上一顶圣(yuan)诞(liang)帽。当然这种事情用很多P图软件都可以做到。但是作为一个学习图像处理的技术人，还是觉得我们有必要写一个程序来做这件事情。而且这完全可以作为一个练手的小项目，工作量不大，而且很有意思。 用到的工具OpenCV库：安装方法–pip install opencv-python -i http://pypi.douban.com/simple/ dlib库：安装方法–pip install dlib -i http://pypi.douban.com/simple/ 图片素材在最下面的Github项目里都有。ps:在国内用豆瓣源速度刚刚的。效果图如下： 代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148import numpy as npimport cv2import dlib# 给img中的人头像加上圣诞帽，人脸最好为正脸def add_hat(img,hat_img): # 分离rgba通道，合成rgb三通道帽子图，a通道后面做mask用 r,g,b,a = cv2.split(hat_img) rgb_hat = cv2.merge((r,g,b)) cv2.imwrite(\"hat_alpha.jpg\",a) # ------------------------- 用dlib的人脸检测代替OpenCV的人脸检测----------------------- # # 灰度变换 # gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # # 用opencv自带的人脸检测器检测人脸 # face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\") # faces = face_cascade.detectMultiScale(gray,1.05,3,cv2.CASCADE_SCALE_IMAGE,(50,50)) # ------------------------- 用dlib的人脸检测代替OpenCV的人脸检测----------------------- # dlib人脸关键点检测器 predictor_path = \"shape_predictor_5_face_landmarks.dat\" predictor = dlib.shape_predictor(predictor_path) # dlib正脸检测器 detector = dlib.get_frontal_face_detector() # 正脸检测 dets = detector(img, 1) # 如果检测到人脸 if len(dets)&gt;0: for d in dets: x,y,w,h = d.left(),d.top(), d.right()-d.left(), d.bottom()-d.top() # x,y,w,h = faceRect # cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2,8,0) # 关键点检测，5个关键点 shape = predictor(img, d) # for point in shape.parts(): # cv2.circle(img,(point.x,point.y),3,color=(0,255,0)) # cv2.imshow(\"image\",img) # cv2.waitKey() # 选取左右眼眼角的点 point1 = shape.part(0) point2 = shape.part(2) # 求两点中心 eyes_center = ((point1.x+point2.x)//2,(point1.y+point2.y)//2) # cv2.circle(img,eyes_center,3,color=(0,255,0)) # cv2.imshow(\"image\",img) # cv2.waitKey() # 根据人脸大小调整帽子大小 factor = 2.2 factor2 = 2 resized_hat_h = int(round(rgb_hat.shape[0]*w/rgb_hat.shape[1]*factor2)) resized_hat_w = int(round(rgb_hat.shape[1]*w/rgb_hat.shape[1]*factor)) if resized_hat_h &gt; y: resized_hat_h = y-1 # 根据人脸大小调整帽子大小 resized_hat = cv2.resize(rgb_hat,(resized_hat_w,resized_hat_h)) # 用alpha通道作为mask mask = cv2.resize(a,(resized_hat_w,resized_hat_h)) mask_inv = cv2.bitwise_not(mask) # 帽子相对与人脸框上线的偏移量 dh = 20 dw = 0 # 原图ROI # bg_roi = img[y+dh-resized_hat_h:y+dh, x+dw:x+dw+resized_hat_w] bg_roi = img[y+dh-resized_hat_h:y+dh,(eyes_center[0]-resized_hat_w//3):(eyes_center[0]+resized_hat_w//3*2)] # 原图ROI中提取放帽子的区域 bg_roi = bg_roi.astype(float) mask_inv = cv2.merge((mask_inv,mask_inv,mask_inv)) alpha = mask_inv.astype(float)/255 # 相乘之前保证两者大小一致（可能会由于四舍五入原因不一致） alpha = cv2.resize(alpha,(bg_roi.shape[1],bg_roi.shape[0])) # print(\"alpha size: \",alpha.shape) # print(\"bg_roi size: \",bg_roi.shape) bg = cv2.multiply(alpha, bg_roi) bg = bg.astype('uint8') cv2.imwrite(\"bg.jpg\",bg) # cv2.imshow(\"image\",img) # cv2.waitKey() # 提取帽子区域 hat = cv2.bitwise_and(resized_hat,resized_hat,mask = mask) cv2.imwrite(\"hat.jpg\",hat) # cv2.imshow(\"hat\",hat) # cv2.imshow(\"bg\",bg) # print(\"bg size: \",bg.shape) # print(\"hat size: \",hat.shape) # 相加之前保证两者大小一致（可能会由于四舍五入原因不一致） hat = cv2.resize(hat,(bg_roi.shape[1],bg_roi.shape[0])) # 两个ROI区域相加 add_hat = cv2.add(bg,hat) # cv2.imshow(\"add_hat\",add_hat) # 把添加好帽子的区域放回原图 img[y+dh-resized_hat_h:y+dh,(eyes_center[0]-resized_hat_w//3):(eyes_center[0]+resized_hat_w//3*2)] = add_hat # 展示效果 # cv2.imshow(\"img\",img ) # cv2.waitKey(0) return img# 读取帽子图，第二个参数-1表示读取为rgba通道，否则为rgb通道hat_img = cv2.imread(\"hat2.png\",-1)# 读取头像图img = cv2.imread(\"test.jpg\")output = add_hat(img,hat_img)# 展示效果cv2.imshow(\"output\",output )cv2.waitKey(0)cv2.imwrite(\"output.jpg\",output)# import glob as gb# img_path = gb.glob(\"./images/*.jpg\")# for path in img_path:# img = cv2.imread(path)# # 添加帽子# output = add_hat(img,hat_img)# # 展示效果# cv2.imshow(\"output\",output )# cv2.waitKey(0)cv2.destroyAllWindows() Tip:可以通过调节帽子的参数让帽子大小更合适些。 项目Github地址：https://github.com/LiuXiaolong19920720/Add-Christmas-Hat —— 膜拜作者大佬。","categories":[{"name":"Python","slug":"Python","permalink":"http://github.com/b2stry/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://github.com/b2stry/tags/Python/"}]},{"title":"教务系统一键评教","slug":"教务系统一键评教","date":"2017-12-23T11:03:21.000Z","updated":"2018-01-13T12:49:23.625Z","comments":true,"path":"2017/12/23/教务系统一键评教/","link":"","permalink":"http://github.com/b2stry/2017/12/23/教务系统一键评教/","excerpt":"这下评教就好受多了。","text":"这下评教就好受多了。 Js代码如下：123456789101112javascript:(function()&#123;var obj=$('iframeautoheight').contentDocument.getElementsByTagName('select');for(i=1;i&lt;obj.length;i++)&#123;obj[i].value=\"优秀\";&#125;var rid=Math.max(1, Math.floor(Math.random()*obj.length));obj[rid].value='良好';var comments=['评语1','评语2','评语3','评语4','评语5'];var cid=Math.floor(Math.random()*comments.length);$('iframeautoheight').contentDocument.getElementById('pjxx').value=comments[cid];$('iframeautoheight').contentDocument.getElementById('Button1').click(); &#125;)() comments数组中可以自定义评语。用法：新建一个标签，名字随意，网址位置输入上述代码。ps:如果第一次没有效果，可以试着将汉字重新输入一遍。","categories":[{"name":"Js","slug":"Js","permalink":"http://github.com/b2stry/categories/Js/"}],"tags":[{"name":"一键评教","slug":"一键评教","permalink":"http://github.com/b2stry/tags/一键评教/"}]},{"title":"列表生成式+过滤器（filter）+映射（map）+lambda总结","slug":"列表生成式-过滤器（filter）-映射（map）-lambda总结","date":"2017-12-18T15:01:29.000Z","updated":"2018-01-13T13:29:58.298Z","comments":true,"path":"2017/12/18/列表生成式-过滤器（filter）-映射（map）-lambda总结/","link":"","permalink":"http://github.com/b2stry/2017/12/18/列表生成式-过滤器（filter）-映射（map）-lambda总结/","excerpt":"这些都是python的特色，不仅强大，而且好用，配合起来使用更是无敌。","text":"这些都是python的特色，不仅强大，而且好用，配合起来使用更是无敌。 lambdalambda用于产生一个匿名表达式，组成部分为：lambda + ‘函数表达式’ ‘函数表达式’由一个冒号加上两个‘操作数’组成，如：1lambda x:x*3 冒号左边的操作数，作为函数的参数；冒号右边的作为函数的放回值！ 那么lambda x:x*3就等价于：12def xxx(x): return x*3 列表生成式列表生成器，可以由3个部分组成，这3个从左到右的顺序是: 1、表达式部分：一般为一个表达式作用一个列表的元素；或者就该元素，不作用任何表达式; 2、列表生成部分：一般为一个for循环产生初始列表，并依次导出元素; 3、过滤部分：一般由一个if判断构成，条件为假的过滤掉。这个部分可选。 总结一下列表生成器就是： [表达式部分 列表生成部分 过滤部分(可选)] , 注意三个部分之间空格隔开即可例子：1[ str(i) for i in range(1, 100) if not(i%3)] 这个理解顺序过程是： 首先由列表生成部分：for i in range(1, 100)逐一产生列表的元素，每个元素经过过滤部分：if not(i%3)检测，判断为假的直接丢弃，判断为真的再交给表达式部分：str(i) 进行处理得到新列表的元素。这个流程直到for结束循环，新列表生成。 过滤器（filter）过滤器有两个参数：function类型，序列 1、序列是将被筛选的原始集合，function类型是制定筛选的规则（公式）。 2、function类型如果返回None，或者说就是None，则序列中为值为假的将被过滤，如：1filter(None,(0,0,False,11,True,1,123))，就会过滤剩下[11, True, 1, 123] 注意到filter()函数返回的是一个Iterator，也就是一个惰性序列，所以要强迫filter()完成计算结果，需要用list()函数获得所有结果并返回list。12&gt;&gt;&gt; list(filter(None,(0,0,False,11,True,1,123)))[11, True, 1, 123] 3、当function类型是一个函数时，filter会将序列中每个元素带入函数，函数返回 假的元素被剔除，如：12&gt;&gt;&gt; list(filter(lambda x:not x%2,[x for x in range(10)]))[0, 2, 4, 6, 8] 4、如果筛选的规则不复杂的话，列表生成式完全可以代替他：1[x for x in range(10) if(not x%2)] 映射（map）1、和filter类似，不过这次不是过滤，而是映射：把一个序列映射成另一个序列，映射规则由一个函数制定。 2、同样map也有两个参数：function类型，序列，如：map(lambda x:x**2,range(10))，这样序列[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]就被映射成了[0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 3、注意到map()函数返回的是一个Iterator，也就是一个惰性序列，所以要强迫filter()完成计算结果，需要用list()函数获得所有结果并返回list。12&gt;&gt;&gt; list(map(lambda x:x**2,range(10)))[0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 4、如果筛选的规则不复杂的话，列表生成式完全可以代替他：1[x**2 for x in range(10)] 总结：1、filter和lambda结合，可以方便的筛选一个序列。 2、map和lambda结合，可以方便的映射出新的序列。 3、如果筛选/映射规则比较简单，可以直接用列表生成式代替，更加方便。 4、他们其实都可以各自结合，产生巨大爆发力。","categories":[{"name":"python","slug":"python","permalink":"http://github.com/b2stry/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://github.com/b2stry/tags/python/"}]},{"title":"关于多表查询sql常用的连接语句：内连接、左外连接、右外连接。","slug":"关于多表查询sql常用的连接语句：左外连接、右外连接、内连接","date":"2017-12-09T17:35:57.000Z","updated":"2018-01-13T07:25:02.400Z","comments":true,"path":"2017/12/10/关于多表查询sql常用的连接语句：左外连接、右外连接、内连接/","link":"","permalink":"http://github.com/b2stry/2017/12/10/关于多表查询sql常用的连接语句：左外连接、右外连接、内连接/","excerpt":"内连接：内连接使用比较运算符（使用像 =或&lt;&gt;之类的比较运算符）根据每个表共有的列的值匹配两个表中的行，根据这两张表中相同列的条件，得出其交集。 例如：检索students和courses表中学生标识号相同的所有行。 内连接有两种，显式的和隐式的，返回连接表中符合连接条件和查询条件的数据行（链接表就是数据库在做查询形成的中间表）。","text":"内连接：内连接使用比较运算符（使用像 =或&lt;&gt;之类的比较运算符）根据每个表共有的列的值匹配两个表中的行，根据这两张表中相同列的条件，得出其交集。 例如：检索students和courses表中学生标识号相同的所有行。 内连接有两种，显式的和隐式的，返回连接表中符合连接条件和查询条件的数据行（链接表就是数据库在做查询形成的中间表）。隐式的内连接：没有INNER JOIN，形成的中间表为两个表的笛卡尔积。123SELECT O.ID,O.ORDER_NUMBER,C.ID,C.NAMEFROM CUSTOMERS C,ORDERS OWHERE C.ID=O.CUSTOMER_ID; 显示的内连接：一般称为内连接，有INNER JOIN，形成的中间表为两个表经过ON条件过滤后的笛卡尔积。12SELECT O.ID,O.ORDER_NUMBER,C.ID,C.NAMEFROM CUSTOMERS C INNER JOIN ORDERS O ON C.ID=O.CUSTOMER_ID; 但是，这两个查询的结果是一样的。 左外连接：用的是LEFT JOIN或LEFT OUTER JOIN连接语句。 根据两张表的关系（外键关联），笛卡尔过滤，也就是求出两张表的交集， 如果交集中，左边表的行， 在右边表中没有匹配，则该条记录左边表有数据， 右边表所有的字段都为null。 左外连接时，写where语句的独立查询条件； 规则，：on后面写连接条件， where后写查询条件123SELECT O.ID,O.ORDER_NUMBER,O.CUSTOMER_ID,C.ID,C.NAMEFROM ORDERS O LEFT OUTER JOIN CUSTOMERS C ON C.ID=O.CUSTOMER_IDWHERE O.ORDER_NUMBER&lt;&gt;'MIKE_ORDER001'; 右外连接：用的是RIGHT JOIN或RIGHT OUTER JOIN连接语句。 和左外连接是相反的，查出的两张表的交集， 如果这条记录，右边表有数据，左边表没有， 则把左边表的字段都设置为null。 案例：123456a表 id name b表 id job parent_id 1 张3 1 23 1 2 李四 2 34 2 3 王武 3 34 4a.id同parent_id 存在关系 内连接查询：12345select a.*,b.* from a inner join b on a.id=b.parent_id结果是1 张3 1 23 12 李四 2 34 2 左连接：123456select a.*,b.* from a left join b on a.id=b.parent_id结果是1 张3 1 23 12 李四 2 34 23 王武 null null null 右连接：123456select a.*,b.* from a right join b on a.id=b.parent_id结果是1 张3 1 23 12 李四 2 34 2null null 3 34 4 对于SQL查询的基本原理： 单表查询：根据WHERE条件过滤表中的记录，然后根据SELECT的选择列选择相应的列进行返回最终结果。 两表连接查询： 在on后面写连接条件， 在where后面写过滤的查询条件，然后再根据SELECT指定的列返回查询结果。 多表连接查询：先对第一个和第二个表按照两表连接做查询，然后用查询结果和第三个表做连接查询，以此类推，直到所有的表都连接上为止，最终形成一个中间的结果表，然后根据WHERE条件过滤中间表的记录，并根据SELECT指定的列返回查询结果。 关于on条件和where条件的区别：ON条件：是过滤两个链接表笛卡尔积形成中间表的约束条件。WHERE条件：ON只进行连接操作，WHERE只过滤中间表的记录。 对于连接类型的选择：在实际运用中如果连接类型选择不当， 不但出现效率低并且可能还会出现逻辑的错误。 查两表关联列相等的数据用内连接 Col_L是Col_R的子集时用右外连接。（左边表是右边表的子集，用右外） Col_R是Col_L的子集时用左外连接。（右边表是左边表的子集， 用左外） 求差操作的时候用联合查询。 并且，多个表查询的时候，这些不同的连接类型可以写到一块 例如：123456SELECT T1.C1,T2.CX,T3.CYFROM TAB1 T1 INNER JOIN TAB2 T2 ON (T1.C1=T2.C2) INNER JOIN TAB3 T3 ON (T1.C1=T2.C3) LEFT OUTER JOIN TAB4 ON(T2.C2=T3.C3);WHERE T1.X &gt;T3.Y; 参考：http://blog.csdn.net/nieson2012/article/details/45789461","categories":[{"name":"sql","slug":"sql","permalink":"http://github.com/b2stry/categories/sql/"}],"tags":[{"name":"sql","slug":"sql","permalink":"http://github.com/b2stry/tags/sql/"}]},{"title":"fetch简介---新一代Ajax API","slug":"深入浅出Fetch-API","date":"2017-12-07T05:14:02.000Z","updated":"2018-01-13T12:50:11.475Z","comments":true,"path":"2017/12/07/深入浅出Fetch-API/","link":"","permalink":"http://github.com/b2stry/2017/12/07/深入浅出Fetch-API/","excerpt":"AJAX半遮半掩的底层API是饱受诟病的一件事情. XMLHttpRequest 并不是专为Ajax而设计的. 虽然各种框架对 XHR 的封装已经足够好用, 但我们可以做得更好。更好用的API是 fetch 。下面简单介绍 window.fetch 方法, 在最新版的 Firefox 和 Chrome 中已经提供支持。","text":"AJAX半遮半掩的底层API是饱受诟病的一件事情. XMLHttpRequest 并不是专为Ajax而设计的. 虽然各种框架对 XHR 的封装已经足够好用, 但我们可以做得更好。更好用的API是 fetch 。下面简单介绍 window.fetch 方法, 在最新版的 Firefox 和 Chrome 中已经提供支持。 XMLHttpRequest在我看来 XHR 有点复杂, 我不想解释为什么“XML”是大写,而“Http”是“骆峰式”写法。使用XHR的方式大致如下:123456789101112131415161718// 获取 XHR 非常混乱!if (window.XMLHttpRequest) &#123; // Mozilla, Safari, ... request = new XMLHttpRequest();&#125; else if (window.ActiveXObject) &#123; // IE try &#123; request = new ActiveXObject('Msxml2.XMLHTTP'); &#125; catch (e) &#123; try &#123; request = new ActiveXObject('Microsoft.XMLHTTP'); &#125; catch (e) &#123;&#125; &#125;&#125;// 打开连接, 发送数据.request.open('GET', 'https://davidwalsh.name/ajax-endpoint', true);request.send(null); 我们可以看出, XHR 其实是很杂乱的; 当然, 通过 JavaScript 框架可以很方便地使用XHR。 fetch 的基本使用fetch 是全局量 window 的一个方法, 第一个参数是URL:12345678// url (必须), options (可选)fetch('/some/url', &#123; method: 'get'&#125;).then(function(response) &#123;&#125;).catch(function(err) &#123; // 出错了;等价于 then 的第二个参数,但这样更好用更直观 :(&#125;); 和 Battery API 类似, fetch API 也使用了 JavaScript Promises 来处理结果/回调:123456789101112131415// 对响应的简单处理fetch('/some/url').then(function(response) &#123;&#125;).catch(function(err) &#123; // 出错了;等价于 then 的第二个参数,但这样更直观 :(&#125;);// 链式处理,将异步变为类似单线程的写法: 高级用法.fetch('/some/url').then(function(response) &#123; return //... 执行成功, 第1步...&#125;).then(function(returnedValue) &#123; // ... 执行成功, 第2步...&#125;).catch(function(err) &#123; // 中途任何地方出错...在此处理 :(&#125;); 如果你还不习惯 then 方式的写法,那最好学习一下,因为很快就会全面流行。 请求头(Request Headers)自定义请求头信息极大地增强了请求的灵活性。我们可以通过 new Headers() 来创建请求头:1234567891011121314151617181920// 创建一个空的 Headers 对象,注意是Headers，不是Headervar headers = new Headers();// 添加(append)请求头信息headers.append('Content-Type', 'text/plain');headers.append('X-My-Custom-Header', 'CustomValue');// 判断(has), 获取(get), 以及修改(set)请求头的值headers.has('Content-Type'); // trueheaders.get('Content-Type'); // \"text/plain\"headers.set('Content-Type', 'application/json');// 删除某条请求头信息(a header)headers.delete('X-My-Custom-Header');// 创建对象时设置初始化信息var headers = new Headers(&#123; 'Content-Type': 'text/plain', 'X-My-Custom-Header': 'CustomValue'&#125;); 可以使用的方法包括: append, has, get, set, 以及 delete 。需要创建一个 Request 对象来包装请求头:1234567var request = new Request('/some-url', &#123; headers: new Headers(&#123; 'Content-Type': 'text/plain' &#125;)&#125;);fetch(request).then(function() &#123; /* handle response */ &#125;); 下面介绍 Response 和Request 的使用方法!Request 简介Request 对象表示一次 fetch调用的请求信息。传入 Request 参数来调用 fetch, 可以执行很多自定义请求的高级用法: method - 支持 GET, POST, PUT, DELETE, HEAD url - 请求的 URL headers - 对应的 Headers 对象 referrer - 请求的 referrer 信息 mode - 可以设置 cors, no-cors, same-origin credentials - 设置 cookies 是否随请求一起发送。可以设置: omit, same-origin redirect - follow, error, manual integrity - subresource 完整性值(integrity value) cache - 设置 cache 模式 (default, reload, no-cache)Request 的示例如下:1234567891011var request = new Request('/users.json', &#123; method: 'POST', mode: 'cors', redirect: 'follow', headers: new Headers(&#123; 'Content-Type': 'text/plain' &#125;)&#125;);// 使用!fetch(request).then(function() &#123; /* handle response */ &#125;); 只有第一个参数 URL 是必需的。在 Request 对象创建完成之后, 所有的属性都变为只读属性. 请注意, Request 有一个很重要的 clone 方法, 特别是在 Service Worker API 中使用时 —— 一个 Request 就代表一串流(stream), 如果想要传递给另一个 fetch 方法,则需要进行克隆。 fetch 的方法签名(signature,可理解为配置参数), 和 Request 很像, 示例如下:12345678fetch(&apos;/users.json&apos;, &#123; method: &apos;POST&apos;, mode: &apos;cors&apos;, redirect: &apos;follow&apos;, headers: new Headers(&#123; &apos;Content-Type&apos;: &apos;text/plain&apos; &#125;)&#125;).then(function() &#123; /* handle response */ &#125;); 因为 Request 和 fetch 的签名一致, 所以在 Service Workers 中, 你可能会更喜欢使用Request 对象。关于 ServiceWorker 的相关博客请等待后续更新! Response 简介Response 代表响应,fetch 的 then 方法接收一个 Response实例, 当然你也可以手动创建Response对象 —— 比如在 service workers 中可能会用到.Response 可以配置的参数包括: type - 类型,支持: basic, cors url useFinalURL - Boolean 值, 代表 url 是否是最终 URL status - 状态码 (例如: 200, 404, 等等) ok - Boolean值,代表成功响应(status 值在 200-299 之间) statusText - 状态值(例如: OK) headers - 与响应相关联的 Headers 对象. 12345678910111213// 在 service worker 测试中手动创建 response// new Response(BODY, OPTIONS)var response = new Response('.....', &#123; ok: false, status: 404, url: '/'&#125;);// fetch 的 `then` 会传入一个 Response 对象fetch('/') .then(function(responseObj) &#123; console.log('status: ', responseObj.status); &#125;); Response 提供的方法如下: clone() - 创建一个新的 Response 克隆对象. error() - 返回一个新的,与网络错误相关的 Response 对象. redirect() - 重定向,使用新的 URL 创建新的 response 对象.. arrayBuffer() - Returns a promise that resolves with an ArrayBuffer. blob()- 返回一个 promise, resolves 是一个 Blob. formData() - 返回一个 promise, resolves 是一个 FormData 对象. json() - 返回一个 promise, resolves 是一个 JSON 对象. text() - 返回一个 promise, resolves 是一个 USVString (text).处理 JSON响应 假设需要请求 JSON —— 回调结果对象 response 中有一个json()方法,用来将原始数据转换成JavaScript对象:1234567fetch(&apos;https://davidwalsh.name/demo/arsenal.json&apos;).then(function(response) &#123; // 转换为 JSON return response.json();&#125;).then(function(j) &#123; // 现在, `j` 是一个 JavaScript object console.log(j);&#125;); 当然这很简单 , 只是封装了 JSON.parse(jsonString) 而已, 但 json 方法还是很方便的。 处理基本的Text / HTML响应JSON 并不总是理想的请求/响应数据格式, 那么我们看看如何处理 HTML或文本结果:1234567fetch(&apos;/next/page&apos;) .then(function(response) &#123; return response.text(); &#125;).then(function(text) &#123; // &lt;!DOCTYPE .... console.log(text); &#125;); 如上面的代码所示, 可以在 Promise 链式的then方法中, 先返回 text() 结果 ,再获取 text 。 处理Blob结果如果你想通过 fetch 加载图像或者其他二进制数据, 则会略有不同:1234567fetch('flowers.jpg') .then(function(response) &#123; return response.blob(); &#125;) .then(function(imageBlob) &#123; document.querySelector('img').src = URL.createObjectURL(imageBlob); &#125;); 响应 Body mixin 的 blob() 方法处理响应流(Response stream), 并且将其读完。 提交表单数据(Posting Form Data)另一种常用的 AJAX 调用是提交表单数据 —— 示例代码如下:1234fetch('/submit', &#123; method: 'post', body: new FormData(document.getElementById('comment-form'))&#125;); 提交 JSON 的示例如下:1234567fetch('/submit-json', &#123; method: 'post', body: JSON.stringify(&#123; email: document.getElementById('email').value answer: document.getElementById('answer').value &#125;)&#125;); 非常非常简单, 妈妈再也不用担心我的Ajax! 未完的故事(Unwritten Story)fetch 是个很实用的API , 当前还不允许取消请求, 这使得很多程序员暂时不会考虑它。 新的 fetch API 比起XHR 更简单也更智能。毕竟,它就是专为AJAX而设计的, 具有后发优势. 而我已经迫不及待地使用了, 即使现在兼容性还不是那么好! 本文简单介绍了 fetch 。更多信息请访问 Fetch简介。如果你要使用fetch, 也想寻找polyfill(兼容代码), 请点击: GitHub上的fetch实现 https://github.com/github/fetch。 翻译人员: 铁锚 http://blog.csdn.net/renfufei 翻译时间: 2016年5月22日 原文时间: 2016年4月15日 原文链接: https://davidwalsh.name/fetch","categories":[{"name":"fetch","slug":"fetch","permalink":"http://github.com/b2stry/categories/fetch/"}],"tags":[{"name":"fetch","slug":"fetch","permalink":"http://github.com/b2stry/tags/fetch/"}]},{"title":"理解Spring中的IOC和AOP","slug":"理解Spring中的IOC和AOP","date":"2017-11-27T13:50:31.000Z","updated":"2018-01-13T12:49:45.833Z","comments":true,"path":"2017/11/27/理解Spring中的IOC和AOP/","link":"","permalink":"http://github.com/b2stry/2017/11/27/理解Spring中的IOC和AOP/","excerpt":"我们是在使用Spring框架的过程中，其实就是为了使用IOC，依赖注入和AOP，面向切面编程，这两个是Spring的灵魂。","text":"我们是在使用Spring框架的过程中，其实就是为了使用IOC，依赖注入和AOP，面向切面编程，这两个是Spring的灵魂。 主要用到的设计模式有工厂模式和代理模式IOC就是典型的工厂模式，通过sessionfactory去注入实例。 AOP就是典型的代理模式的体现。 代理模式是常用的java设计模式，他的特征是代理类与委托类有同样的接口，代理类主要负责为委托类预处理消息、过滤消息、把消息转发给委托类，以及事后处理消息等。代理类与委托类之间通常会存在关联关系，一个代理类的对象与一个委托类的对象关联，代理类的对象本身并不真正实现服务，而是通过调用委托类的对象的相关方法，来提供特定的服务。 spring的IoC容器是spring的核心，spring AOP是spring框架的重要组成部分。在传统的程序设计中，当调用者需要被调用者的协助时，通常由调用者来创建被调用者的实例。但在spring里创建被调用者的工作不再由调用者来完成，因此控制反转（IoC）；创建被调用者实例的工作通常由spring容器来完成，然后注入调用者，因此也被称为依赖注入（DI），依赖注入和控制反转是同一个概念。 面向方面编程（AOP)是以另一个角度来考虑程序结构，通过分析程序结构的关注点来完善面向对象编程（OOP）。OOP将应用程序分解成各个层次的对象，而AOP将程序分解成多个切面。spring AOP 只实现了方法级别的连接点，在J2EE应用中，AOP拦截到方法级别的操作就已经足够。在spring中，未来使IoC方便地使用健壮、灵活的企业服务，需要利用spring AOP实现为IoC和企业服务之间建立联系。 IOC:控制反转也叫依赖注入。利用了工厂模式将对象交给容器管理，你只需要在spring配置文件总配置相应的bean，以及设置相关的属性，让spring容器来生成类的实例对象以及管理对象。在spring容器启动的时候，spring会把你在配置文件中配置的bean都初始化好，然后在你需要调用的时候，就把它已经初始化好的那些bean分配给你需要调用这些bean的类（假设这个类名是A），分配的方法就是调用A的setter方法来注入，而不需要你在A里面new这些bean了。 注意：面试的时候，如果有条件，画图，这样更加显得你懂了. AOP:面向切面编程。（Aspect-Oriented Programming）AOP可以说是对OOP的补充和完善。OOP引入封装、继承和多态性等概念来建立一种对象层次结构，用以模拟公共行为的一个集合。当我们需要为分散的对象引入公共行为的时候，OOP则显得无能为力。也就是说，OOP允许你定义从上到下的关系，但并不适合定义从左到右的关系。例如日志功能。日志代码往往水平地散布在所有对象层次中，而与它所散布到的对象的核心功能毫无关系。在OOP设计中，它导致了大量代码的重复，而不利于各个模块的重用。 将程序中的交叉业务逻辑（比如安全，日志，事务等），封装成一个切面，然后注入到目标对象（具体业务逻辑）中去。 实现AOP的技术，主要分为两大类：一是采用动态代理技术，利用截取消息的方式，对该消息进行装饰，以取代原有对象行为的执行；二是采用静态织入的方式，引入特定的语法创建“方面”，从而使得编译器可以在编译期间织入有关“方面”的代码.简单点解释，比方说你想在你的biz层所有类中都加上一个打印‘你好’的功能,这时就可以用aop思想来做.你先写个类写个类方法，方法经实现打印‘你好’,然后Ioc这个类 ref＝“biz.*”让每个类都注入即可实现。 spring 的优点？1.降低了组件之间的耦合性 ，实现了软件各层之间的解耦 2.可以使用容易提供的众多服务，如事务管理，消息服务等 3.容器提供单例模式支持 4.容器提供了AOP技术，利用它很容易实现如权限拦截，运行期监控等功能 5.容器提供了众多的辅助类，能加快应用的开发 6.spring对于主流的应用框架提供了集成支持，如hibernate，JPA，Struts等 7.spring属于低侵入式设计，代码的污染极低 8.独立于各种应用服务器 9.spring的DI机制降低了业务对象替换的复杂性 10.Spring的高度开放性，并不强制应用完全依赖于Spring，开发者可以自由选择spring的部分或全部 什么是DI机制？依赖注入（Dependecy Injection）和控制反转（Inversion of Control）是同一个概念，具体的讲：当某个角色需要另外一个角色协助的时候，在传统的程序设计过程中，通常由调用者来创建被调用者的实例。但在spring中 创建被调用者的工作不再由调用者来完成，因此称为控制反转。创建被调用者的工作由spring来完成，然后注入调用者因此也称为依赖注入。 spring以动态灵活的方式来管理对象 ， 注入的两种方式，设置注入和构造注入。 设置注入的优点：直观，自然 构造注入的优点：可以在构造器中决定依赖关系的顺序。 什么是AOP？面向切面编程（AOP）完善spring的依赖注入（DI），面向切面编程在spring中主要表现为两个方面 1.面向切面编程提供声明式事务管理 2.spring支持用户自定义的切面 面向切面编程（aop）是对面向对象编程（oop）的补充：面向对象编程将程序分解成各个层次的对象，面向切面编程将程序运行过程分解成各个切面。 AOP从程序运行角度考虑程序的结构，提取业务处理过程的切面，oop是静态的抽象，aop是动态的抽象，是对应用执行过程中的步骤进行抽象，，从而获得步骤之间的逻辑划分。 aop框架具有的两个特征：1.各个步骤之间的良好隔离性2.源代码无关性 来源：博客园链接：https://www.cnblogs.com/gaopeng527/p/5290997.html","categories":[{"name":"spring","slug":"spring","permalink":"http://github.com/b2stry/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://github.com/b2stry/tags/spring/"}]},{"title":"消息队列之ActiveMQ","slug":"消息队列之ActiveMQ","date":"2017-11-22T10:46:58.000Z","updated":"2018-02-21T09:36:35.161Z","comments":true,"path":"2017/11/22/消息队列之ActiveMQ/","link":"","permalink":"http://github.com/b2stry/2017/11/22/消息队列之ActiveMQ/","excerpt":"1.概述中间件非底层操作系统软件，非业务应用软件，不是直接给最终用户使用的，不能直接给客户带来价值的软件统称为中间件。 消息中间件管制关注于数据的发送和接收，利用高效可靠的异步消息传递机制集成分布式系统。 优点： 解耦 异步 横向扩展 安全可靠 顺序保证（比如kafka）","text":"1.概述中间件非底层操作系统软件，非业务应用软件，不是直接给最终用户使用的，不能直接给客户带来价值的软件统称为中间件。 消息中间件管制关注于数据的发送和接收，利用高效可靠的异步消息传递机制集成分布式系统。 优点： 解耦 异步 横向扩展 安全可靠 顺序保证（比如kafka） JMSJava消息服务(Java Message Service)即JMS，是一个Java平台中关于面向消息中间件的api，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。 什么是AMQP？AMQP(advanced message queuing protocol)是一个提供统一消息服务的应用层标准协议，基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同开发语言等条件的限制。 常见消息中间件ActiveMQ（支持多语言，实现jms1.1，j2ee1.4规范），RabbitMQ（支持更多语言，基于AMQP规范），Kafka(高吞吐量，分布式，分区,O(1)磁盘顺序提供消息持久化) JMS消息模式队列模式客户端包括生产者和消费者 队列中的消息只能被一个消费者消费 消费者可以随时消费队列中的消息 主题模式客户端包括发布者和订阅者 主题中的消息被所有订阅者消费 消费者不能消费订阅之前就发送到主题中的消息 JMS编码接口ConnectionFactory 用于创建连接到消息中间件的连接工厂 Connection 代表了应用程序和消息服务器之间的通信链路 Destination 指消息发布和接收的地点，包括队列或主题 Session 表示一个单线程的上下文，用于发送和接收消息 MessageConsumer 由会话创建，用于接收发送到目标的消息 MessageProducer 由会话创建，用于发送消息到目标 Message 是在消费者和生产者之间传送的对象，消息头，一组消息属性，一个消息体流程 2.安装activeMQ官网 ： http://activemq.apache.org/ 第一种启动进入bin ，activemq.bat 启动进入浏览器 http://127.0.0.1:8161用户名密码默认为admin 第二种以服务启动InstallService.bat以管理员身份运行服务中有ActiveMQlinux解压压缩包进入bin 输入 activemq start，启动完成 3.jms 演示创建maven项目 添加依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-all&lt;/artifactId&gt; &lt;version&gt;5.9.0&lt;/version&gt;&lt;/dependency&gt; 创建队列模式的生产者AppProducer 1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.jms.queue;import javax.jms.Connection;import javax.jms.ConnectionFactory;import javax.jms.Destination;import javax.jms.JMSException;import javax.jms.MessageProducer;import javax.jms.Session;import javax.jms.TextMessage;import org.apache.activemq.ActiveMQConnectionFactory;public class AppProducer &#123; public static final String url = \"tcp://localhost:61616\"; public static final String queueName = \"queue-test \"; public static void main(String[] args) throws JMSException &#123; //1.创建连接工厂 ConnectionFactory connectionFactory = new ActiveMQConnectionFactory(url); //2.创建Connection Connection connection = connectionFactory.createConnection(); //3.启动连接 connection.start(); //4.创建会话 第一个参数：是否在事务中去处理， 第二个参数.应答模式 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); //5.创建一个目标 Destination destination = session.createQueue(queueName); //6创建一个生产者 MessageProducer producer = session.createProducer(destination); for (int i = 0; i &lt; 100; i++) &#123; //7.创建消息 TextMessage textMessage = session.createTextMessage(\"test:\"+i); producer.send(textMessage); System.out.println(\"发送消息:\"+textMessage.getText()); &#125; //8关闭连接 connection.close(); &#125;&#125; 运行项目，打开activeMQ管理工具 100个消息，没有消费者消费,连接已关闭 消费者 AppConsumer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.jms.queue;import javax.jms.Connection;import javax.jms.ConnectionFactory;import javax.jms.Destination;import javax.jms.JMSException;import javax.jms.Message;import javax.jms.MessageConsumer;import javax.jms.MessageListener;import javax.jms.Session;import javax.jms.TextMessage;import org.apache.activemq.ActiveMQConnectionFactory;public class AppConsumer &#123; public static final String url = \"tcp://localhost:61616\"; public static final String queueName = \"queue-test \"; public static void main(String[] args) throws JMSException &#123; //1.创建连接工厂 ConnectionFactory connectionFactory = new ActiveMQConnectionFactory(url); //2.创建Connection Connection connection = connectionFactory.createConnection(); //3.启动连接 connection.start(); //4.创建会话 第一个参数：是否在事务中去处理， 第二个参数.应答模式 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); //5.创建一个目标 Destination destination = session.createQueue(queueName); //6创建一个消费者 MessageConsumer consumber = session.createConsumer(destination); //7创建一个监听器 consumber.setMessageListener( (message) -&gt; &#123; TextMessage textMessage = (TextMessage) message; try &#123; System.out.println(\"接收消息：\"+textMessage.getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;); //8关闭连接 消息是异步的 ，在程序退出是关闭，在这里不可以关闭 //connection.close(); &#125;&#125; 当启动2个消费者，再启动生产者，结果是2个消费者平均消费 创建主题模式发布者AppProducer 1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.jms.topic;import javax.jms.Connection;import javax.jms.ConnectionFactory;import javax.jms.Destination;import javax.jms.JMSException;import javax.jms.MessageProducer;import javax.jms.Session;import javax.jms.TextMessage;import org.apache.activemq.ActiveMQConnectionFactory;public class AppProducer &#123; public static final String url = \"tcp://localhost:61616\"; public static final String topicName = \"topic-test \"; public static void main(String[] args) throws JMSException &#123; //1.创建连接工厂 ConnectionFactory connectionFactory = new ActiveMQConnectionFactory(url); //2.创建Connection Connection connection = connectionFactory.createConnection(); //3.启动连接 connection.start(); //4.创建会话 第一个参数：是否在事务中去处理， 第二个参数.应答模式 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); //5.创建一个目标 Destination destination = session.createTopic(topicName); //6创建一个生产者 MessageProducer producer = session.createProducer(destination); for (int i = 0; i &lt; 100; i++) &#123; //7.创建消息 TextMessage textMessage = session.createTextMessage(\"test:\"+i); producer.send(textMessage); System.out.println(\"发送消息:\"+textMessage.getText()); &#125; //8关闭连接 connection.close(); &#125;&#125; 这时直接运行订阅者接收不到消息，因为发布者先运行了 订阅者AppConsumer12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.jms.topic;import javax.jms.Connection;import javax.jms.ConnectionFactory;import javax.jms.Destination;import javax.jms.JMSException;import javax.jms.Message;import javax.jms.MessageConsumer;import javax.jms.MessageListener;import javax.jms.Session;import javax.jms.TextMessage;import org.apache.activemq.ActiveMQConnectionFactory;public class AppConsumer &#123; public static final String url = \"tcp://localhost:61616\"; public static final String topicName = \"topic-test \"; public static void main(String[] args) throws JMSException &#123; //1.创建连接工厂 ConnectionFactory connectionFactory = new ActiveMQConnectionFactory(url); //2.创建Connection Connection connection = connectionFactory.createConnection(); //3.启动连接 connection.start(); //4.创建会话 第一个参数：是否在事务中去处理， 第二个参数.应答模式 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); //5.创建一个目标 Destination destination = session.createTopic(topicName); //6创建一个消费者 MessageConsumer consumber = session.createConsumer(destination); //7创建一个监听器 consumber.setMessageListener(new MessageListener() &#123; public void onMessage(Message message) &#123; TextMessage textMessage = (TextMessage) message; try &#123; System.out.println(\"接收消息：\"+textMessage.getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); //8关闭连接 消息是异步的 ，在程序退出是关闭 //connection.close(); &#125;&#125; 启动两个订阅者，再启动发布者，两个订阅者均可收到发布者的消息 4.使用spring集成jms链接activeMQConnectionFactory 用于管理连接的工厂 JmsTemplate 用于发送和接收消息的模板类 MessageListerner 消息监听器 ConnectionFactory 是spring为我们提供的连接池 两种连接池SingleConnectionFactory和 CachingConnectionFactory SingleConnectionFactory 是对于jms建立请求，只会返回一个连接 CachingConnectionFactory 实现了SingleConnectionFactory 的所有功能，还提供了缓存 JmsTemplate spring提供，线程安全，可以使用JmsTemplate 操作jms MessageListerner 实现onMessage方法，接收Message参数 添加依赖12345678910111213141516171819202122232425262728293031323334353637&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-all&lt;/artifactId&gt; &lt;version&gt;5.7.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-core&lt;/artifactId&gt; &lt;version&gt;5.7.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 这里注意org.apache.activemq也依赖了spring-context 所以这里要把他排除掉 生产者 创建接口ProducerService1234567package com.spring.producer;public interface ProducerService &#123; void sendMessage(String message);&#125; 实现类12345678910111213141516171819202122232425262728293031323334package com.spring.producer.impl;import javax.annotation.Resource;import javax.jms.Destination;import javax.jms.JMSException;import javax.jms.Message;import javax.jms.Session;import javax.jms.TextMessage;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jms.core.JmsTemplate;import org.springframework.jms.core.MessageCreator;import org.springframework.stereotype.Service;import com.spring.producer.ProducerService;@Servicepublic class ProducerServiceImpl implements ProducerService &#123; @Autowired private JmsTemplate jmsTemplate; @Resource(name= \"queueDestination\")//因为可能配置多个目的地，所以使用resource name进行区分 Destination destination; public void sendMessage(final String message) &#123; //使用JmsTemplate发送消息 jmsTemplate.send(destination,new MessageCreator() &#123; //创建消息 public Message createMessage(Session session) throws JMSException &#123; TextMessage textMessage = session.createTextMessage(message); return textMessage; &#125; &#125;); System.out.println(\"发送消息：\"+message); &#125;&#125; 配置1234567891011121314151617181920212223242526272829303132&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.3.xsd\"&gt; &lt;context:annotation-config&gt;&lt;/context:annotation-config&gt; &lt;context:component-scan base-package=\"com.spring.*\" /&gt; &lt;!-- ActiveMQ提供 ConnectionFactory--&gt; &lt;bean id=\"targetConnectionFactory\" class=\"org.apache.activemq.ActiveMQConnectionFactory\"&gt; &lt;property name=\"brokerURL\" value=\"tcp://localhost:61616\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- spring-jms提供的连接池 --&gt; &lt;bean id=\"connectionFactory\" class=\"org.springframework.jms.connection.SingleConnectionFactory\"&gt; &lt;property name=\"targetConnectionFactory\" ref=\"targetConnectionFactory\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 一个队列目的地，点对点 --&gt; &lt;bean id=\"queueDestination\" class=\"org.apache.activemq.command.ActiveMQQueue\"&gt; &lt;!-- 指定队列名字 --&gt; &lt;constructor-arg value=\"springQueue\"&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id=\"jmsTemplate\" class=\"org.springframework.jms.core.JmsTemplate\"&gt; &lt;property name=\"connectionFactory\" ref=\"connectionFactory\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- &lt;bean id=\"ProducerServiceImpl\" class=\"com.spring.producer.impl.ProducerServiceImpl\"&gt;&lt;/bean&gt; --&gt;&lt;/beans&gt; 启动类1234567891011121314151617package com.spring.producer;import org.springframework.context.ApplicationContext;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class AppProducer &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"producer.xml\"); ProducerService service = context.getBean(ProducerService.class); for(int i=0;i&lt;100;i++)&#123; service.sendMessage(\"test\"+i); &#125; //会自动清理资源 ((AbstractApplicationContext) context).close(); &#125;&#125; 可以把公共地方提取出来1&lt;import resource=\"common.xml\"/&gt; 创建消费者 配置12345678910111213141516171819202122232425&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.3.xsd\"&gt; &lt;import resource=\"common.xml\"/&gt; &lt;!-- 导入公共配置 --&gt; &lt;!-- 配置消息监听器 --&gt; &lt;bean id=\"consumerMessageListener\" class=\"com.spring.consumer.ConsumberMessageListener\"&gt; &lt;/bean&gt; &lt;!-- jms容器 管理容器指定消息目的地，和消息监听者 --&gt; &lt;bean id=\"jmsContainer\" class=\"org.springframework.jms.listener.DefaultMessageListenerContainer\"&gt; &lt;property name=\"connectionFactory\" ref=\"connectionFactory\"&gt;&lt;/property&gt; &lt;!-- 目的地 --&gt; &lt;property name=\"destination\" ref=\"queueDestination\"&gt;&lt;/property&gt; &lt;!-- 监听器 --&gt; &lt;property name=\"messageListener\" ref=\"consumerMessageListener\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 监听类12345678910111213141516171819202122package com.spring.consumer;import javax.jms.JMSException;import javax.jms.Message;import javax.jms.MessageListener;import javax.jms.TextMessage;import org.springframework.stereotype.Service;@Servicepublic class ConsumberMessageListener implements MessageListener &#123; public void onMessage(Message message) &#123; TextMessage textMessage = (TextMessage)message; try &#123; System.out.println(\"接收消息\"+textMessage.getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 启动类1234567891011package com.spring.consumer;import org.springframework.context.ApplicationContext;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class AppConsumber &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"consumer.xml\"); &#125;&#125; 主题模式 在common配置添加123&lt;bean id=\"topicDestination\" class=\"org.apache.activemq.command.ActiveMQTopic\"&gt; &lt;constructor-arg value=\"toptic\"/&gt; &lt;/bean&gt; 改ProducerServiceImpl Resource为1name= \"topicDestination\" 改consumer.xml jmsContainer1&lt;property name=\"destination\" ref=\"queueDestination\"&gt;&lt;/property&gt;","categories":[{"name":"activemq","slug":"activemq","permalink":"http://github.com/b2stry/categories/activemq/"}],"tags":[{"name":"消息中间件","slug":"消息中间件","permalink":"http://github.com/b2stry/tags/消息中间件/"}]},{"title":"跨域资源共享CORS详解","slug":"跨域资源共享CORS详解","date":"2017-11-21T03:48:44.000Z","updated":"2018-01-13T12:49:40.114Z","comments":true,"path":"2017/11/21/跨域资源共享CORS详解/","link":"","permalink":"http://github.com/b2stry/2017/11/21/跨域资源共享CORS详解/","excerpt":"CORS是一个W3C标准，全称是”跨域资源共享”（Cross-origin resource sharing）。 它允许浏览器向跨源服务器，发出XMLHttpRequest请求，从而克服了AJAX只能同源使用的限制。 本文详细介绍CORS的内部机制。","text":"CORS是一个W3C标准，全称是”跨域资源共享”（Cross-origin resource sharing）。 它允许浏览器向跨源服务器，发出XMLHttpRequest请求，从而克服了AJAX只能同源使用的限制。 本文详细介绍CORS的内部机制。 一、简介CORS需要浏览器和服务器同时支持。目前，所有浏览器都支持该功能，IE浏览器不能低于IE10。 整个CORS通信过程，都是浏览器自动完成，不需要用户参与。对于开发者来说，CORS通信与同源的AJAX通信没有差别，代码完全一样。浏览器一旦发现AJAX请求跨源，就会自动添加一些附加的头信息，有时还会多出一次附加的请求，但用户不会有感觉。 因此，实现CORS通信的关键是服务器。只要服务器实现了CORS接口，就可以跨源通信。 二、两种请求浏览器将CORS请求分成两类：简单请求（simple request）和非简单请求（not-so-simple request）。 只要同时满足以下两大条件，就属于简单请求。12345678910111213（1) 请求方法是以下三种方法之一：HEADGETPOST（2）HTTP的头信息不超出以下几种字段：AcceptAccept-LanguageContent-LanguageLast-Event-IDContent-Type：只限于三个值:application/x-www-form-urlencoded、multipart/form-data、text/plain 凡是不同时满足上面两个条件，就属于非简单请求。浏览器对这两种请求的处理，是不一样的。 三、简单请求3.1 基本流程对于简单请求，浏览器直接发出CORS请求。具体来说，就是在头信息之中，增加一个Origin字段。 下面是一个例子，浏览器发现这次跨源AJAX请求是简单请求，就自动在头信息之中，添加一个Origin字段。123456GET /cors HTTP/1.1Origin: http://api.bob.comHost: api.alice.comAccept-Language: en-USConnection: keep-aliveUser-Agent: Mozilla/5.0... 上面的头信息中，Origin字段用来说明，本次请求来自哪个源（协议 + 域名 + 端口）。服务器根据这个值，决定是否同意这次请求。 如果Origin指定的源，不在许可范围内，服务器会返回一个正常的HTTP回应。浏览器发现，这个回应的头信息没有包含Access-Control-Allow-Origin字段（详见下文），就知道出错了，从而抛出一个错误，被XMLHttpRequest的onerror回调函数捕获。注意，这种错误无法通过状态码识别，因为HTTP回应的状态码有可能是200。如果Origin指定的域名在许可范围内，服务器返回的响应，会多出几个头信息字段。1234Access-Control-Allow-Origin: http://api.bob.comAccess-Control-Allow-Credentials: trueAccess-Control-Expose-Headers: FooBarContent-Type: text/html; charset=utf-8 上面的头信息之中，有三个与CORS请求相关的字段，都以Access-Control-开头。 （1）Access-Control-Allow-Origin该字段是必须的。它的值要么是请求时Origin字段的值，要么是一个*，表示接受任意域名的请求。 （2）Access-Control-Allow-Credentials该字段可选。它的值是一个布尔值，表示是否允许发送Cookie。默认情况下，Cookie不包括在CORS请求之中。设为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器。这个值也只能设为true，如果服务器不要浏览器发送Cookie，删除该字段即可。 （3）Access-Control-Expose-Headers该字段可选。CORS请求时，XMLHttpRequest对象的getResponseHeader()方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma。如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定。上面的例子指定，getResponseHeader(&#39;FooBar&#39;)可以返回FooBar字段的值。 3.2 withCredentials 属性上面说到，CORS请求默认不发送Cookie和HTTP认证信息。如果要把Cookie发到服务器，一方面要服务器同意，指定Access-Control-Allow-Credentials字段。1Access-Control-Allow-Credentials: true 另一方面，开发者必须在AJAX请求中打开withCredentials属性。12var xhr = new XMLHttpRequest();xhr.withCredentials = true; 否则，即使服务器同意发送Cookie，浏览器也不会发送。或者，服务器要求设置Cookie，浏览器也不会处理。 但是，如果省略withCredentials设置，有的浏览器还是会一起发送Cookie。这时，可以显式关闭withCredentials。1xhr.withCredentials = false; 需要注意的是，如果要发送Cookie，Access-Control-Allow-Origin就不能设为星号，必须指定明确的、与请求网页一致的域名。同时，Cookie依然遵循同源政策，只有用服务器域名设置的Cookie才会上传，其他域名的Cookie并不会上传，且（跨源）原网页代码中的document.cookie也无法读取服务器域名下的Cookie。 四、非简单请求4.1 预检请求非简单请求是那种对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是application/json。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为”预检”请求（preflight）。 浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 下面是一段浏览器的JavaScript脚本。12345var url = 'http://api.alice.com/cors';var xhr = new XMLHttpRequest();xhr.open('PUT', url, true);xhr.setRequestHeader('X-Custom-Header', 'value');xhr.send(); 上面代码中，HTTP请求的方法是PUT，并且发送一个自定义头信息X-Custom-Header。 浏览器发现，这是一个非简单请求，就自动发出一个”预检”请求，要求服务器确认可以这样请求。下面是这个”预检”请求的HTTP头信息。12345678OPTIONS /cors HTTP/1.1Origin: http://api.bob.comAccess-Control-Request-Method: PUTAccess-Control-Request-Headers: X-Custom-HeaderHost: api.alice.comAccept-Language: en-USConnection: keep-aliveUser-Agent: Mozilla/5.0... “预检”请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。 除了Origin字段，”预检”请求的头信息包括两个特殊字段。 （1）Access-Control-Request-Method该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT。 （2）Access-Control-Request-Headers该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，上例是X-Custom-Header。 4.2 预检请求的回应服务器收到”预检”请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。123456789101112HTTP/1.1 200 OKDate: Mon, 01 Dec 2008 01:15:39 GMTServer: Apache/2.0.61 (Unix)Access-Control-Allow-Origin: http://api.bob.comAccess-Control-Allow-Methods: GET, POST, PUTAccess-Control-Allow-Headers: X-Custom-HeaderContent-Type: text/html; charset=utf-8Content-Encoding: gzipContent-Length: 0Keep-Alive: timeout=2, max=100Connection: Keep-AliveContent-Type: text/plain 上面的HTTP回应中，关键的是Access-Control-Allow-Origin字段，表示http://api.bob.com可以请求数据。该字段也可以设为星号，表示同意任意跨源请求。1Access-Control-Allow-Origin: * 如果浏览器否定了”预检”请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头信息字段。这时，浏览器就会认定，服务器不同意预检请求，因此触发一个错误，被XMLHttpRequest对象的onerror回调函数捕获。控制台会打印出如下的报错信息。12XMLHttpRequest cannot load http://api.alice.com.Origin http://api.bob.com is not allowed by Access-Control-Allow-Origin. 服务器回应的其他CORS相关字段如下。1234Access-Control-Allow-Methods: GET, POST, PUTAccess-Control-Allow-Headers: X-Custom-HeaderAccess-Control-Allow-Credentials: trueAccess-Control-Max-Age: 1728000 （1）Access-Control-Allow-Methods该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次”预检”请求。 （2）Access-Control-Allow-Headers如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在”预检”中请求的字段。 （3）Access-Control-Allow-Credentials该字段与简单请求时的含义相同。 （4）Access-Control-Max-Age该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。 4.3 浏览器的正常请求和回应一旦服务器通过了”预检”请求，以后每次浏览器正常的CORS请求，就都跟简单请求一样，会有一个Origin头信息字段。服务器的回应，也都会有一个Access-Control-Allow-Origin头信息字段。 下面是”预检”请求之后，浏览器的正常CORS请求。1234567PUT /cors HTTP/1.1Origin: http://api.bob.comHost: api.alice.comX-Custom-Header: valueAccept-Language: en-USConnection: keep-aliveUser-Agent: Mozilla/5.0... 上面头信息的Origin字段是浏览器自动添加的。 下面是服务器正常的回应。12Access-Control-Allow-Origin: http://api.bob.comContent-Type: text/html; charset=utf-8 上面头信息中，Access-Control-Allow-Origin字段是每次回应都必定包含的。 五、与JSONP的比较CORS与JSONP的使用目的相同，但是比JSONP更强大。 JSONP只支持GET请求，CORS支持所有类型的HTTP请求。JSONP的优势在于支持老式浏览器，以及可以向不支持CORS的网站请求数据。 作者： 阮一峰原文链接：http://www.ruanyifeng.com/blog/2016/04/cors.html","categories":[{"name":"Ajax","slug":"Ajax","permalink":"http://github.com/b2stry/categories/Ajax/"}],"tags":[{"name":"Ajax","slug":"Ajax","permalink":"http://github.com/b2stry/tags/Ajax/"}]},{"title":"利用JSONP实现跨域请求","slug":"利用JSONP实现跨域请求","date":"2017-11-21T03:10:45.000Z","updated":"2018-01-13T13:49:19.942Z","comments":true,"path":"2017/11/21/利用JSONP实现跨域请求/","link":"","permalink":"http://github.com/b2stry/2017/11/21/利用JSONP实现跨域请求/","excerpt":"JSONP在讲实现之前，我们先来看看何为JSONP。以下是维基百科的解释： JSONP or &quot;JSON with padding&quot; is a communication technique used in JavaScript programs running in web browsers to request data from a server in a different domain, something prohibited by typical web browsers because of the same-origin policy. JSONP takes advantage of the fact that browsers do not enforce the same-origin policy on &lt;script&gt; tags. Since it works through &lt;script&gt; tags, JSONP supports only the GET request method. There are significant security implications and risks associated to using JSONP; unless you have no choice, CORS is usually the better choice.","text":"JSONP在讲实现之前，我们先来看看何为JSONP。以下是维基百科的解释： JSONP or &quot;JSON with padding&quot; is a communication technique used in JavaScript programs running in web browsers to request data from a server in a different domain, something prohibited by typical web browsers because of the same-origin policy. JSONP takes advantage of the fact that browsers do not enforce the same-origin policy on &lt;script&gt; tags. Since it works through &lt;script&gt; tags, JSONP supports only the GET request method. There are significant security implications and risks associated to using JSONP; unless you have no choice, CORS is usually the better choice. 我粗陋翻译一下：JSONP又称JSON with padding，它是用在浏览器上运行的JS程序里的一项交互技术，目的是为了从不同的服务器域名上请求数据。由于同源政策的限制，部分功能会受到浏览器的禁止。JSONP利用的是浏览器不会对&lt;script&gt;标签实施同源政策的情况。又因为它是通过&lt;script&gt;作用的，所以JSONP只支持GET请求方式。但值得注意的是，使用JSONP会存在安全隐患和危险。CORS（跨域资源共享，Cross-Origin Resource Sharing）是一个更为常用的佳选，除非你没得选。 解释： 简单来说就是，一般情况下，在本域名的页面想要获取其他域名下的数据是会受到限制的。但是在HTML页面中的&lt;script&gt;(img,iframe亦可）就突破了这种限制，可以通过src属性来访问其他域名并获得返回的数据，但这种方法并不安全，只能通过GET方法获取。 其实我们平常稍加留意，就不难发现其实我们平常在页面中使用CDN也是这个原理，我们的页面同样可以访问其他服务器上的JS文件。 如我们常用的百度CDN，我们通常在&lt;head&gt;里面加上是&lt;script&gt;标签就可以使用其他服务器的CDN。 1&lt;script src=&quot;https://apps.bdimg.com/libs/jquery/2.1.4/jquery.js&quot;&gt;&lt;/script&gt; 只不过稍有不同的是现在变为向服务器运行文件请求数据，然后再以JavaScript的形式返回。 客户端实现： JSONP是一种非正式传输协议，具体传输内容以及格式可以由用户自己定义。不过我觉得JSONP可能会在ES6中新增也说不定哦，到时候可能会规范传输格式。JSONP其中有一个要点就是允许用户传递一个callback参数给服务端，然后服务端输出的数据作为callback的参数再一并返回到客户端页面，最后根据参数在客户端页面执行这个回调函数，最终达到返回数据并处理的目的。 代码形式如下： 12345678910 &lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;JSONP&lt;/title&gt; &lt;script type=\"text/javscript\"&gt; var cbFn = function(str)&#123; alert(str); &#125; &lt;/script&gt; &lt;script src=\"https://www.sp0.baidu.com?data=value&amp;cb=cbFn\"&gt;&lt;/script&gt;10 &lt;/head&gt; 首先要预定义一个回调函数，用于处理返回数据；其次请求数据的&lt;script&gt;标签的src要传递请求参数以及回调的函数名字到后台，这一点要沟通协调好。 由于在实际应用中请求地址、请求参数以及次数都是不固定的，所以我们要把JSONP封装成一个函数，增强灵活性和复用性。 1234567891011121314151617var s1;function jsonp(url,data,cb)&#123; data.cb = cb?cb:\"callback\"; data.t = new Date().getTime(); for (var i in data)&#123; var str = i+\"=\"+data[i]; arr.push(str); &#125; var str = url+\"?\"+arr.join(\"&amp;\"); var headEl = document.getElementsByTagName(\"head\")[0]; if (s1)&#123; headEl.removeChild(s1); &#125; s1 = document.createElement(\"script\"); s1.src = str; headEl.appendChild(s1); &#125; 代码解释：首先给传递的数据中插入一个回调函数名和一个新的时间参数值，让后台获取你要回调的函数名以及防止调用缓存。接下来就是判断之前已经是否发送过JSONP请求，如果存在，则删除重建。 服务端实现： 假设我在客户端执行了以下请求12345678jsonp(\"https://www.chengguanhui.com/test.php\",&#123; name:\"ray\", age:23&#125;,\"cbFn\");cbFn(str)&#123; alert(str);&#125; 那么HTTP传递的URL为https://www.chengguanhui.com/test.php?name=ray&amp;age=23cb=cbFn&amp;t=242566(某个不定时间值) PHP代码： 12345678&lt;?php require_once(\"common.php\"); $name = $_GET['name']; $age = $_GET['age']; $cb = $_GET['cb']; $str = $name.$age; echo $cb.\"(\".$str.\")\";?&gt; 客户端接收到cbFn(&quot;ray23&quot;)并弹出对应内容。 JQ实现： $.ajax(url,[settings]) 123456789101112131415$.ajax(&#123; type: \"get\", url: \"https://www.chengguanhui.com/test.php\", dataType: \"jsonp\", data: \"name=ray&amp;age=23\", jsonp: \"callback\",//传递给请求处理程序或页面的，用以获得jsonp回调函数名的参数名(一般默认为:callback) jsonpCallback:\"handleFn\",//自定义的jsonp回调函数名称，默认为jQuery自动生成的随机函数名，也可以写\"?\"，jQuery会自动为你处理数据 success: function(json)&#123; alert(json.name+json.age); &#125;, error: function()&#123; alert('fail'); &#125; &#125;); &#125;); $.getJSON(url,data,callback) 12345678910$.getJSON(\"https://www.chengguanhui.com/test.php\",//如果请求值固定时，可以省略data参数而直接写在url参数里。 &#123; name:ray, age:23&#125;,function(data)&#123; $.each(data.items, function(i,item)&#123; alert(i+item); &#125;);&#125;);","categories":[{"name":"Ajax","slug":"Ajax","permalink":"http://github.com/b2stry/categories/Ajax/"}],"tags":[{"name":"Ajax","slug":"Ajax","permalink":"http://github.com/b2stry/tags/Ajax/"}]},{"title":"LeetCode之 4Sum II(454)","slug":"LeetCode之-4Sum-II-454","date":"2017-11-20T08:55:59.000Z","updated":"2018-01-13T06:36:04.085Z","comments":true,"path":"2017/11/20/LeetCode之-4Sum-II-454/","link":"","permalink":"http://github.com/b2stry/2017/11/20/LeetCode之-4Sum-II-454/","excerpt":"QuestionGiven four lists A, B, C, D of integer values, compute how many tuples (i, j, k, l) there are such that A[i] + B[j] + C[k] + D[l] is zero. To make problem a bit easier, all A, B, C, D have same length of N where 0 ≤ N ≤ 500. All integers are in the range of -228 to 228 - 1 and the result is guaranteed to be at most 231 - 1. 给出4个整形数组A,B,C,D，寻找有多少i,j,k,l的组合，使得A[i]+B[j]+C[k]+D[l] == 0。其中，ABCD中均含有相同的元素个数N，且0&lt;=N&lt;=500。","text":"QuestionGiven four lists A, B, C, D of integer values, compute how many tuples (i, j, k, l) there are such that A[i] + B[j] + C[k] + D[l] is zero. To make problem a bit easier, all A, B, C, D have same length of N where 0 ≤ N ≤ 500. All integers are in the range of -228 to 228 - 1 and the result is guaranteed to be at most 231 - 1. 给出4个整形数组A,B,C,D，寻找有多少i,j,k,l的组合，使得A[i]+B[j]+C[k]+D[l] == 0。其中，ABCD中均含有相同的元素个数N，且0&lt;=N&lt;=500。Example: 12345678910111213Input:A = [ 1, 2]B = [-2,-1]C = [-1, 2]D = [ 0, 2]Output:2Explanation:The two tuples are:1. (0, 0, 0, 1) -&gt; A[0] + B[0] + C[0] + D[1] = 1 + (-2) + (-1) + 2 = 02. (1, 1, 0, 0) -&gt; A[1] + B[1] + C[0] + D[0] = 2 + (-1) + (-1) + 0 = 0 Solution 数据规模的概念：如果想要在1S内解决问题：O(n^2)的算法可以处理大约10^4级别的数据；O(nlogn)的算法可以处理大约10^7级别的数据；O(n)的算法可以处理大约10^8级别的数据。 这个问题非常直接的告诉了我们N的取值范围，我们就可以根据这个数据规模的取值范围推算出大概需要一个什么时间复杂度的算法。 下面我们来分析一下这个问题，首先，最直观的方法是暴力解法，用4重循环分别在ABCD中遍历每一种选择的可能性，那么我们可以设想一下，由于N的取值最多为500，这样做它的时间复杂度是O(n^4)的级别，50^4 = 625,0000,0000,根据上面的数据规模的概念，一般的计算机肯定是承受不了的; 可以使用哈希表来将D中的元素全部放入一个map集合中，这样我们只需要遍历ABC这三个数组，然后在map中只需要查找0-A[i]-B[j]-C[k]这个元素是否存在，这样时间复杂度就优化到了O(n^3)的级别 500^3 = 1,2500,000，一般的计算机也是承受不了的; 由于N=500，一个O(n^2)的算法是可以承受的,可以使用哈希表来将C+D的每一种可能全部放入一个map集合中，然后只需遍历AB两个数组，然后然后在map中只需要查找`0-A[i]-B[j]这个元素是否存在就可以了。 CodeC++:123456789101112131415161718192021222324class Solution &#123;public: int fourSumCount(vector&lt;int&gt;&amp; A, vector&lt;int&gt;&amp; B, vector&lt;int&gt;&amp; C, vector&lt;int&gt;&amp; D) &#123; unordered_map&lt;int, int&gt; record; for (int i = 0; i &lt; C.size(); ++i) &#123; for (int j = 0; j &lt; D.size(); ++j) &#123; record[C[i] + D[j]]++; &#125; &#125; int res = 0; for (int i = 0; i &lt; A.size(); ++i) &#123; for (int j = 0; j &lt; B.size(); ++j) &#123; if (record.find(0 - A[i] - B[j]) != record.end()) res += record[0 - A[i] - B[j]]; &#125; &#125; return res; &#125;&#125;; Java:1234567891011121314151617181920212223242526class Solution &#123; public int fourSumCount(int[] A, int[] B, int[] C, int[] D) &#123; Map&lt;Integer, Integer&gt; record = new HashMap(); int n = 0; for (int i = 0; i &lt; C.length; i++) &#123; for (int j = 0; j &lt; D.length; j++) &#123; if (record.containsKey(C[i] + D[j])) &#123; n = record.get(C[i] + D[j]) + 1; &#125; else &#123; n = 1; &#125; record.put(C[i] + D[j], n); &#125; &#125; int res = 0; for (int i = 0; i &lt; A.length; i++) &#123; for (int j = 0; j &lt; B.length; j++) &#123; if (record.containsKey(0 - A[i] - B[j])) &#123; res += record.get(0 - A[i] - B[j]); &#125; &#125; &#125; return res; &#125;&#125; 最后，将A+B和C+D的每一种可能放到两个map中，时间复杂度依然是O(n^2)。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://github.com/b2stry/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://github.com/b2stry/tags/算法/"}]},{"title":"LeetCode之Two Sum(1)","slug":"LeetCode之Two-Sum","date":"2017-11-17T08:43:20.000Z","updated":"2018-01-13T06:36:01.824Z","comments":true,"path":"2017/11/17/LeetCode之Two-Sum/","link":"","permalink":"http://github.com/b2stry/2017/11/17/LeetCode之Two-Sum/","excerpt":"QuestionGiven an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: 1234Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1].","text":"QuestionGiven an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: 1234Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. SolutionApproach #1 (Brute Force) [Accepted]The brute force approach is simple. Loop through each element xx and find if there is another value that equals to target - x.12345678910public int[] twoSum(int[] nums, int target) &#123; for (int i = 0; i &lt; nums.length; i++) &#123; for (int j = i + 1; j &lt; nums.length; j++) &#123; if (nums[j] == target - nums[i]) &#123; return new int[] &#123; i, j &#125;; &#125; &#125; &#125; throw new IllegalArgumentException(\"No two sum solution\");&#125; Complexity Analysis Time complexity : O(n^2) . For each element, we try to find its complement by looping through the rest of array which takes O(n) time. Therefore, the time complexity is O(n^2). Space complexity : O(1). Approach #2 (Two-pass Hash Table) [Accepted]To improve our run time complexity, we need a more efficient way to check if the complement exists in the array. If the complement exists, we need to look up its index. What is the best way to maintain a mapping of each element in the array to its index? A hash table. We reduce the look up time fromO(n)to O(1) by trading space for speed. A hash table is built exactly for this purpose, it supports fast look up in near constant time. I say “near” because if a collision occurred, a look up could degenerate to O(n) time. But look up in hash table should be amortized O(1) time as long as the hash function was chosen carefully. A simple implementation uses two iterations. In the first iteration, we add each element’s value and its index to the table. Then, in the second iteration we check if each element’s complement (target - nums[i]) exists in the table. Beware that the complement must not be nums[i] itself! 12345678910111213public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; map.put(nums[i], i); &#125; for (int i = 0; i &lt; nums.length; i++) &#123; int complement = target - nums[i]; if (map.containsKey(complement) &amp;&amp; map.get(complement) != i) &#123; return new int[] &#123; i, map.get(complement) &#125;; &#125; &#125; throw new IllegalArgumentException(\"No two sum solution\");&#125; Complexity Analysis: Time complexity : O(n). We traverse the list containing nn elements exactly twice. Since the hash table reduces the look up time to O(1), the time complexity is O(n). Space complexity : O(n). The extra space required depends on the number of items stored in the hash table, which stores exactly nn elements. Approach #3 (One-pass Hash Table) [Accepted]It turns out we can do it in one-pass. While we iterate and inserting elements into the table, we also look back to check if current element’s complement already exists in the table. If it exists, we have found a solution and return immediately. 1234567891011public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; int complement = target - nums[i]; if (map.containsKey(complement)) &#123; return new int[] &#123; map.get(complement), i &#125;; &#125; map.put(nums[i], i); &#125; throw new IllegalArgumentException(\"No two sum solution\");&#125; Complexity Analysis: Time complexity : O(n). We traverse the list containing nn elements only once. Each look up in the table costs only O(1) time. Space complexity : O(n). The extra space required depends on the number of items stored in the hash table, which stores at most nn elements.","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://github.com/b2stry/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://github.com/b2stry/tags/算法/"}]},{"title":"Git和Vim的基本操作命令","slug":"Git和Vim的基本操作命令","date":"2017-11-05T10:38:02.000Z","updated":"2018-01-13T12:51:01.395Z","comments":true,"path":"2017/11/05/Git和Vim的基本操作命令/","link":"","permalink":"http://github.com/b2stry/2017/11/05/Git和Vim的基本操作命令/","excerpt":"Git和Vim的常用命令速查。","text":"Git和Vim的常用命令速查。 Git命令速查 Vim命令","categories":[{"name":"command","slug":"command","permalink":"http://github.com/b2stry/categories/command/"}],"tags":[{"name":"git","slug":"git","permalink":"http://github.com/b2stry/tags/git/"},{"name":"vim","slug":"vim","permalink":"http://github.com/b2stry/tags/vim/"}]},{"title":"常用shell脚本","slug":"一些常用的shell脚本","date":"2017-11-04T10:15:31.000Z","updated":"2018-01-13T06:58:09.730Z","comments":true,"path":"2017/11/04/一些常用的shell脚本/","link":"","permalink":"http://github.com/b2stry/2017/11/04/一些常用的shell脚本/","excerpt":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们在运维中，尤其是linux运维，都知道脚本的重要性，脚本会让我们的 运维事半功倍，所以学会写脚本是每个linux运维必须学会的一门功课，这里收藏linux运维常用的脚本。如何学好脚本，最关键的是就是大量的练习 和实践。根据以下脚本我们可以拓展，这样我们提高的很快！举一反三！","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们在运维中，尤其是linux运维，都知道脚本的重要性，脚本会让我们的 运维事半功倍，所以学会写脚本是每个linux运维必须学会的一门功课，这里收藏linux运维常用的脚本。如何学好脚本，最关键的是就是大量的练习 和实践。根据以下脚本我们可以拓展，这样我们提高的很快！举一反三！1．用Shell编程，判断一文件是不是字符设备文件，如果是将其拷贝到 /dev 目录下。12345678#!/bin/shFILENAME=echo “Input file name：”read FILENAMEif [ -c \"$FILENAME\" ]thencp $FILENAME /devfi 2．设计一个shell程序，添加一个新组为class1，然后添加属于这个组的30个用户，用户名的形式为stdxx，其中xx从01到30。12345678910111213141516#!/bin/shi=1groupadd class1while [ $i -le 30 ]doif [ $i -le 9 ] ;thenUSERNAME=stu0$&#123;i&#125;elseUSERNAME=stu$&#123;i&#125;fiuseradd $USERNAMEmkdir /home/$USERNAMEchown -R $USERNAME /home/$USERNAMEchgrp -R class1 /home/$USERNAMEi=$(($i+1))done 3．编写shell程序，实现自动删除50个账号的功能。账号名为stud1至stud50。1234567#!/bin/shi=1while [ $i -le 50 ]douserdel -r stud$&#123;i&#125;i=$(($i+1 ))done 4．某系统管理员需每天做一定的重复工作，请按照下列要求，编制一个解决方案： 1）在下午4 :50删除/abc目录下的全部子目录和全部文件； 2）从早8:00～下午6:00每小时读取/xyz目录下x1文件中每行第一个域的全部数据加入到/backup目录下的bak01.txt文件内； 3）每逢星期一下午5:50将/data目录下的所有目录和文件归档并压缩为文件：backup.tar.gz； 4）在下午5:55将IDE接口的CD-ROM卸载（假设：CD-ROM的设备名为hdc）； 5）在早晨8:00前开机后启动。 解决方案： （1）用vi创建编辑一个名为prgx的crontab文件； （2）prgx文件的内容：123450 16 * * * rm -r /abc/*0 8-18/1 * * * cut -f1 /xyz/x1 &gt;;&gt;; /backup/bak01.txt50 17 * * * tar zcvf backup.tar.gz /data55 17 * * * umount /dev/hdc （3）由超级用户登录，用crontab执行 prgx文件中的内容：root@xxx:#crontab prgx；在每日早晨8:00之前开机后即可自动启动crontab。 5.设计一个shell程序，在每月第一天备份并压缩/etc目录的所有内容，存放在/root/bak目录里，且文件名为如下形式yymmdd_etc，yy为年，mm为月，dd为日。Shell程序fileback存放在/usr/bin目录下。 （1）编写shell程序fileback：123456789101112#!/bin/shDIRNAME=`ls /root | grep bak`if [ -z \"$DIRNAME\" ] ; thenmkdir /root/bakcd /root/bakfiYY=`date +%y`MM=`date +%m`DD=`date +%d`BACKETC=$YY$MM$DD_etc.tar.gztar zcvf $BACKETC /etcecho “fileback finished!” （2）编写任务定时器：1234echo “0 0 1 * * /bin/sh /usr/bin/fileback” &gt;; /root/etcbakcroncrontab /root/etcbakcron或使用crontab -e 命令添加定时任务：0 1 * * * /bin/sh /usr/bin/fileback 6．有一普通用户想在每周日凌晨零点零分定期备份/user/backup到/tmp目录下，该用户应如何做？ （1）第一种方法： 12用户应使用crontab –e 命令创建crontab文件。格式如下：0 0 * * sun cp –r /user/backup /tmp （2）第二种方法： 用户先在自己目录下新建文件file，文件内容如下： 10 * * sun cp –r /user/backup /tmp 7.设计一个Shell程序，在/userdata目录下建立50个目录，即user1～user50，并设置每个目录的权限，其中其他用户的权限为：读；文件所有者的权限为：读、写、执行；文件所有者所在组的权限为：读、执行。 1234567891011121314151617#!/bin/shi=1while [ i -le 50 ]doif [ -d /userdata ];thenmkdir -p /userdata/user$ichmod 754 /userdata/user$iecho “user$i”let “i = i + 1″ （或i=$（（$i＋1））elsemkdir /userdatamkdir -p /userdata/user$ichmod 754 /userdata/user$iecho “user$i”let “i = i + 1″ （或i=$（（$i＋1））fidone 8、mysql备份实例，自动备份mysql，并删除30天前的备份文件 12345678910111213141516171819202122232425262728293031323334#!/bin/sh#auto backup mysql#wugk 2012-07-14#PATH DEFINEBAKDIR=/data/backup/mysql/`date +%Y-%m-%d`MYSQLDB=wwwMYSQLPW=backupMYSQLUSR=backupif[ $UID -ne 0 ];thenecho This script must use administrator or root user ,please exit!sleep 2exit 0fiif[ ! -d $BAKDIR ];thenmkdir -p $BAKDIRelseecho This is $BAKDIR exists ,please exit ….sleep 2exitfi###mysqldump backup mysql/usr/bin/mysqldump -u$MYSQLUSR -p$MYSQLPW -d $MYSQLDB &gt;/data/backup/mysql/`date +%Y-%m-%d`/www_db.sqlcd $BAKDIR ; tar -czf www_mysql_db.tar.gz *.sqlcd $BAKDIR ;find . -name “*.sql” |xargs rm -rf[ $? -eq 0 ]&amp;&amp;echo “This `date +%Y-%m-%d` RESIN BACKUP is SUCCESS”cd /data/backup/mysql/ ;find . -mtime +30 |xargs rm -rf 9、自动安装Nginx脚本，采用case方式，选择方式，也可以根据实际需求改成自己想要的脚本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#!/bin/sh###nginx install shell###wugk 2012-07-14###PATH DEFINESOFT_PATH=/data/soft/NGINX_FILE=nginx-1.2.0.tar.gzDOWN_PATH=http://nginx.org/download/if[ $UID -ne 0 ];thenecho This script must use administrator or root user ,please exit!sleep 2exit 0fiif[ ! -d $SOFT_PATH ];thenmkdir -p $SOFT_PATHfidownload ()&#123;cd $SOFT_PATH ;wget $DOWN_PATH/$NGINX_FILE&#125;install ()&#123;yum install pcre-devel -ycd $SOFT_PATH ;tar xzf $NGINX_FILE ;cd nginx-1.2.0/ &amp;&amp;./configure –prefix=/usr/local/nginx/ –with-http_stub_status_module –with-http_ssl_module[ $? -eq 0 ]&amp;&amp;make &amp;&amp;make install&#125;start ()&#123;lsof -i :80[ $? -ne 0 ]&amp;&amp;/usr/local/nginx/sbin/nginx&#125;stop ()&#123;ps -ef |grep nginx |grep -v grep |awk ‘&#123;print $2&#125;’|xargs kill -9&#125;exit ()&#123;echo $? ;exit&#125;###case menu #####case $1 indownload )download;;install )install;;start )start;;stop )stop;;* )echo “USAGE:$0 &#123;download or install or start or stop&#125;”exitesac 10、批量解压tar脚本，批量解压zip并且建立当前目录。1234567#!/bin/shPATH1=/tmp/imagesPATH2=/usr/www/imagesfor i in `ls $&#123;PATH1&#125;/*`dotar xvf $i -C $PATH2done 这个脚本是针对所有tar文件在一个目录，但是实际情况中，有可能在下级或者更深的目录，我们可以使用find查找1234567#!/bin/shPATH1=/tmp/imagesPATH2=/usr/www/imagesfor i in `find $PATH1 -name ”*.tar” `dotar xvf $i -C $PATH2done 如何是zip文件，例如123189.zip 132342.zip 等等批量文件，默认unzip直接解压不带自身目录，意思是解压123189.zip完当前目录就是图片，不能创建123189目录下并解压，可以用shell脚本实现123456789101112#!/bin/shPATH1=/tmp/imagesPATH2=/usr/www/imagescd $PATH1for i in `find . -name ”*.zip”|awk -F. &#123;print $2&#125; `domkdir -p PATH2$iunzip -o .$i.zip -d PATH2$idone","categories":[{"name":"Shell","slug":"Shell","permalink":"http://github.com/b2stry/categories/Shell/"}],"tags":[{"name":"shell","slug":"shell","permalink":"http://github.com/b2stry/tags/shell/"}]},{"title":"常用正则表达式速查","slug":"正则表达式速查","date":"2017-11-03T06:59:41.000Z","updated":"2018-01-13T12:50:35.932Z","comments":true,"path":"2017/11/03/正则表达式速查/","link":"","permalink":"http://github.com/b2stry/2017/11/03/正则表达式速查/","excerpt":"常用正则。","text":"常用正则。 用户名： /^[a-z0-9_-]{3,16}$/ 密码： /^[a-z0-9_-]{6,18}$/ 密码2： (?=^.{8,}$)(?=.*\\d)(?=.*\\W+)(?=.*[A-Z])(?=.*[a-z])(?!.*\\n).*$ (由数字/大写字母/小写字母/标点符号组成，四种都必有，8位以上) 十六进制值： /^#?([a-f0-9]{6}|[a-f0-9]{3})$/ 电子邮箱： /^([a-z0-9_\\.-]+)@([\\da-z\\.-]+)\\.([a-z\\.]{2,6})$/ /^[a-z\\d]+(\\.[a-z\\d]+)*@([\\da-z](-[\\da-z])?)+(\\.{1,2}[a-z]+)+$/或\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)* URL： /^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$/ 或[a-zA-z]+://[^\\s]* IP 地址： /((2[0-4]\\d|25[0-5]|[01]?\\d\\d?)\\.){3}(2[0-4]\\d|25[0-5]|[01]?\\d\\d?)/ /^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$/ 或 ((2[0-4]\\d|25[0-5]|[01]?\\d\\d?)\\.){3}(2[0-4]\\d|25[0-5]|[01]?\\d\\d?) HTML 标签： /^&lt;([a-z]+)([^&lt;]+)*(?:&gt;(.*)&lt;\\/\\1&gt;|\\s+\\/&gt;)$/ 或&lt;(.*)(.*)&gt;.*&lt;\\/\\1&gt;|&lt;(.*) \\/&gt; 删除代码\\注释： (?&lt;!http:|\\S)//.*$ 匹配双字节字符(包括汉字在内)： [^\\x00-\\xff] 汉字(字符)： [\\u4e00-\\u9fa5] Unicode编码中的汉字范围： /^[\\u2E80-\\u9FFF]+$/ 中文及全角标点符号(字符)： [\\u3000-\\u301e\\ufe10-\\ufe19\\ufe30-\\ufe44\\ufe50-\\ufe6b\\uff01-\\uffee] 日期(年-月-日)： (\\d{4}|\\d{2})-((0?([1-9]))|(1[1|2]))-((0?[1-9])|([12]([1-9]))|(3[0|1])) 日期(月/日/年)： ((0?[1-9]{1})|(1[1|2]))/(0?[1-9]|([12][1-9])|(3[0|1]))/(\\d{4}|\\d{2}) 时间(小时:分钟, 24小时制)： ((1|0?)[0-9]|2[0-3]):([0-5][0-9]) 中国大陆固定电话号码： (\\d{4}-|\\d{3}-)?(\\d{8}|\\d{7}) 中国大陆手机号码： 1\\d{10} 中国大陆邮政编码： [1-9]\\d{5} 中国大陆身份证号(15位或18位)： \\d{15}(\\d\\d[0-9xX])? 非负整数(正整数或零)： \\d+ 正整数： [0-9]*[1-9][0-9]* 负整数： -[0-9]*[1-9][0-9]* 整数： -?\\d+ 小数： (-?\\d+)(\\.\\d+)? 空白行： \\n\\s*\\r 或者 \\n\\n(editplus) 或者 ^[\\s\\S ]*\\n QQ号码： [1-9]\\d{4,} 不包含abc的单词： \\b((?!abc)\\w)+\\b 匹配首尾空白字符： ^\\s*|\\s*$ 编辑常用以下是针对特殊中文的一些替换(editplus)^[0-9].*\\n ^[^第].*\\n ^[习题].*\\n ^[\\s\\S ]*\\n ^[0-9]*\\. ^[\\s\\S ]*\\n &lt;p[^&lt;&gt;*]&gt; href=&quot;javascript:if\\(confirm\\(&#39;(.*?)&#39;\\)\\)window\\.location=&#39;(.*?)&#39;&quot; &lt;span style=&quot;.[^&quot;]*rgb\\(255,255,255\\)&quot;&gt;.[^&lt;&gt;]*&lt;/span&gt; &lt;DIV class=xs0&gt;[\\s\\S]*?&lt;/DIV&gt;","categories":[{"name":"regex","slug":"regex","permalink":"http://github.com/b2stry/categories/regex/"}],"tags":[{"name":"正则表达式","slug":"正则表达式","permalink":"http://github.com/b2stry/tags/正则表达式/"}]},{"title":"一些对面试很用帮助的文章分享","slug":"一些对面试很用帮助的文章","date":"2017-11-02T10:32:44.000Z","updated":"2018-02-03T07:33:51.146Z","comments":true,"path":"2017/11/02/一些对面试很用帮助的文章/","link":"","permalink":"http://github.com/b2stry/2017/11/02/一些对面试很用帮助的文章/","excerpt":"下面分享一些在面试中关于电商项目很有用的知识点的文章的链接，文章作者的博客：http://my.csdn.net/lihang_1994 淘淘商城面试问题:—-可能会被问到的问题：http://blog.csdn.net/lihang_1994/article/details/77839350 电商项目介绍—说的很好：http://blog.csdn.net/lihang_1994/article/details/74620538 淘淘商城业务–加油：http://blog.csdn.net/lihang_1994/article/details/74353154 电商秒杀抢购：http://blog.csdn.net/lihang_1994/article/details/72589136 面试:—-电商项目中比较难得问题：http://blog.csdn.net/lihang_1994/article/details/72886071 电商项目总结：http://blog.csdn.net/lihang_1994/article/details/72588520","text":"下面分享一些在面试中关于电商项目很有用的知识点的文章的链接，文章作者的博客：http://my.csdn.net/lihang_1994 淘淘商城面试问题:—-可能会被问到的问题：http://blog.csdn.net/lihang_1994/article/details/77839350 电商项目介绍—说的很好：http://blog.csdn.net/lihang_1994/article/details/74620538 淘淘商城业务–加油：http://blog.csdn.net/lihang_1994/article/details/74353154 电商秒杀抢购：http://blog.csdn.net/lihang_1994/article/details/72589136 面试:—-电商项目中比较难得问题：http://blog.csdn.net/lihang_1994/article/details/72886071 电商项目总结：http://blog.csdn.net/lihang_1994/article/details/72588520 redis基础总结：http://blog.csdn.net/lihang_1994/article/details/74356204 对spring的理解：http://blog.csdn.net/lihang_1994/article/details/72597979 怎么理解的:—-mybatis：http://blog.csdn.net/lihang_1994/article/details/73176216 面试:—-Spring MVC 文件上传下载：http://blog.csdn.net/lihang_1994/article/details/72598440 面试:—-springmvc常用注解标签详解：http://blog.csdn.net/lihang_1994/article/details/72625601 面试:—-Struts2的工作原理及工作流程：http://blog.csdn.net/lihang_1994/article/details/72625585 面试:—-技术分析之Struts2的拦截器技术：http://blog.csdn.net/lihang_1994/article/details/72598059 面试：—-Struts2实现文件上传和下载：http://blog.csdn.net/lihang_1994/article/details/72598404 面试:—-Struts和springmvc的区别–区别上：http://blog.csdn.net/lihang_1994/article/details/72598008 面试:—-Struts和springmvc的区别–区别下：http://blog.csdn.net/lihang_1994/article/details/72598019 面试:—-会话技术/session,cookie：http://blog.csdn.net/lihang_1994/article/details/72598324 面试:—-Spring常用注解：http://blog.csdn.net/lihang_1994/article/details/72764609 面试:—Sql语句练习：http://blog.csdn.net/lihang_1994/article/details/72802532 面试:—-Hibernate工作原理及为什么要用?：http://blog.csdn.net/lihang_1994/article/details/72598166 面试:—–Hibernate和mybatis的区别面试中：http://blog.csdn.net/lihang_1994/article/details/72597997 面试:—-利用solr实现商品的搜索功能：http://blog.csdn.net/lihang_1994/article/details/72599449 面试:—-Nginx的一理解：http://blog.csdn.net/lihang_1994/article/details/72598955 面试:—-Java中使用Jedis操作Redis：http://blog.csdn.net/lihang_1994/article/details/72598493 实际工作:—-Redis之——Spring基于注解整合Redis：http://blog.csdn.net/lihang_1994/article/details/72598592 实际工作中:—-mybatis长用的mapper.xml：http://blog.csdn.net/lihang_1994/article/details/73189283 实际工作:—-Poi报表导入导出：http://blog.csdn.net/lihang_1994/article/details/72647083 实际工作中:—-FastDFS在项目中的应用：http://blog.csdn.net/lihang_1994/article/details/72598894 实际工作中:—-dubbo+zookeeper实现服务远程调用：http://blog.csdn.net/lihang_1994/article/details/72598765","categories":[{"name":"interview","slug":"interview","permalink":"http://github.com/b2stry/categories/interview/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://github.com/b2stry/tags/面试/"}]},{"title":"【Java】@SuppressWarnings注解","slug":"【Java】-SuppressWarnings注解","date":"2017-10-31T13:49:47.000Z","updated":"2018-02-03T07:33:05.471Z","comments":true,"path":"2017/10/31/【Java】-SuppressWarnings注解/","link":"","permalink":"http://github.com/b2stry/2017/10/31/【Java】-SuppressWarnings注解/","excerpt":"java.lang.SuppressWarnings是J2SE 5.0中标准的Annotation之一。可以标注在类、字段、方法、参数、构造方法，以及局部变量上。 作用：告诉编译器忽略指定的警告，不用在编译完成后出现警告信息。","text":"java.lang.SuppressWarnings是J2SE 5.0中标准的Annotation之一。可以标注在类、字段、方法、参数、构造方法，以及局部变量上。 作用：告诉编译器忽略指定的警告，不用在编译完成后出现警告信息。使用： @SuppressWarnings(“”) @SuppressWarnings({}) @SuppressWarnings(value={}) 根据sun的官方文档描述： value - 将由编译器在注释的元素中取消显示的警告集。允许使用重复的名称。忽略第二个和后面出现的名称。出现未被识别的警告名不是 错误：编译器必须忽略无法识别的所有警告名。但如果某个注释包含未被识别的警告名，那么编译器可以随意发出一个警告。 各编译器供应商应该将它们所支持的警告名连同注释类型一起记录。鼓励各供应商之间相互合作，确保在多个编译器中使用相同的名称。 示例： @SuppressWarnings(‘unchecked’) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;告诉编译器忽略 unchecked 警告信息，如使用List，ArrayList等未进行参数化产生的警告信息。 @SuppressWarnings(‘serial’) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果编译器出现这样的警告信息：The serializable class WmailCalendar does not declare a static final serialVersionUID field of type long使用这个注释将警告信息去掉。 @SuppressWarnings(‘deprecation’) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果使用了使用@Deprecated注释的方法，编译器将出现警告信息。使用这个注释将警告信息去掉。 @SuppressWarnings(‘unchecked’, ‘deprecation’) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;告诉编译器同时忽略unchecked和deprecation的警告信息。 @SuppressWarnings(value={‘unchecked’, ‘deprecation’}) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;等同于@SuppressWarnings(‘unchecked’, ‘deprecation’)","categories":[{"name":"Java","slug":"Java","permalink":"http://github.com/b2stry/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://github.com/b2stry/tags/java/"}]},{"title":"微信公众号支付开发全过程--Java","slug":"【转载】微信公众号支付开发全过程-Java","date":"2017-10-30T14:04:37.000Z","updated":"2018-02-03T07:32:55.980Z","comments":true,"path":"2017/10/30/【转载】微信公众号支付开发全过程-Java/","link":"","permalink":"http://github.com/b2stry/2017/10/30/【转载】微信公众号支付开发全过程-Java/","excerpt":"Java集成微信支付指南。 转载自：http://www.cnblogs.com/yimiyan/p/5603657.html","text":"Java集成微信支付指南。 转载自：http://www.cnblogs.com/yimiyan/p/5603657.html业务流程 这个微信官网说的还是很详细的，还配了图。我还要再说一遍。 用户点击一个支付按钮–&gt;{后台一大推处理}–&gt;用户看到了一个输入密码的界面，包含金额等一些信息–&gt;用户输入密码后出来一个支付成功的页面（这部分流程都是微信自己完成的，我们什么都不用做）–&gt;返回系统自己的页面（总不能让用户一直看着一个支付完成的页面吧。花了钱，正心疼的，赶紧跳转啊~一会后悔了，申请退款怎么整。可怜的工程师还得开发退款功能） 开发流程 1）获取用户授权(这个做不做没有啥关系，反正我还没做呢)2）调用统一下单接口获取预支付id3）H5调起微信支付的内置JS4）支付完成后，微信回调URL的处理 看着大段的文字，是不是很不爽。忘记了在哪里看到的一句话。One picture instead thousands of words. 本文最主要的部分开始了(想直接看代码，贴上代码，你也不一定能看懂，不是说代码难，各种分离，各种类，不直接。看懂了，不一定能调试通。最后一个签名错误，或者$get_brand_wcpay_request:fail.$key0 还是得回来乖乖的对参数。)最近学习英语有点魔怔了。各种插入语。不懂什么梗的，可略过。朋友一直说我的笑点和别人不一样。文后会讲个笑话。1、生成统一下单接口，获取prepay_id.需要的参数 ==名称==从哪里找到他们：微信官方给了个参数的详细说明。https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=9_1 部分参数，仍然不知道哪里找的小伙伴们请继续向下看appid ==应用ID==登陆微信公众号后台-开发-基本配置mch_id == 微信支付商户号==登陆微信支付后台，即可看到device_info==设备号==终端设备号(门店号或收银设备ID)，注意：PC网页或公众号内支付请传”WEB”body==商品描述==商品或支付单简要描述（不知道是什么鬼，没关系，先随便传个字符串，随便的传个英文的字符串。你会为你这个时候的英明决定打个满分。如果是中文，可能会遇到毫无头绪的签名错误，严重者开始怀疑人生）trade_type==交易类型==取值如下：JSAPI，NATIVE，APP。我们这里使用的JSAPI。标题已经说了，是微信公众号支付。他们的区别，请参考https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=4_2 ps：JSAPI–公众号支付、NATIVE–原生扫码支付、APP–app支付，统一下单接口trade_type的传参可参考这里。MICROPAY–刷卡支付，刷卡支付有单独的支付接口，不调用统一下单接口nonce_str==随机字符串==随机字符串，不长于32位（参考算法https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=4_3） ps:小伙伴们可能会对nonce这个命名，很诧异，微信team的人，都是逗必吗~。查了一下百科，随机数也用nonce表示。瞬间伤害满满的。（好奇的宝宝可用剑桥词典查一下nonce的意思）。在我的不懈努力下， 发现了这个。nonce ==number used once.恍然大悟的赶脚。 notify_url==通知地址==接收微信支付异步通知回调地址，通知url必须为直接可访问的url，不能携带参数。（这，起个什么名字好呢。随便起吧，反正一时半会也用不到）out_trade_no==商户订单号==商户系统内部的订单号,32个字符内、可包含字母（参考：https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=4_2）（每次看完微信的官方解释就更迷茫了，有木有。没关系，我就传个1咋了。）total_fee==总金额==订单总金额，单位为分（为了公司的项目测试，还得自己掏银子，1分钱也是钱啊。ps:这个时候总会想起，一个同学说过，苍蝇腿也是肉啊）openid==用户标识==trade_type=JSAPI，此参数必传，用户在商户appid下的唯一标识。（要是不知道这个从哪里来的话，没关系。微信不是给咱写文档了吗https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=4_4）还有最最重要的一个，重要的角色总要在最后登场。sign==签名==官方给的签名算法。https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=4_3。没有看懂，看不太懂，你觉得你看懂了，没关系，不遇到几次签名错误，好意思说自己做过微信支付开发吗。说道这个sign还有一个更重要的参数。参与签名的参数。反正我是找了好久才找到。（公司运营申请的微信支付，当我找她要的时候，他的表情是这样子的。）key==key设置路径：微信商户平台(pay.weixin.qq.com)–&gt;账户设置–&gt;API安全–&gt;密钥设置在这里：网上有说怎么找得。我也懒得去找。直接自己想了一个字符串，然后用MD5加密成32位的字符串，重新设置的。生成sign签名的时候，要用到这个key值，所以，要保存好。我看别人生成签名（sign）用了很多，反正我就用了上面给出的那些参数生成的sign.（这个上面指的是我的博客上面，不是微信上面。为了减少误解，贴出我生成sign签名的参数）我生成sign签名的参数 准备好以上参数之后，封装成XML格式如下：复制代码1234567891011121314&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;&lt;xml&gt; &lt;appid&gt;wxb1427ebebexxxxxx&lt;/appid&gt; &lt;body&gt;XXX费用&lt;/body&gt; &lt;device_info&gt;WEB&lt;/device_info&gt; &lt;mch_id&gt;132186xxxx&lt;/mch_id&gt; &lt;nonce_str&gt;6AED000AF86A084F9CB0264161E29DD3&lt;/nonce_str&gt; &lt;notify_url&gt;https://一个域名/api/wechatPay/jsapiPayNotify&lt;/notify_url&gt; &lt;openid&gt;oo8WUt0taCqjt552htW1vw-xxxxx&lt;/openid&gt; &lt;out_trade_no&gt;1&lt;/out_trade_no&gt; &lt;sign&gt;各种排序+key生成的那个sign&lt;/sign&gt; &lt;total_fee&gt;1&lt;/total_fee&gt; &lt;trade_type&gt;JSAPI&lt;/trade_type&gt;&lt;/xml&gt; 调用微信的统一下单地址：https://api.mch.weixin.qq.com/pay/unifiedorderNB：=================划重点beign==20170728增加============调用统一下单地址时传的参数个参数名称和参数值与生成签名时相比， ★ 调用统一下单时多了一个sign的参数，其他参数名称需要全部相同。 ★ 调用统一下单时多了一个sign的参数，其他参数名称需要全部相同。 ★ 调用统一下单时多了一个sign的参数，其他参数名称需要全部相同。 nonce_str的值可以不同获取预支付ID时，如果返回值是【签名错误】。那真是的你的签名错了，请仔细核对生成sign的参数名称、参数值和调用统一下单接口的参数名称和参数值=================划重点end==20170728增加============ 见证奇迹的时刻。如果以上参数都神奇的对了，那么会收到微信返回的XML字符串，格式如下复制代码123456789101112&lt;xml&gt; &lt;return_code&gt;&lt;![CDATA[SUCCESS]]&gt;&lt;/return_code&gt; &lt;return_msg&gt;&lt;![CDATA[OK]]&gt;&lt;/return_msg&gt; &lt;appid&gt;&lt;![CDATA[wxb1427ebebexxxxxx]]&gt;&lt;/appid&gt; &lt;mch_id&gt;&lt;![CDATA[132186xxxx]]&gt;&lt;/mch_id&gt; &lt;device_info&gt;&lt;![CDATA[WEB]]&gt;&lt;/device_info&gt; &lt;nonce_str&gt;&lt;![CDATA[Hh4LFHUUvtDYtNdp]]&gt;&lt;/nonce_str&gt; &lt;sign&gt;&lt;![CDATA[079F8A915FD3044F4A17D75F4945E955]]&gt;&lt;/sign&gt; &lt;result_code&gt;&lt;![CDATA[SUCCESS]]&gt;&lt;/result_code&gt; &lt;prepay_id&gt;&lt;![CDATA[wx20160617155030d9e6a0e48b0533061255]]&gt;&lt;/prepay_id&gt; &lt;trade_type&gt;&lt;![CDATA[JSAPI]]&gt;&lt;/trade_type&gt;&lt;/xml&gt; 我们需要的，就是这货 prepay_id 获取到这货之后，第一步骤已经结束了，可以去喝个茶，吃个冰棍，小庆祝一下。 2、H5调起微信支付的内置JS 后台传回前台的参数中，应包含以下几项：appId==这个是不变的==永远不变timeStamp==时间戳==规则：https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=4_2。看完仍是一脸迷茫的，没关系，我们有工具类。谁知道呢，直接调用就好了 nonceStr ==反正我用的跟刚才签名是同一个随机字符串。理论上不用应该也没有关系的，勤快的小伙伴可以试试 package==订单详情扩展字符串==统一下单接口返回的prepay_id参数值，提交格式如：prepay_id=*（你猜对了。刚才我们费那么大力气，获得到的prepay_id就是在这里用的。第一次使用的时候，一直拿订单ID去请求，然后微信给我的信息就是请求参数错误，缺少参数$key0$.不要告诉我，只有我一个人。）signType==签名方式==签名算法，暂支持MD5paySign==签名==这个签名，要重新生成，在后台。使用如上4个参数 5个参数+一个key（永远不变）。（我生成签名的时间戳和传回给前台的时间戳就是timeStamp是同一个。不一样行不行，木有验证）生成paySign的代码NB:生成prepay_id时appid是小写的i,生成paySign时，appId是大写的I 到此为止如果一切顺利，你会看到这样子的一个页面。但是如果，你没有看到这个。而是提示，目录未授权，或者测试账号未在白名单中，我觉得，这才是这篇博客的正确打开方式呢。微信公众号后台，微信支付，开发配置中有一个支付授权目录，测试授权目录，测试白名单。（终于登场了，等的花都谢了） 支付授权目录：开发时，先放一放。（邓小平爷爷在对待中日关系时曾经说过，我们解决不了的问题，先放一放。）测试授权目录：我们要填写的就是这个了。要一个外网能访问到的地址。ip也可以（亲测可以的）。如果你的ip，外网不能访问，找运维同学解决。建议，配置一个测试用的外网可访问的域名。支付配置不允许ip地址了啊，只允许icp认证过的网址。（感谢评论区的同学）测试白名单：这个不解释输入密码，然后，就会看到这样子的结果。（这也不需要我们做什么了） 好激动啊，赶紧去吃点东西，抑制一下，内心的激动。剩下的，我们还有两件事情要去做。先说简单的。3、支付成功之后跳转回自己的系统的某个页面复制代码123456789101112131415161718function onBridgeReady()&#123; WeixinJSBridge.invoke( 'getBrandWCPayRequest', &#123; \"appId\" : appId, //公众号名称，由商户传入 \"timeStamp\":timeStamp, //时间戳，自1970年以来的秒数 \"nonceStr\" : nonceStr, //随机串 \"package\" : Package, \"signType\" :signType, //微信签名方式： \"paySign\" : paySign //微信签名 &#125;, function(res)&#123; if(res.err_msg == \"get_brand_wcpay_request:ok\" ) &#123; window.location.replace(\"index.html\"); &#125; &#125; );&#125; 上述代码中的，红色部分，修改成你想去的页面即可。是不是好奇replace是什么鬼。移步这里，看一下：http://www.xuebuyuan.com/2140432.html 4，最后一部分啦。fighting 该部分有以下3小步骤 1）解析传过来的流信息，通过重新签名的方式验证流中包含的信息的正确性。就是判断这个信息到底是不是微信发的 2）return_code和result_code都是SUCCESS的话，处理商户自己的业务逻辑。就是订单的支付状态啊等一些信息。 3）告诉微信，我收到你的返回值了。不用在发了。 关于以上三点的解释。微信官方是这么说的 12345678910//支付完成后，微信会把相关支付和用户信息发送到商户设定的通知URL，//验证签名，并回应微信。//对后台通知交互时，如果微信收到商户的应答不是成功或超时，微信认为通知失败，//微信会通过一定的策略（如30分钟共8次）定期重新发起通知，//尽可能提高通知的成功率，但微信不保证通知最终能成功。//商户自行增加处理流程,//例如：更新订单状态//例如：数据库操作//例如：推送支付完成信息 还记得我们在第一步生成预支付id(prepay_id时的那个notify_url吗。如果不记得了，请往上翻。如果当时只是随便写了一个，那么这会需要去改一改了。) 一个能访问的到的action.同样地址需要外网能访问的到。没有试ip好不好使。开发这部分功能的时候，运维同学已经配置了测试域名。好开心啊，终于不用在纠结于一些交互配置了。 和支付宝不同，微信返回的是流。和支付宝不同，微信返回的是流。和支付宝不同，微信返回的是流。重要的事情说三遍 解析之后，得到的格式是这样子的 123456789101112131415161718&lt;xml&gt;&lt;appid&gt;&lt;![CDATA[wxb1427ebebeeaxxxx]]&gt;&lt;/appid&gt;&lt;bank_type&gt;&lt;![CDATA[CFT]]&gt;&lt;/bank_type&gt;&lt;cash_fee&gt;&lt;![CDATA[1]]&gt;&lt;/cash_fee&gt;&lt;device_info&gt;&lt;![CDATA[WEB]]&gt;&lt;/device_info&gt;&lt;fee_type&gt;&lt;![CDATA[CNY]]&gt;&lt;/fee_type&gt;&lt;is_subscribe&gt;&lt;![CDATA[Y]]&gt;&lt;/is_subscribe&gt;&lt;mch_id&gt;&lt;![CDATA[132186xxxx]]&gt;&lt;/mch_id&gt;&lt;nonce_str&gt;&lt;![CDATA[07FC15C9D169EE48573EDD749D25945D]]&gt;&lt;/nonce_str&gt;&lt;openid&gt;&lt;![CDATA[oo8WUt0taCqjt552htW1vw-xxxxx]]&gt;&lt;/openid&gt;&lt;out_trade_no&gt;&lt;![CDATA[你的订单编号]]&gt;&lt;/out_trade_no&gt;&lt;result_code&gt;&lt;![CDATA[SUCCESS]]&gt;&lt;/result_code&gt;&lt;return_code&gt;&lt;![CDATA[SUCCESS]]&gt;&lt;/return_code&gt;&lt;sign&gt;&lt;![CDATA[E69940B3EDC437CB5A181210D523806E]]&gt;&lt;/sign&gt;&lt;time_end&gt;&lt;![CDATA[20160621134204]]&gt;&lt;/time_end&gt;&lt;total_fee&gt;1&lt;/total_fee&gt;&lt;trade_type&gt;&lt;![CDATA[JSAPI]]&gt;&lt;/trade_type&gt;&lt;transaction_id&gt;&lt;![CDATA[400386200120160621763973xxxx]]&gt;&lt;/transaction_id&gt;&lt;/xml&gt; ==========================================20170726更新==================================== 微信目前有鼓励金支付，在支付过程中支付金额大于鼓励金金额既可以使用。如果在支付时使用了鼓励金，则回调的流的xml格式如下 1234567891011121314151617181920212223&lt;xml&gt;&lt;appid&gt;&lt;![CDATA[XXXXXX]]&gt;&lt;/appid&gt;&lt;bank_type&gt;&lt;![CDATA[CFT]]&gt;&lt;/bank_type&gt;&lt;cash_fee&gt;&lt;![CDATA[1]]&gt;&lt;/cash_fee&gt;&lt;coupon_count&gt;&lt;![CDATA[1]]&gt;&lt;/coupon_count&gt;&lt;coupon_fee&gt;30&lt;/coupon_fee&gt;&lt;coupon_fee_0&gt;&lt;![CDATA[30]]&gt;&lt;/coupon_fee_0&gt;&lt;coupon_id_0&gt;&lt;![CDATA[2000000001082991244]]&gt;&lt;/coupon_id_0&gt;&lt;device_info&gt;&lt;![CDATA[WEB]]&gt;&lt;/device_info&gt;&lt;fee_type&gt;&lt;![CDATA[CNY]]&gt;&lt;/fee_type&gt;&lt;is_subscribe&gt;&lt;![CDATA[Y]]&gt;&lt;/is_subscribe&gt;&lt;mch_id&gt;&lt;![CDATA[1321867101]]&gt;&lt;/mch_id&gt;&lt;nonce_str&gt;&lt;![CDATA[D0AC1ED0C5CB9ECBCA3D2496EC1AD984]]&gt;&lt;/nonce_str&gt;&lt;openid&gt;&lt;![CDATA[oo8WUt0AjI8G9TNb5W3wmGUxVV1U]]&gt;&lt;/openid&gt;&lt;out_trade_no&gt;&lt;![CDATA[XXXXXX]]&gt;&lt;/out_trade_no&gt;&lt;result_code&gt;&lt;![CDATA[SUCCESS]]&gt;&lt;/result_code&gt;&lt;return_code&gt;&lt;![CDATA[SUCCESS]]&gt;&lt;/return_code&gt;&lt;sign&gt;&lt;![CDATA[9895XXXXX91E1D3F710A3D021147]]&gt;&lt;/sign&gt;&lt;time_end&gt;&lt;![CDATA[20170726184839]]&gt;&lt;/time_end&gt;&lt;total_fee&gt;31&lt;/total_fee&gt;&lt;trade_type&gt;&lt;![CDATA[JSAPI]]&gt;&lt;/trade_type&gt;&lt;transaction_id&gt;&lt;![CDATA[40027220012017072XXX93391176]]&gt;&lt;/transaction_id&gt;&lt;/xml&gt; 注意上述代码中阴影的4行代码。 微信返回的 coupon_countcoupon_fee&gt;coupon_fee_0coupon_id_0 的这4个参数中coupon_fee_0和coupon_id_0，官方给出的参数是couponid$n和couponfee$n，$n为下标，从0开始编号。笔者在处理验签时是将xml转化成了JAVA对象，对于这种动态的参数目前没有什么好的解决方案，目前因为只有_0，所以只定义了一个参数叫做_0，万能的网友在使用的过程中如果有好的解决方案，请不吝赐教。 ============================================20170726更新================================= 对以上第一点和第三点做个解释。 再次吐槽一下。微信真的很喜欢用签名啊。整个过程，3遍签名。也是醉了。 1）我们看到上述微信返回的xml中含有很多字段。使用上述xml中，处sign意外的值+key，进行签名。你没有看错。包含result_code和return_code。 微信的官方对于签名有解释。 原谅我真的好久不学语文了。真的没理解这句话，是用微信回调函数中传的参数，进行重新签名。傻傻的，还在想，用第二次签名是的参数进行签名，时间戳怎么办，要不要存在数据库里面。 将获得的签名与xml中的sign对比，如果相同，证明是微信返回的通知。如果不同，你的通知地址可能被黑客破解了。要不要告诉老板呢，告诉老板了，我怎么解决呢。 2）商户逻辑处理，不解释 3）告诉微信，我收到了你的通知，不需要在发送了。 怎么告诉微信呢。我翻遍了微信的文档，也没有找到回复微信通知这个url。 经人指导，再一次的刷新了认知观。用response. 我是这么写的:(20170727修改) 返回参数参照微信官方文档：https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=9_7 12String xml = \"&lt;xml&gt; &lt;return_code&gt;&lt;![CDATA[SUCCESS]]&gt;&lt;/return_code&gt;&lt;return_msg&gt;&lt;![CDATA[OK]]&gt;&lt;/return_msg&gt;&lt;/xml&gt;\";response.getWriter().write(xml); 这个xml就是微信给你的那个流转化的字符串。 xml中的return_code要是SUCCESS或者FAIL 别问我怎么知道的。官方的demo里面写的 1234567if($notify-&gt;checkSign() == FALSE)&#123; $notify-&gt;setReturnParameter(\"return_code\",\"FAIL\");//返回状态码 $notify-&gt;setReturnParameter(\"return_msg\",\"签名失败\");//返回信息&#125;else&#123; $notify-&gt;setReturnParameter(\"return_code\",\"SUCCESS\");//设置返回码&#125;$returnXml = $notify-&gt;returnXml(); 按照这个写法，返回的数据。在没有收到微信的通知。 之前在测试的时候，返回字符串之后，在没有收到微信的通知，这两天偶然查日志，发现，微信在一直的，通知，不一定是8次。从打印的日志看 有4次，6次。突然，好晕啊。有明白的朋友，还请多多指教 微信公众号支付–JSAPI的开发思路和一下参数的具体解释，全部完成了。具体代码。等我从公司项目里面抽出来。在整理。 还有一个坑：我们在第一步的时候，body传的是英文，如果传中文，直接能用的赶紧感谢一下上苍，返回参数错误的，应该是正常吧。 我的对象和xml转化是用的Java的JAXBContext。很好用的赶脚。赶脚比XMLStream好用。具体写法，会在稍后的代码中，写明。 ———————-我的幽默你不懂————————————- 最后一个问题：开头说了要讲一个笑话。 最近在学英语，一天和同学一起走路。旁边的建筑特别密集。此为背景。 他说，他不喜欢这边的建筑，楼与楼之间距离太密了，像集装箱一样。 然后我问，你知道集装箱的英文怎么说嘛。 朋友一脸的表情，看着我，问我，是什么。 说完，我就后悔了，我也不知道。然后我镇定自若的说，docker. ————————-我的幽默你不懂—————————————— ————————–AD—————————————","categories":[{"name":"Java","slug":"Java","permalink":"http://github.com/b2stry/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://github.com/b2stry/tags/java/"}]},{"title":"Python爬虫小例子","slug":"Python爬虫小例子","date":"2017-10-28T17:04:39.000Z","updated":"2018-02-03T07:32:42.558Z","comments":true,"path":"2017/10/29/Python爬虫小例子/","link":"","permalink":"http://github.com/b2stry/2017/10/29/Python爬虫小例子/","excerpt":"爬取京东手机图片和糗百。","text":"爬取京东手机图片和糗百。 1.爬取京东手机图片 123456789101112131415161718192021222324252627282930import reimport urllib.requestdef craw(url, page): html1 = urllib.request.urlopen(url).read() html1 = str(html1) pat1 = '&lt;div id=\"J_goodsList\" class=\"goods-list-v2 gl-type-3 J-goods-list\"&gt;.+?&lt;div class=\"page clearfix\"&gt;' result1 = re.compile(pat1).findall(html1) result1 = result1[0] pat2 = 'src=\"//(img.+?[\\.jpg|\\.png])\"' imagelist = re.compile(pat2).findall(result1) x = 1 for imageurl in imagelist: imagename = \"D:/Python/images/jd/\" + str(page) + str(x) + \".jpg\" imageurl = \"http://\" + imageurl try: urllib.request.urlretrieve(imageurl, filename=imagename) except urllib.error.URLError as e: if hasattr(e, \"code\"): x += 1 if hasattr(e, \"reason\"): x += 1 x += 1for i in range(1, 79): url = \"https://search.jd.com/Search?keyword=%E6%89%8B%E6%9C%BA&amp;enc=utf-8&amp;qrst=1&amp;rt=1&amp;stop=1&amp;vt=2&amp;wq=shou&amp;cid2=653&amp;cid3=655&amp;page=\" + str( i) + \"&amp;s=164&amp;click=0\" craw(url, i) 2.爬取糗百段子 123456789101112131415161718192021222324252627282930313233343536373839404142434445import urllib.requestimport reimport http.cookiejardef getcontent(url, page): headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.91 Safari/537.36\") cjar = http.cookiejar.CookieJar() opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cjar)) opener.addheaders = [headers] urllib.request.install_opener(opener) data = urllib.request.urlopen(url).read().decode(\"utf-8\") pat = '&lt;div class=\"content-block clearfix\"&gt;(.*?)&lt;div class=\"foot\"&gt;' userpat = '&lt;h2&gt;(.*?)&lt;/h2&gt;' contentpat = '&lt;div class=\"content\"&gt;(.*?)&lt;/div&gt;' data2 = re.compile(pat, re.S).findall(data) userlist = re.compile(userpat, re.S).findall(data2[0]) contentlist = re.compile(contentpat, re.S).findall(data2[0]) x = 1 for content in contentlist: content = content.replace(\"\\n\", \"\") content = content.replace(\"&lt;span&gt;\", \"\") content = content.replace(\"&lt;/span&gt;\", \"\") content = content.replace(\"&lt;br/&gt;\", \"\") name = \"content\" + str(x) exec(name + '=content') x += 1 y = 1 for user in userlist: name = \"content\" + str(y) print(\"用户\" + str(page) + str(y) + \"是：\" + user.replace(\"\\n\", \"\")) print(\"内容是：\") exec(\"print(\" + name + \")\") print(\"\\n\") y += 1for i in range(1, 5): url = \"https://www.qiushibaike.com/8hr/page/\" + str(i) getcontent(url, i)","categories":[{"name":"python","slug":"python","permalink":"http://github.com/b2stry/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://github.com/b2stry/tags/python/"}]},{"title":"定时重启Tomcat的shell脚本","slug":"定时重启Tomcat的shell脚本","date":"2017-10-26T15:20:21.000Z","updated":"2018-01-13T12:48:32.740Z","comments":true,"path":"2017/10/26/定时重启Tomcat的shell脚本/","link":"","permalink":"http://github.com/b2stry/2017/10/26/定时重启Tomcat的shell脚本/","excerpt":"因为面试的时候被问到过，就百度了下。","text":"因为面试的时候被问到过，就百度了下。 1.先写一个retomcat.sh的脚本 习惯性把tomcat放到/usr/local/目录下 12345678910111213141516171819202122232425262728293031#!/bin/shpid=`ps aux | grep tomcat | grep -v grep | grep -v retomcat | awk '&#123;print $2&#125;'`echo $pidif [ -n \"$pid\" ]then&#123; echo ===========shutdown================ /usr/local/tomcat/bin/shutdown.sh sleep 2 pid=`ps aux | grep tomcat | grep -v grep | grep -v retomcat | awk '&#123;print $2&#125;'` if [ -n \"$pid\" ] then &#123; sleep 2 echo ========kill tomcat begin============== echo $pid kill -9 $pid echo ========kill tomcat end============== &#125; fi sleep 2 echo ===========startup.sh============== /usr/local/tomcat/bin/startup.sh&#125;elseecho ===========startup.sh==============/usr/local/tomcat/bin/startup.shfi 2.新建定时任务 终端输入：crontab –e回车 输入：00 03 * * * /usr/local/tomcat/bin/retomcat.sh每天凌晨3点运行脚本ps:12345678Example of job definition:.---------------- minute (0 - 59)| .------------- hour (0 - 23)| | .---------- day of month (1 - 31)| | | .------- month (1 - 12) OR jan,feb,mar,apr ...| | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat| | | | |* * * * * user-name command to be executed 3.重启定时服务 service crond stop service crond start 完美收工！","categories":[{"name":"Shell","slug":"Shell","permalink":"http://github.com/b2stry/categories/Shell/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://github.com/b2stry/tags/linux/"},{"name":"shell","slug":"shell","permalink":"http://github.com/b2stry/tags/shell/"},{"name":"tomcat","slug":"tomcat","permalink":"http://github.com/b2stry/tags/tomcat/"}]},{"title":"docker基本操作","slug":"docker基本操作","date":"2017-10-26T05:48:08.000Z","updated":"2018-01-13T12:50:41.755Z","comments":true,"path":"2017/10/26/docker基本操作/","link":"","permalink":"http://github.com/b2stry/2017/10/26/docker基本操作/","excerpt":"什么是Docker? &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Docker是基于Go语言实现的云开源项目。Docker的主要目标是”Build, Ship and Run Any App, Anywhere”,也就是通过对应用组件的封装、分发、部署、运行等生命周期的管理，使用户的APP（可以是一个WEB应用或者数据库应用等等）及其运行环境能够做到”一次封装,到处运行”。","text":"什么是Docker? &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Docker是基于Go语言实现的云开源项目。Docker的主要目标是”Build, Ship and Run Any App, Anywhere”,也就是通过对应用组件的封装、分发、部署、运行等生命周期的管理，使用户的APP（可以是一个WEB应用或者数据库应用等等）及其运行环境能够做到”一次封装,到处运行”。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Docker引擎的基础是Linux自带的容器（Linux Containers, LXC）技术。IBM对于容器技术的准确描述如下：容器有效的将单个操作系统管理的资源划分到孤立的组中，以便更好的在孤立的组之间平衡有冲突的资源使用需求。与虚拟化相比，这样既不需要指令级模拟，也不需要即时编译。容器可以在核心CPU本地运行指令，而不需要任何专门的解释机制。此外，也避免了准虚拟化（paravirtualization）和系统调用替换中的复杂性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以将容器理解为一种沙盒。每个容器内运行一个应用，不同的容器相互隔离，容器之间可以建立通信机制。容器的创建和停止都十分快速（秒级），容器自身对资源的需求十分有限，远比虚拟机本身占用的资源少。 基本命令 获取镜像：docker pull ubuntu 查找镜像：docker search ubuntu 查看镜像：docker images 删除镜像：docker rmi ubuntu,也可以用id，取前几位就行。 创建镜像：docker commit CONTAINER self：ubuntu CONTAINER为容器id 导出镜像：docker export xxxid &gt; xxx.tar 导入镜像：docker import xxx.tar test/ubuntu 创建容器：docker create ubuntu 启动容器：docker run ubuntu 查看容器：docker ps -a 停止容器：docker stop xxxid 进入容器：docker attach xxxid 删除容器：docker rm xxxid 启动容器，输出hello world：docker run ubuntu /bin/echo &#39;Hello world&#39; 启动容器，进入容器bash终端:docker run -t -i ubuntu /bin/bash 启动容器，守护态运行：docker run -d ubuntu /bin/bash 启动容器，守护态运行,把本地80端口映射到容器81端口：docker run -p 80:81 -d ubuntu /bin/bash","categories":[{"name":"docker","slug":"docker","permalink":"http://github.com/b2stry/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://github.com/b2stry/tags/docker/"}]},{"title":"Mysql面试题","slug":"Mysql面试题","date":"2017-10-23T08:52:10.000Z","updated":"2018-01-13T12:51:28.540Z","comments":true,"path":"2017/10/23/Mysql面试题/","link":"","permalink":"http://github.com/b2stry/2017/10/23/Mysql面试题/","excerpt":"Mysql知识点。","text":"Mysql知识点。 1、MySQL的复制原理以及流程 基本原理流程，3个线程以及之间的关联； 主：binlog线程——记录下所有改变了数据库数据的语句，放进master上的binlog中； 从：io线程——在使用start slave 之后，负责从master上拉取 binlog 内容，放进 自己的relay log中； 从：sql执行线程——执行relay log中的语句； 2、MySQL中myisam与innodb的区别，至少5点 (1)、问5点不同；1&gt;.InnoDB支持事物，而MyISAM不支持事物2&gt;.InnoDB支持行级锁，而MyISAM支持表级锁3&gt;.InnoDB支持MVCC, 而MyISAM不支持4&gt;.InnoDB支持外键，而MyISAM不支持5&gt;.InnoDB不支持全文索引，而MyISAM支持。(2)、innodb引擎的4大特性插入缓冲（insert buffer),二次写(double write),自适应哈希索引(ahi),预读(read ahead)(3)、2者selectcount(*)哪个更快，为什么myisam更快，因为myisam内部维护了一个计数器，可以直接调取。 3、MySQL中varchar与char的区别以及varchar(50)中的50代表的涵义 (1)、varchar与char的区别char是一种固定长度的类型，varchar则是一种可变长度的类型(2)、varchar(50)中50的涵义最多存放50个字符，varchar(50)和(200)存储hello所占空间一样，但后者在排序时会消耗更多内存，因为order by col采用fixed_length计算col长度(memory引擎也一样)(3)、int（20）中20的涵义是指显示字符的长度但要加参数的，最大为255，比如它是记录行数的id,插入10笔资料，它就显示00000000001 ~~~00000000010，当字符的位数超过11,它也只显示11位，如果你没有加那个让它未满11位就前面加0的参数，它不会在前面加020表示最大显示宽度为20，但仍占4字节存储，存储范围不变；(4)、mysql为什么这么设计对大多数应用没有意义，只是规定一些工具用来显示字符的个数；int(1)和int(20)存储和计算均一样； 4、问了innodb的事务与日志的实现方式 (1)、有多少种日志；错误日志：记录出错信息，也记录一些警告信息或者正确的信息。查询日志：记录所有对数据库请求的信息，不论这些请求是否得到了正确的执行。慢查询日志：设置一个阈值，将运行时间超过该值的所有SQL语句都记录到慢查询的日志文件中。二进制日志：记录对数据库执行更改的所有操作。中继日志：事务日志： (2)、事物的4种隔离级别隔离级别读未提交(RU)读已提交(RC)可重复读(RR)串行 (3)、事务是如何通过日志来实现的，说得越深入越好。事务日志是通过redo和innodb的存储引擎日志缓冲（Innodb log buffer）来实现的，当开始一个事务的时候，会记录该事务的lsn(log sequence number)号; 当事务执行时，会往InnoDB存储引擎的日志的日志缓存里面插入事务日志；当事务提交时，必须将存储引擎的日志缓冲写入磁盘（通过innodb_flush_log_at_trx_commit来控制），也就是写数据前，需要先写日志。这种方式称为“预写日志方式” 5、问了MySQL binlog的几种日志录入格式以及区别? (1)、binlog的日志格式的种类和分别(2)、适用场景；(3)、结合第一个问题，每一种日志格式在复制中的优劣。Statement：每一条会修改数据的sql都会记录在binlog中。优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。(相比row能节约多少性能 与日志量，这个取决于应用的SQL情况，正常同一条记录修改或者插入row格式所产生的日志量还小于Statement产生的日志量，但是考虑到如果带条 件的update操作，以及整表删除，alter表等操作，ROW格式会产生大量日志，因此在考虑是否使用ROW格式日志时应该跟据应用的实际情况，其所 产生的日志量会增加多少，以及带来的IO性能问题。)缺点：由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的 一些相关信息，以保证所有语句能在slave得到和在master端执行时候相同 的结果。另外mysql 的复制,像一些特定函数功能，slave可与master上要保持一致会有很多相关问题(如sleep()函数， last_insert_id()，以及user-defined functions(udf)会出现问题).使用以下函数的语句也无法被复制： LOAD_FILE() UUID() USER() FOUND_ROWS() SYSDATE() (除非启动时启用了 –sysdate-is-now 选项)同时在INSERT …SELECT 会产生比 RBR 更多的行级锁2.Row:不记录sql语句上下文相关信息，仅保存哪条记录被修改。优点： binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下 每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题缺点:所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容,比 如一条update语句，修改多条记录，则binlog中每一条修改都会有记录，这样造成binlog日志量会很大，特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中。3.Mixedlevel: 是以上两种level的混合使用，一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则 采用row格式保存binlog,MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择 一种.新版本的MySQL中队row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变更的时候就会以statement模式来记录。至于update或者delete等修改数据的语句，还是会记录所有行的 变更。 6、问了下MySQL数据库cpu飙升到500%的话他怎么处理？ (1)、没有经验的，可以不问；(2)、有经验的，问他们的处理思路。列出所有进程 show processlist 观察所有进程 多秒没有状态变化的(干掉)查看超时日志或者错误日志 (做了几年开发,一般会是查询以及大批量的插入会导致cpu与i/o上涨,,,,当然不排除网络状态突然断了,,导致一个请求服务器只接受到一半，比如where子句或分页子句没有发送,,当然的一次被坑经历) 7、sql优化 (1)、explain出来的各种item的意义；select_type表示查询中每个select子句的类型type表示MySQL在表中找到所需行的方式，又称“访问类型”possible_keys指出MySQL能使用哪个索引在表中找到行，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用key显示MySQL在查询中实际使用的索引，若没有使用索引，显示为NULLkey_len表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度ref表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值Extra包含不适合在其他列中显示但十分重要的额外信息 (2)、profile的意义以及使用场景；查询到 SQL 会执行多少时间, 并看出 CPU/Memory 使用量, 执行过程中 Systemlock, Table lock 花多少时间等等 8、备份计划，mysqldump以及xtranbackup的实现原理? (1)、备份计划；这里每个公司都不一样，您别说那种1小时1全备什么的就行(2)、备份恢复时间；这里跟机器，尤其是硬盘的速率有关系，以下列举几个仅供参考20G的2分钟（mysqldump）80G的30分钟(mysqldump)111G的30分钟（mysqldump)288G的3小时（xtra)3T的4小时（xtra)逻辑导入时间一般是备份时间的5倍以上 (3)、xtrabackup实现原理在InnoDB内部会维护一个redo日志文件，我们也可以叫做事务日志文件。事务日志会存储每一个InnoDB表数据的记录修改。当InnoDB启动时，InnoDB会检查数据文件和事务日志，并执行两个步骤：它应用（前滚）已经提交的事务日志到数据文件，并将修改过但没有提交的数据进行回滚操作。 9、mysqldump中备份出来的sql，如果我想sql文件中，一行只有一个insert….value()的话，怎么办？如果备份需要带上master的复制点信息怎么办？ –skip-extended-insert[root@helei-zhuanshu ~]# mysqldump -uroot -p helei –skip-extended-insertEnter password: KEY idx_c1 (c1), KEY idx_c2 (c2)) ENGINE=InnoDB AUTO_INCREMENT=51 DEFAULT CHARSET=latin1;/!40101 SET character_set_client = @saved_cs_client /; – – Dumping data for table heleiLOCK TABLES helei WRITE;/!40000 ALTER TABLE helei DISABLE KEYS /;INSERT INTO helei VALUES (1,32,37,38,’2016-10-18 06:19:24’,’susususususususususususu’);INSERT INTO helei VALUES (2,37,46,21,’2016-10-18 06:19:24’,’susususususu’);INSERT INTO helei VALUES (3,21,5,14,’2016-10-18 06:19:24’,’susu’); 10、500台db，在最快时间之内重启? puppet，dsh 11、innodb的读写参数优化? (1)、读取参数global buffer pool以及 local buffer； (2)、写入参数；innodb_flush_log_at_trx_commitinnodb_buffer_pool_size (3)、与IO相关的参数；innodb_write_io_threads = 8innodb_read_io_threads = 8innodb_thread_concurrency = 0 (4)、缓存参数以及缓存的适用场景。query cache/query_cache_type并不是所有表都适合使用query cache。造成query cache失效的原因主要是相应的table发生了变更 第一个：读操作多的话看看比例，简单来说，如果是用户清单表，或者说是数据比例比较固定，比如说商品列表，是可以打开的，前提是这些库比较集中，数据库中的实务比较小。 第二个：我们“行骗”的时候，比如说我们竞标的时候压测，把query cache打开，还是能收到qps激增的效果，当然前提示前端的连接池什么的都配置一样。大部分情况下如果写入的居多，访问量并不多，那么就不要打开，例如社交网站的，10%的人产生内容，其余的90%都在消费，打开还是效果很好的，但是你如果是qq消息，或者聊天，那就很要命。 第三个：小网站或者没有高并发的无所谓，高并发下，会看到 很多 qcache 锁 等待，所以一般高并发下，不建议打开query cache 12、你是如何监控你们的数据库的？你们的慢日志都是怎么查询的？ 监控的工具有很多，例如zabbix，lepus，我这里用的是lepus 13、你是否做过主从一致性校验，如果有，怎么做的，如果没有，你打算怎么做？ 主从一致性校验有多种工具 例如checksum、mysqldiff、pt-table-checksum等 14、你们数据库是否支持emoji表情，如果不支持，如何操作？ 如果是utf8字符集的话，需要升级至utf8_mb4方可支持 15、你是如何维护数据库的数据字典的？ 这个大家维护的方法都不同，我一般是直接在生产库进行注释，利用工具导出成excel方便流通。 16、你们是否有开发规范，如果有，如何执行的? 有，开发规范网上有很多了，可以自己看看总结下 17、表中有大字段X(例如：text类型)，且字段X不会经常更新，以读为为主，请问? (1)、您是选择拆成子表，还是继续放一起；(2)、写出您这样选择的理由。答：拆带来的问题：连接消耗 + 存储拆分空间；不拆可能带来的问题：查询性能；如果能容忍拆分带来的空间问题,拆的话最好和经常要查询的表的主键在物理结构上放置在一起(分区) 顺序IO,减少连接消耗,最后这是一个文本列再加上一个全文索引来尽量抵消连接消耗如果能容忍不拆分带来的查询性能损失的话:上面的方案在某个极致条件下肯定会出现问题,那么不拆就是最好的选择 18、MySQL中InnoDB引擎的行锁是通过加在什么上完成(或称实现)的？为什么是这样子的？ 答：InnoDB是基于索引来完成行锁例: select * from tab_with_index where id = 1 for update;for update 可以根据条件来完成行锁锁定,并且 id 是有索引键的列,如果 id 不是索引键那么InnoDB将完成表锁,,并发将无从谈起 . 19、如何从mysqldump产生的全库备份中只恢复某一个库、某一张表？ 答案见：http://suifu.blog.51cto.com/9167728/1830651 开放性问题：据说是腾讯的 一个6亿的表a，一个3亿的表b，通过外间tid关联，你如何最快的查询出满足条件的第50000到第50200中的这200条数据记录。1、如果A表TID是自增长,并且是连续的,B表的ID为索引select * from a,b where a.tid = b.id and a.tid&gt;500000 limit 200; 2、如果A表的TID不是连续的,那么就需要使用覆盖索引.TID要么是主键,要么是辅助索引,B表ID也需要有索引。select * from b , (select tid from a limit 50000,200) a where b.id = a .tid;","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://github.com/b2stry/categories/Mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://github.com/b2stry/tags/mysql/"}]},{"title":"Java Web基础知识点","slug":"Java Web基础知识点","date":"2017-09-25T03:31:21.000Z","updated":"2018-02-03T07:39:30.851Z","comments":true,"path":"2017/09/25/Java Web基础知识点/","link":"","permalink":"http://github.com/b2stry/2017/09/25/Java Web基础知识点/","excerpt":"Java Web 的一些小知识点。","text":"Java Web 的一些小知识点。 1、jsp的组成元素? 静态内容 指令(&lt;%@ %&gt;) 表达式(&lt;%=表达式%&gt;，输出) 小脚本(&lt;%Java代码%&gt;) 声明(&lt;%! %&gt;) 动作(&lt;jsp:动作名&gt;&lt;/jsp:动作名&gt;) 注释(&lt;!-- --&gt;客户端可见，&lt;%– –%&gt;客户端不可见) 2、jsp的隐式对象? 输入输出对象：request、response、out 作用域通信对象：pageContext、request、session、application Java对象：page、config 错误对象：exception 3、转发和重定向的区别以及各自的语句? 区别： 转发在服务器端完成；重定向在客户端完成； 转发速度快；重定向速度慢； 转发是同一次请求；重定向是两次不同请求； 转发的地址栏没变化；重定向的地址栏有变化； 转发必须在同一服务器下完成；重定向可以在不同服务器下完成。 语句： 转发：request.getRequestDispatcher(&quot;页面&quot;).forward(request, response); 重定向：response.sendRedirection(&quot;页面&quot;); 4、get和post的区别？ get请求：相当于明信片，所有的信息都显示在地址栏；请求的信息有限，而且只能传递字符串；方便。 post请求：相当于信封，安全性高；大小没有限制。 5、默认端口？ tomcat : 8080 http : 80 mysql ：3306 sqlserver : 1433 oracle : 1521 6、会话跟踪的原理？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对一个用户对服务器连续的请求和接收响应进行监视。 7、session和cookie的区别？ cookie数据存放在客户的浏览器上；session数据放在服务器上； cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗，考虑到安全应当使用session； session会在一定时间内保存在服务器，当访问增多，会比较占用服务器性能，考虑到减轻服务器性能方面，应当用cookie； 单个cookie最大4k，很多浏览器都限制一个站点最多存20个cookie； 建议：登录信息等重要信息存session，其他需保留信息放在cookie。 8、jsp的三大指令？ page taglib include 9、实现会话跟踪的四种方式？ URL重写 隐藏表单域 session cookie 10、b/s和c/s的区别？ c/s是客户端/服务器，一般要在电脑安装软件，或运行一个客户端软件才能使用，在系统升级时稍麻烦点。 b/s是浏览器/服务器，打开浏览器就能使用，一般是网页形式，与c/s相比，web应用程序易维护、升级和部署，速度相对较慢，因为数据全保存在服务器上。 11、getParameter()和getAttribute()的区别？ getPatameter()是用来接收用post和get方式传递过来的参数的；getAttribute()必须先setAttribute(); getParameter()传递的数据会从web客户端传到web服务器端，代表HTTP请求数据，返回String类型的数据。setAttribute()和getAttribute()传递的数据只会存在于web容器内部。","categories":[{"name":"Java","slug":"Java","permalink":"http://github.com/b2stry/categories/Java/"}],"tags":[{"name":"JSP","slug":"JSP","permalink":"http://github.com/b2stry/tags/JSP/"}]},{"title":"计算机网络复习","slug":"计算机网络复习","date":"2017-09-21T05:29:01.000Z","updated":"2018-01-13T12:45:01.378Z","comments":true,"path":"2017/09/21/计算机网络复习/","link":"","permalink":"http://github.com/b2stry/2017/09/21/计算机网络复习/","excerpt":"网络的主要知识点。","text":"网络的主要知识点。 OSI，TCP/IP，五层协议的体系结构，以及各层协议 OSI分层 （7层）：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。TCP/IP分层（4层）：网络接口层、 网际层、运输层、 应用层。五层协议(5层）：物理层、数据链路层、网络层、运输层、 应用层。每一层的协议如下：物理层：RJ45、CLOCK、IEEE802.3 （中继器，集线器，网关）数据链路：PPP、FR、HDLC、VLAN、MAC （网桥，交换机）网络层：IP、ICMP、ARP、RARP、OSPF、IPX、RIP、IGRP、 （路由器）传输层：TCP、UDP、SPX会话层：NFS、SQL、NETBIOS、RPC表示层：JPEG、MPEG、ASII应用层：FTP、DNS、Telnet、SMTP、HTTP、WWW、NFS 每一层的作用如下： 物理层：通过媒介传输比特,确定机械及电气规范（比特Bit） 数据链路层：将比特组装成帧和点到点的传递（帧Frame） 网络层：负责数据包从源到宿的传递和网际互连（包PackeT） 传输层：提供端到端的可靠报文传递和错误恢复（段Segment） 会话层：建立、管理和终止会话（会话协议数据单元SPDU） 表示层：对数据进行翻译、加密和压缩（表示协议数据单元PPDU） 应用层：允许访问OSI环境的手段（应用协议数据单元APDU） IP地址的分类 A类地址：以0开头，第一个字节范围：0~127（1.0.0.0 - 126.255.255.255）； B类地址：以10开头，第一个字节范围：128~191（128.0.0.0 - 191.255.255.255）； C类地址：以110开头，第一个字节范围：192~223（192.0.0.0 - 223.255.255.255）； 10.0.0.0—10.255.255.255， 172.16.0.0—172.31.255.255， 192.168.0.0—192.168.255.255。（Internet上保留地址用于内部） IP地址与子网掩码相与得到主机号 ARP是地址解析协议，简单语言解释一下工作原理。 1：首先，每个主机都会在自己的ARP缓冲区中建立一个ARP列表，以表示IP地址和MAC地址之间的对应关系。 2：当源主机要发送数据时，首先检查ARP列表中是否有对应IP地址的目的主机的MAC地址，如果有，则直接发送数据，如果没有，就向本网段的所有主机发送ARP数据包，该数据包包括的内容有：源主机 IP地址，源主机MAC地址，目的主机的IP 地址。 3：当本网络的所有主机收到该ARP数据包时，首先检查数据包中的IP地址是否是自己的IP地址，如果不是，则忽略该数据包，如果是，则首先从数据包中取出源主机的IP和MAC地址写入到ARP列表中，如果已经存在，则覆盖，然后将自己的MAC地址写入ARP响应包中，告诉源主机自己是它想要找的MAC地址。 4：源主机收到ARP响应包后。将目的主机的IP和MAC地址写入ARP列表，并利用此信息发送数据。如果源主机一直没有收到ARP响应数据包，表示ARP查询失败。 广播发送ARP请求，单播发送ARP响应。 各种协议 ICMP协议： 因特网控制报文协议。它是TCP/IP协议族的一个子协议，用于在IP主机、路由器之间传递控制消息。 TFTP协议： 是TCP/IP协议族中的一个用来在客户机与服务器之间进行简单文件传输的协议，提供不复杂、开销不大的文件传输服务。 HTTP协议： 超文本传输协议，是一个属于应用层的面向对象的协议，由于其简捷、快速的方式，适用于分布式超媒体信息系统。 DHCP协议： 动态主机配置协议，是一种让系统得以连接到网络上，并获取所需要的配置参数手段。 NAT协议：网络地址转换属接入广域网(WAN)技术，是一种将私有（保留）地址转化为合法IP地址的转换技术， DHCP协议：一个局域网的网络协议，使用UDP协议工作，用途：给内部网络或网络服务供应商自动分配IP地址，给用户或者内部网络管理员作为对所有计算机作中央管理的手段。 描述：RARP RARP是逆地址解析协议，作用是完成硬件地址到IP地址的映射，主要用于无盘工作站，因为给无盘工作站配置的IP地址不能保存。工作流程：在网络中配置一台RARP服务器，里面保存着IP地址和MAC地址的映射关系，当无盘工作站启动后，就封装一个RARP数据包，里面有其MAC地址，然后广播到网络上去，当服务器收到请求包后，就查找对应的MAC地址的IP地址装入响应报文中发回给请求者。因为需要广播请求报文，因此RARP只能用于具有广播能力的网络。 TCP三次握手和四次挥手的全过程 三次握手： 第一次握手：客户端发送syn包(syn=x)到服务器，并进入SYN_SEND状态，等待服务器确认； 第二次握手：服务器收到syn包，必须确认客户的SYN（ack=x+1），同时自己也发送一个SYN包（syn=y），即SYN+ACK包，此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack=y+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。 握手过程中传送的包里不包含数据，三次握手完毕后，客户端与服务器才正式开始传送数据。理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。 四次握手: 与建立连接的“三次握手”类似，断开一个TCP连接则需要“四次握手”。 第一次挥手：主动关闭方发送一个FIN，用来关闭主动方到被动关闭方的数据传送，也就是主动关闭方告诉被动关闭方：我已经不 会再给你发数据了(当然，在fin包之前发送出去的数据，如果没有收到对应的ack确认报文，主动关闭方依然会重发这些数据)，但是，此时主动关闭方还可 以接受数据。 第二次挥手：被动关闭方收到FIN包后，发送一个ACK给对方，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号）。第三次挥手：被动关闭方发送一个FIN，用来关闭被动关闭方到主动关闭方的数据传送，也就是告诉主动关闭方，我的数据也发送完了，不会再给你发数据了。第四次挥手：主动关闭方收到FIN后，发送一个ACK给被动关闭方，确认序号为收到序号+1，至此，完成四次挥手。 在浏览器中输入www.baidu.com后执行的全部过程 1、客户端浏览器通过DNS解析到www.baidu.com的IP地址220.181.27.48，通过这个IP地址找到客户端到服务器的路径。客户端浏览器发起一个HTTP会话到220.161.27.48，然后通过TCP进行封装数据包，输入到网络层。 2、在客户端的传输层，把HTTP会话请求分成报文段，添加源和目的端口，如服务器使用80端口监听客户端的请求，客户端由系统随机选择一个端口如5000，与服务器进行交换，服务器把相应的请求返回给客户端的5000端口。然后使用IP层的IP地址查找目的端。 3、客户端的网络层不用关系应用层或者传输层的东西，主要做的是通过查找路由表确定如何到达服务器，期间可能经过多个路由器，这些都是由路由器来完成的工作，我不作过多的描述，无非就是通过查找路由表决定通过那个路径到达服务器。 4、客户端的链路层，包通过链路层发送到路由器，通过邻居协议查找给定IP地址的MAC地址，然后发送ARP请求查找目的地址，如果得到回应后就可以使用ARP的请求应答交换的IP数据包现在就可以传输了，然后发送IP数据包到达服务器的地址。 TCP和UDP的区别？ TCP提供面向连接的、可靠的数据流传输，而UDP提供的是非面向连接的、不可靠的数据流传输。 TCP传输单位称为TCP报文段，UDP传输单位称为用户数据报。 TCP注重数据安全性，UDP数据传输快，因为不需要连接等待，少了许多操作，但是其安全性却一般。 TCP对应的协议和UDP对应的协议 TCP对应的协议：（1） FTP：定义了文件传输协议，使用21端口。 （2） Telnet：一种用于远程登陆的端口，使用23端口，用户可以以自己的身份远程连接到计算机上，可提供基于DOS模式下的通信服务。 （3） SMTP：邮件传送协议，用于发送邮件。服务器开放的是25号端口。 （4） POP3：它是和SMTP对应，POP3用于接收邮件。POP3协议所用的是110端口。 （5）HTTP：是从Web服务器传输超文本到本地浏览器的传送协议。 UDP对应的协议：（1） DNS：用于域名解析服务，将域名地址转换为IP地址。DNS用的是53号端口。 （2） SNMP：简单网络管理协议，使用161号端口，是用来管理网络设备的。由于网络设备很多，无连接的服务就体现出其优势。 （3） TFTP(Trival File Tran敏感词er Protocal)，简单文件传输协议，该协议在熟知端口69上使用UDP服务。 DNS域名系统，简单描述其工作原理。 当DNS客户机需要在程序中使用名称时，它会查询DNS服务器来解析该名称。客户机发送的每条查询信息包括三条信息：包括：指定的DNS域名，指定的查询类型，DNS域名的指定类别。基于UDP服务，端口53. 该应用一般不直接为用户使用，而是为其他应用服务，如HTTP，SMTP等在其中需要完成主机名到IP地址的转换。 面向连接和非面向连接的服务的特点是什么？ 面向连接的服务，通信双方在进行通信之前，要先在双方建立起一个完整的可以彼此沟通的通道，在通信过程中，整个连接的情况一直可以被实时地监控和管理。 非面向连接的服务，不需要预先建立一个联络两个通信节点的连接，需要通信的时候，发送节点就可以往网络上发送信息，让信息自主地在网络上去传，一般在传输的过程中不再加以监控。 TCP的三次握手过程？为什么会采用三次握手，若采用二次握手可以吗？ 答：建立连接的过程是利用客户服务器模式，假设主机A为客户端，主机B为服务器端。 （1）TCP的三次握手过程：主机A向B发送连接请求；主机B对收到的主机A的报文段进行确认；主机A再次对主机B的确认进行确认。 （2）采用三次握手是为了防止失效的连接请求报文段突然又传送到主机B，因而产生错误。失效的连接请求报文段是指：主机A发出的连接请求没有收到主机B的确认，于是经过一段时间后，主机A又重新向主机B发送连接请求，且建立成功，顺序完成数据传输。考虑这样一种特殊情况，主机A第一次发送的连接请求并没有丢失，而是因为网络节点导致延迟达到主机B，主机B以为是主机A又发起的新连接，于是主机B同意连接，并向主机A发回确认，但是此时主机A根本不会理会，主机B就一直在等待主机A发送数据，导致主机B的资源浪费。 （3）采用两次握手不行，原因就是上面说的实效的连接请求的特殊情况。 IP数据包的格式 IP数据报由首部和数据两部分组成。首部由固定部分和可选部分组成。首部的固定部分有20字节。可选部分的长度变化范围为1——40字节。 TCP数据报的格式？ 一个TCP报文段分为首部和数据两部分。首部由固定部分和选项部分组成，固定部分是20字节。TCP首部的最大长度为60。 了解交换机、路由器、网关的概念，并知道各自的用途 1）交换机 在计算机网络系统中，交换机是针对共享工作模式的弱点而推出的。交换机拥有一条高带宽的背部总线和内部交换矩阵。交换机的所有的端口都挂接在这条背 部总线上，当控制电路收到数据包以后，处理端口会查找内存中的地址对照表以确定目的MAC（网卡的硬件地址）的NIC（网卡）挂接在哪个端口上，通过内部 交换矩阵迅速将数据包传送到目的端口。目的MAC若不存在，交换机才广播到所有的端口，接收端口回应后交换机会“学习”新的地址，并把它添加入内部地址表 中。 交换机工作于OSI参考模型的第二层，即数据链路层。交换机内部的CPU会在每个端口成功连接时，通过ARP协议学习它的MAC地址，保存成一张 ARP表。在今后的通讯中，发往该MAC地址的数据包将仅送往其对应的端口，而不是所有的端口。因此，交换机可用于划分数据链路层广播，即冲突域；但它不 能划分网络层广播，即广播域。 交换机被广泛应用于二层网络交换，俗称“二层交换机”。 交换机的种类有：二层交换机、三层交换机、四层交换机、七层交换机分别工作在OSI七层模型中的第二层、第三层、第四层盒第七层，并因此而得名。 2）路由器 路由器（Router）是一种计算机网络设备，提供了路由与转送两种重要机制，可以决定数据包从来源端到目的端所经过 的路由路径（host到host之间的传输路径），这个过程称为路由；将路由器输入端的数据包移送至适当的路由器输出端(在路由器内部进行)，这称为转 送。路由工作在OSI模型的第三层——即网络层，例如网际协议。 路由器的一个作用是连通不同的网络，另一个作用是选择信息传送的线路。 路由器与交换器的差别，路由器是属于OSI第三层的产品，交换器是OSI第二层的产品(这里特指二层交换机)。 3）网关 网关（Gateway），网关顾名思义就是连接两个网络的设备，区别于路由器（由于历史的原因，许多有关TCP/IP 的文献曾经把网络层使用的路由器（Router）称为网关，在今天很多局域网采用都是路由来接入网络，因此现在通常指的网关就是路由器的IP），经常在家 庭中或者小型企业网络中使用，用于连接局域网和Internet。 网关也经常指把一种协议转成另一种协议的设备，比如语音网关。 在传统TCP/IP术语中，网络设备只分成两种，一种为网关（gateway），另一种为主机（host）。网关能在网络间转递数据包，但主机不能 转送数据包。在主机（又称终端系统，end system）中，数据包需经过TCP/IP四层协议处理，但是在网关（又称中介系 统，intermediate system）只需要到达网际层（Internet layer），决定路径之后就可以转送。在当时，网关 （gateway）与路由器（router）还没有区别。 在现代网络术语中，网关（gateway）与路由器（router）的定义不同。网关（gateway）能在不同协议间移动数据，而路由器（router）是在不同网络间移动数据，相当于传统所说的IP网关（IP gateway）。 网关是连接两个网络的设备，对于语音网关来说，他可以连接PSTN网络和以太网，这就相当于VOIP，把不同电话中的模拟信号通过网关而转换成数字信号，而且加入协议再去传输。在到了接收端的时候再通过网关还原成模拟的电话信号，最后才能在电话机上听到。 对于以太网中的网关只能转发三层以上数据包，这一点和路由是一样的。而不同的是网关中并没有路由表，他只能按照预先设定的不同网段来进行转发。网关最重要的一点就是端口映射，子网内用户在外网看来只是外网的IP地址对应着不同的端口，这样看来就会保护子网内的用户。","categories":[{"name":"network","slug":"network","permalink":"http://github.com/b2stry/categories/network/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://github.com/b2stry/tags/计算机网络/"}]}]}